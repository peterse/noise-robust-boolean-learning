{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "p = 0.1\n",
    "def maj(b):\n",
    "    return 1 if np.sum(b) > k/2 else 0\n",
    "\n",
    "def parity(b):\n",
    "    return np.sum(b) % 2\n",
    "\n",
    "def bool_func_from_signature(n, signature):\n",
    "    \"\"\"\n",
    "    A signature of a boolean function is a length 2^n array that assigns\n",
    "    a 0 or 1 to each input bitstring. The array is shape (2, )^n and the \n",
    "    value at each position is 0 or 1.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find a counterexample\n",
    "\n",
    "We want an example of data and noise such that MLD (for the noisy data) evaluated on noiseless data is _worse_ than MLD (for noiseless data) evaluated on noiseless data.\n",
    "\n",
    "Suppose $f^*(z^{n-1}) = \\argmax_{x'} p_{X|Z^{n-1}}(x|z^{n-1})$ is our MLD for noisy data and $g*(x^{n-1}) = \\argmax_{x'} p_{X|X^{n-1}}(x|x^{n-1})$ is our MLD for noiseless data. We build $f^*$ analytically, then compare it to $g^*$. \n",
    "\n",
    "Our example will be $k=3$ majority function (in which case $g^*$ is just majority)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s(f, x):\n",
    "    \"\"\"Compute the sensitivity of f at x\"\"\"\n",
    "    n = len(x)\n",
    "    out = 0\n",
    "    for i in range(n):\n",
    "        x_ = x.copy()\n",
    "        x_[i] = 1 - x_[i]\n",
    "        if f(x) != f(x_):\n",
    "            out += 1\n",
    "    return out\n",
    "\n",
    "def average_sensitivity(f, X_arr):\n",
    "    n = len(X_arr[0])\n",
    "    out = 0\n",
    "    for x in X_arr:\n",
    "        out += s(f, x)\n",
    "    return out / len(X_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bits_to_idx(b):\n",
    "    out = 0\n",
    "    for bit in b:\n",
    "        out = (out << 1) | bit\n",
    "    return out\n",
    "\n",
    "def idx_to_bits(x, n):\n",
    "    if x == 0: \n",
    "        return [0] * n\n",
    "    bit = []\n",
    "    while x:\n",
    "        bit.append(x % 2)\n",
    "        x >>= 1\n",
    "    return (bit + [0] * (n-len(bit)))[::-1]\n",
    "\n",
    "bits_to_idx(idx_to_bits(22, 7)) == 22 and idx_to_bits(bits_to_idx([1, 0, 0, 1, 0]), 5) == [1, 0, 0, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noisy_distr(n, p, f):\n",
    "    \"\"\"For a true label function f, generate a distribution p_{Z, Y}\n",
    "    where Y = f(X). \n",
    "\n",
    "    ASSUMES UNIFORM DISTR FOR X\n",
    "\n",
    "    we will return P_{Z, Y}(z, y) with shape (2**n, 2)\n",
    "    \n",
    "    \"\"\"\n",
    "    out = np.zeros((2 ** n, 2))\n",
    "    for i, x in enumerate(itertools.product([0, 1], repeat=n)):\n",
    "        p_x = 1 / 2 ** n\n",
    "        y = int(f(x))\n",
    "        for e in product([0, 1], repeat=n):\n",
    "            p_z_given_x = p ** np.sum(e) * (1 - p) ** (n - np.sum(e))\n",
    "            z = np.array(x) ^ np.array(e)\n",
    "            z_idx = bits_to_idx(z)\n",
    "            out[z_idx, y] += p_z_given_x * p_x\n",
    "\n",
    "    return out\n",
    "\n",
    "def compute_acc_noisytest(p_zy, func, n):\n",
    "    \"\"\"THIS ONLY WORKS FOR X UNIFORM DISTRIBUTION!\"\"\"\n",
    "    acc = 0\n",
    "    for i, p_zy in enumerate(p_zy):\n",
    "        # p_z = np.sum(p_zy)\n",
    "        bitstring = idx_to_bits(i, n)\n",
    "        pred = func(bitstring)\n",
    "        acc += p_zy[pred]\n",
    "\n",
    "    return acc\n",
    "\n",
    "def compute_acc_test(func, true_func, n):\n",
    "    \"\"\"assumes x is uniform distribution\"\"\"\n",
    "    acc = 0\n",
    "    for i, x in enumerate(itertools.product([0, 1], repeat=n)):\n",
    "        truth = true_func(x)\n",
    "        pred = func(x)\n",
    "        if truth == pred:\n",
    "            acc += 1 / 2 ** n\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brute-force for $n \\leq 4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "don't run this",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# n = 5\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdon\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt run this\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mboolean_function_from_signature\u001b[39m(f):\n\u001b[0;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Given a length 2^n binary array, return the function with that boolean signature.\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: don't run this"
     ]
    }
   ],
   "source": [
    "# n = 5\n",
    "\n",
    "raise ValueError(\"don't run this\")\n",
    "def boolean_function_from_signature(f):\n",
    "    \"\"\"Given a length 2^n binary array, return the function with that boolean signature.\"\"\"\n",
    "    X_arr = list(itertools.product([0, 1], repeat=n))\n",
    "    X_arr = [tuple(x) for x in X_arr]\n",
    "    lookup = dict(zip(X_arr, f))\n",
    "    def func(x):\n",
    "        return lookup[tuple(x)]\n",
    "    return lookup, func\n",
    "\n",
    "n = 4\n",
    "assert n <= 4\n",
    "k = n\n",
    "pvals = [0.01, 0.25, 0.49]\n",
    "\n",
    "# a \"signature\" of a boolean function is a length 2**n bitstring S where f(x) = S[bin(x)]\n",
    "sig_arr = np.array(list(itertools.product([0, 1], repeat=2 ** n)))\n",
    "X_arr = np.array(list(itertools.product([0, 1], repeat=n)))\n",
    "p_x = 1 / (2 ** k) # uniform distribution over x\n",
    "for i, signature in enumerate(sig_arr):\n",
    "    # iterate over signatures\n",
    "    dct, func = boolean_function_from_signature(signature)\n",
    "    for p in pvals:        \n",
    "        # noisy_lookup[row,col] is the JOINT probability Pr(f(z)=row| x=col)\n",
    "        noisy_lookup = np.zeros((2, 2**n))\n",
    "        true_lookup = np.zeros((2, 2**n))\n",
    "        # simulate a noisy dataset essentially\n",
    "        for i, x in enumerate(product([0,1], repeat=k)):\n",
    "            func_value = func(x)\n",
    "            # true lookup is an array with 2 rows; there is a p_x at [row, column] if \n",
    "            # f[column] = row]. so, true_lookup[i, j] = pr(f(x) = i| x=j)\n",
    "            true_lookup[func(x), i] = 1\n",
    "            # iterate over all of the z values that contribute to \n",
    "            for e in product([0, 1], repeat=k):\n",
    "                z = np.array(x) ^ np.array(e)\n",
    "                p_x_given_z = p ** sum(e) * (1-p)**(k - sum(e))\n",
    "                # increment noisy_lookup at the binary index of z\n",
    "                # noisy_lookup[i, j] = pr(f(z) = i,  x=j) \n",
    "                noisy_lookup[func_value, int(''.join(map(str, z)), 2)] += p_x_given_z \n",
    "        \n",
    "        # the function is balanced if the sums of the two rows of true_lookup are equal\n",
    "        # imbal = abs(true_lookup[0,:].sum() - true_lookup[1,:].sum())  / 2 ** n\n",
    "        # round up to get argmax \n",
    "        noisy_mle = np.round(noisy_lookup)  \n",
    "        out = np.multiply(noisy_mle, true_lookup) / 2 ** n # \"inner product\" of the functions\n",
    "        diff = out.sum()\n",
    "        fnstar_dct = {}\n",
    "        for i, x in enumerate(X_arr):\n",
    "            fnstar_dct[tuple(x)] = np.argmax(noisy_lookup[:, i])\n",
    "        def fnstar(x):\n",
    "            return fnstar_dct[tuple(x)]\n",
    "        \n",
    "        sensitivity_f = average_sensitivity(func, X_arr)\n",
    "        sensitivity_fnstar = average_sensitivity(fnstar, X_arr)\n",
    "        sensitivity_diff = sensitivity_f - sensitivity_fnstar\n",
    "        # accuracies on dataset\n",
    "        p_zy = generate_noisy_distr(k, p, func)\n",
    "        noisy_f_acc = compute_acc_noisytest(p_zy, func, n) # accuracy of f on noisy data\n",
    "        # noiseless_fnstar_acc = compute_acc_test(fnstar, func, n) # accuracy of fN* on noiseless data\n",
    "        noisy_fnstar_acc = compute_acc_noisytest(p_zy, fnstar, n) # accuracy of fN* MLE on noisy data\n",
    "        if sensitivity_f < sensitivity_fnstar:\n",
    "            print(\"ERROR: fn* has lower sensitivity than f\")\n",
    "            raise ValueError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counterexamples computations\n",
    "Here we search over functions where $f(x) = g(\\text{wt}(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counterexample= [1, 0, 0, 1, 1, 0]\n",
      "\t noisy fnstar acc= [np.float64(0.6042299999999998)]\n",
      "\t noisy fnstar sensitivity= 1.875\n",
      "\t noisy f acc= [np.float64(0.5828199999999999)]\n",
      "\t noisy f sensitivity= 2.5\n",
      "(np.float64(0.6042299999999998), 1.875) (np.float64(0.5828199999999999), 2.5)\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Searching for counterexamples\n",
    "\n",
    "Instead of defining boolean functions as $f:\\{0,1\\}^n \\rightarrow \\{0,1\\}$, we define $f$ such that \n",
    "$$\n",
    "f(x) = g(\\text{wt}(x))\n",
    "$$\n",
    "what this does is, there are only $2^{n+1}$ possible functions $g$, instead of $2^{2^n}$ possible functions $f$. For example, if $n=3$, then a possible $g$ is \n",
    "\\begin{equation}\n",
    "     \\text{wt}(x) \\rightarrow \\begin{cases}\n",
    "        0 \\rightarrow 0 \\\\\n",
    "        1 \\rightarrow 1 \\\\\n",
    "        2 \\rightarrow 0 \\\\\n",
    "        3 \\rightarrow 1\n",
    "    \\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boolean signature | imbal? | Pr(fN*=f) | taccfN* |naccfN*-naccf| S(f) | S(fN*) | S(f) - S(fN*)\n",
      "--------------------------------------------------------------------------\n",
      "signature= (0, 0, 0, 0, 0, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(1.0), np.float64(1.0000000000000002), np.float64(0.9999999999999998)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(1.0), np.float64(1.0000000000000002), np.float64(0.9999999999999998)]\n",
      "\t noisy fnstar sensitivity= [0.0, 0.0, 0.0, 0.0]\n",
      "\t noisy f sensitivity= [0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "signature= (0, 0, 0, 0, 0, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.9744056249999998), np.float64(0.9687500000000006), np.float64(0.9687499999999993)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.9744056249999998), np.float64(0.9579800000000006), np.float64(0.9480043749999993)]\n",
      "\t noisy fnstar sensitivity= [0.3125, 0.3125, 0.0, 0.0]\n",
      "\t noisy f sensitivity= [0.3125, 0.3125, 0.3125, 0.3125]\n",
      "\n",
      "signature= (0, 0, 0, 0, 1, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.8811406250000001), np.float64(0.8437500000000003), np.float64(0.8437499999999993)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.8811406250000001), np.float64(0.8155000000000004), np.float64(0.7786093749999996)]\n",
      "\t noisy fnstar sensitivity= [1.5625, 1.5625, 0.0, 0.0]\n",
      "\t noisy f sensitivity= [1.5625, 1.5625, 1.5625, 1.5625]\n",
      "\n",
      "signature= (0, 0, 0, 0, 1, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.8965524999999998), np.float64(0.8273300000000001), np.float64(0.8142637499999997)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.8965524999999998), np.float64(0.8246800000000003), np.float64(0.7716324999999995)]\n",
      "\t noisy fnstar sensitivity= [1.25, 1.25, 0.3125, 0.3125]\n",
      "\t noisy f sensitivity= [1.25, 1.25, 1.25, 1.25]\n",
      "\n",
      "signature= (0, 0, 0, 1, 0, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.7715625000000003), np.float64(0.6875000000000003), np.float64(0.6874999999999998)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.7715625000000003), np.float64(0.6590000000000001), np.float64(0.6064374999999999)]\n",
      "\t noisy fnstar sensitivity= [3.125, 3.125, 0.0, 0.0]\n",
      "\t noisy f sensitivity= [3.125, 3.125, 3.125, 3.125]\n",
      "\n",
      "signature= (0, 0, 0, 1, 0, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.7550806250000003), np.float64(0.6582800000000003), np.float64(0.6562499999999996)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.7550806250000003), np.float64(0.6425800000000003), np.float64(0.5930293749999997)]\n",
      "\t noisy fnstar sensitivity= [3.4375, 3.4375, 0.3125, 0.0]\n",
      "\t noisy f sensitivity= [3.4375, 3.4375, 3.4375, 3.4375]\n",
      "\n",
      "signature= (0, 0, 0, 1, 1, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.8197656249999998), np.float64(0.7056500000000002), np.float64(0.6252874999999999)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.8197656249999998), np.float64(0.6985000000000002), np.float64(0.6147343749999999)]\n",
      "\t noisy fnstar sensitivity= [2.1875, 2.1875, 1.875, 1.875]\n",
      "\t noisy f sensitivity= [2.1875, 2.1875, 2.1875, 2.1875]\n",
      "\n",
      "signature= (0, 0, 0, 1, 1, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.8442900000000001), np.float64(0.7332800000000005), np.float64(0.6463449999999998)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.8442900000000001), np.float64(0.7332800000000005), np.float64(0.6463449999999998)]\n",
      "\t noisy fnstar sensitivity= [1.875, 1.875, 1.875, 1.875]\n",
      "\t noisy f sensitivity= [1.875, 1.875, 1.875, 1.875]\n",
      "\n",
      "signature= (0, 0, 1, 0, 0, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.7715625000000003), np.float64(0.6875), np.float64(0.6874999999999998)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.7715625000000003), np.float64(0.659), np.float64(0.6064375)]\n",
      "\t noisy fnstar sensitivity= [3.125, 3.125, 0.0, 0.0]\n",
      "\t noisy f sensitivity= [3.125, 3.125, 3.125, 3.125]\n",
      "\n",
      "signature= (0, 0, 1, 0, 0, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.7469806250000003), np.float64(0.6562500000000001), np.float64(0.6562499999999999)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.7469806250000003), np.float64(0.6233799999999999), np.float64(0.570979375)]\n",
      "\t noisy fnstar sensitivity= [3.4375, 3.4375, 0.0, 0.0]\n",
      "\t noisy f sensitivity= [3.4375, 3.4375, 3.4375, 3.4375]\n",
      "\n",
      "signature= (0, 0, 1, 0, 1, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.6802656250000003), np.float64(0.5545000000000002), np.float64(0.53125)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.6802656250000003), np.float64(0.5545000000000002), np.float64(0.5149843749999997)]\n",
      "\t noisy fnstar sensitivity= [4.6875, 4.6875, 4.6875, 0.0]\n",
      "\t noisy f sensitivity= [4.6875, 4.6875, 4.6875, 4.6875]\n",
      "\n",
      "signature= (0, 0, 1, 0, 1, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.6966900000000004), np.float64(0.57008), np.float64(0.540195)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.6966900000000004), np.float64(0.57008), np.float64(0.5245449999999997)]\n",
      "\t noisy fnstar sensitivity= [4.375, 4.375, 4.375, 1.875]\n",
      "\t noisy f sensitivity= [4.375, 4.375, 4.375, 4.375]\n",
      "\n",
      "signature= (0, 0, 1, 1, 0, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.7952500000000005), np.float64(0.6640000000000003), np.float64(0.6323749999999995)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.7952500000000005), np.float64(0.6640000000000003), np.float64(0.5852499999999997)]\n",
      "\t noisy fnstar sensitivity= [2.5, 2.5, 2.5, 0.625]\n",
      "\t noisy f sensitivity= [2.5, 2.5, 2.5, 2.5]\n",
      "\n",
      "signature= (0, 0, 1, 1, 0, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.779780625), np.float64(0.6753300000000003), np.float64(0.6597856249999995)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.779780625), np.float64(0.6539800000000003), np.float64(0.5883793749999996)]\n",
      "\t noisy fnstar sensitivity= [2.8125, 2.8125, 1.25, 0.3125]\n",
      "\t noisy f sensitivity= [2.8125, 2.8125, 2.8125, 2.8125]\n",
      "\n",
      "signature= (0, 0, 1, 1, 1, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.8710156250000001), np.float64(0.7961000000000003), np.float64(0.7831656249999993)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.8710156250000001), np.float64(0.7835000000000002), np.float64(0.7234843749999994)]\n",
      "\t noisy fnstar sensitivity= [1.5625, 1.5625, 0.3125, 0.3125]\n",
      "\t noisy f sensitivity= [1.5625, 1.5625, 1.5625, 1.5625]\n",
      "\n",
      "signature= (0, 0, 1, 1, 1, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.8965524999999995), np.float64(0.8273300000000006), np.float64(0.8142637499999996)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.8965524999999995), np.float64(0.8246800000000005), np.float64(0.7716324999999996)]\n",
      "\t noisy fnstar sensitivity= [1.25, 1.25, 0.3125, 0.3125]\n",
      "\t noisy f sensitivity= [1.25, 1.25, 1.25, 1.25]\n",
      "\n",
      "signature= (0, 1, 0, 0, 0, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.881140625), np.float64(0.8437500000000001), np.float64(0.8437499999999994)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.881140625), np.float64(0.8155), np.float64(0.7786093749999995)]\n",
      "\t noisy fnstar sensitivity= [1.5625, 1.5625, 0.0, 0.0]\n",
      "\t noisy f sensitivity= [1.5625, 1.5625, 1.5625, 1.5625]\n",
      "\n",
      "signature= (0, 1, 0, 0, 0, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.8556024999999998), np.float64(0.8125000000000007), np.float64(0.8124999999999996)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.8556024999999998), np.float64(0.7742800000000005), np.float64(0.7301574999999996)]\n",
      "\t noisy fnstar sensitivity= [1.875, 1.875, 0.0, 0.0]\n",
      "\t noisy f sensitivity= [1.875, 1.875, 1.875, 1.875]\n",
      "\n",
      "signature= (0, 1, 0, 0, 1, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.7643125000000005), np.float64(0.6875000000000004), np.float64(0.6874999999999993)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.7643125000000005), np.float64(0.6440000000000002), np.float64(0.5918124999999997)]\n",
      "\t noisy fnstar sensitivity= [3.125, 3.125, 0.0, 0.0]\n",
      "\t noisy f sensitivity= [3.125, 3.125, 3.125, 3.125]\n",
      "\n",
      "signature= (0, 1, 0, 0, 1, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.7797806249999999), np.float64(0.6753300000000002), np.float64(0.6597856249999996)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.7797806249999999), np.float64(0.6539800000000001), np.float64(0.5883793749999995)]\n",
      "\t noisy fnstar sensitivity= [2.8125, 2.8125, 1.25, 0.3125]\n",
      "\t noisy f sensitivity= [2.8125, 2.8125, 2.8125, 2.8125]\n",
      "\n",
      "signature= (0, 1, 0, 1, 0, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.6802656250000001), np.float64(0.5545000000000002), np.float64(0.53125)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.6802656250000001), np.float64(0.5545000000000002), np.float64(0.5149843749999998)]\n",
      "\t noisy fnstar sensitivity= [4.6875, 4.6875, 4.6875, 0.0]\n",
      "\t noisy f sensitivity= [4.6875, 4.6875, 4.6875, 4.6875]\n",
      "\n",
      "signature= (0, 1, 0, 1, 0, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.6638400000000004), np.float64(0.5388800000000002), np.float64(0.5051199999999998)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.6638400000000004), np.float64(0.5388800000000002), np.float64(0.5051199999999998)]\n",
      "\t noisy fnstar sensitivity= [5.0, 5.0, 5.0, 5.0]\n",
      "\t noisy f sensitivity= [5.0, 5.0, 5.0, 5.0]\n",
      "\n",
      "signature= (0, 1, 0, 1, 1, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.7305), np.float64(0.6270500000000002), np.float64(0.6249999999999999)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.7305), np.float64(0.6070000000000003), np.float64(0.557875)]\n",
      "\t noisy fnstar sensitivity= [3.75, 3.75, 0.3125, 0.0]\n",
      "\t noisy f sensitivity= [3.75, 3.75, 3.75, 3.75]\n",
      "\n",
      "signature= (0, 1, 0, 1, 1, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.7550806250000002), np.float64(0.65828), np.float64(0.6562499999999998)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.7550806250000002), np.float64(0.6425800000000002), np.float64(0.5930293749999997)]\n",
      "\t noisy fnstar sensitivity= [3.4375, 3.4375, 0.3125, 0.0]\n",
      "\t noisy f sensitivity= [3.4375, 3.4375, 3.4375, 3.4375]\n",
      "\n",
      "signature= (0, 1, 1, 0, 0, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.819765625), np.float64(0.7056499999999999), np.float64(0.6252875)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.819765625), np.float64(0.6984999999999999), np.float64(0.614734375)]\n",
      "\t noisy fnstar sensitivity= [2.1875, 2.1875, 1.875, 1.875]\n",
      "\t noisy f sensitivity= [2.1875, 2.1875, 2.1875, 2.1875]\n",
      "\n",
      "signature= (0, 1, 1, 0, 0, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.7952399999999998), np.float64(0.6780200000000001), np.float64(0.6042299999999998)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.7952399999999998), np.float64(0.6636800000000003), np.float64(0.5828199999999999)]\n",
      "\t noisy fnstar sensitivity= [2.5, 2.5, 1.875, 1.875]\n",
      "\t noisy f sensitivity= [2.5, 2.5, 2.5, 2.5]\n",
      "\n",
      "signature= (0, 1, 1, 0, 1, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.7305000000000005), np.float64(0.6270500000000001), np.float64(0.6249999999999997)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.7305000000000005), np.float64(0.6070000000000002), np.float64(0.5578749999999998)]\n",
      "\t noisy fnstar sensitivity= [3.75, 3.75, 0.3125, 0.0]\n",
      "\t noisy f sensitivity= [3.75, 3.75, 3.75, 3.75]\n",
      "\n",
      "signature= (0, 1, 1, 0, 1, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.7469806250000003), np.float64(0.6562500000000001), np.float64(0.6562499999999996)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.7469806250000003), np.float64(0.6233800000000002), np.float64(0.5709793749999998)]\n",
      "\t noisy fnstar sensitivity= [3.4375, 3.4375, 0.0, 0.0]\n",
      "\t noisy f sensitivity= [3.4375, 3.4375, 3.4375, 3.4375]\n",
      "\n",
      "signature= (0, 1, 1, 1, 0, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.8710156250000004), np.float64(0.7961000000000005), np.float64(0.7831656249999994)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.8710156250000004), np.float64(0.7835000000000003), np.float64(0.7234843749999998)]\n",
      "\t noisy fnstar sensitivity= [1.5625, 1.5625, 0.3125, 0.3125]\n",
      "\t noisy f sensitivity= [1.5625, 1.5625, 1.5625, 1.5625]\n",
      "\n",
      "signature= (0, 1, 1, 1, 0, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.8556024999999998), np.float64(0.8125000000000004), np.float64(0.8124999999999997)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.8556024999999998), np.float64(0.7742800000000003), np.float64(0.7301574999999995)]\n",
      "\t noisy fnstar sensitivity= [1.875, 1.875, 0.0, 0.0]\n",
      "\t noisy f sensitivity= [1.875, 1.875, 1.875, 1.875]\n",
      "\n",
      "signature= (0, 1, 1, 1, 1, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.9488125000000004), np.float64(0.9375000000000003), np.float64(0.9374999999999996)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.9488125000000004), np.float64(0.9160000000000003), np.float64(0.8963124999999996)]\n",
      "\t noisy fnstar sensitivity= [0.625, 0.625, 0.0, 0.0]\n",
      "\t noisy f sensitivity= [0.625, 0.625, 0.625, 0.625]\n",
      "\n",
      "signature= (0, 1, 1, 1, 1, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.9744056249999998), np.float64(0.9687500000000006), np.float64(0.9687499999999994)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.9744056249999998), np.float64(0.9579800000000004), np.float64(0.9480043749999995)]\n",
      "\t noisy fnstar sensitivity= [0.3125, 0.3125, 0.0, 0.0]\n",
      "\t noisy f sensitivity= [0.3125, 0.3125, 0.3125, 0.3125]\n",
      "\n",
      "signature= (1, 0, 0, 0, 0, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.9744056249999998), np.float64(0.9687500000000006), np.float64(0.9687499999999994)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.9744056249999998), np.float64(0.9579800000000004), np.float64(0.9480043749999995)]\n",
      "\t noisy fnstar sensitivity= [0.3125, 0.3125, 0.0, 0.0]\n",
      "\t noisy f sensitivity= [0.3125, 0.3125, 0.3125, 0.3125]\n",
      "\n",
      "signature= (1, 0, 0, 0, 0, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.9488125000000004), np.float64(0.9375000000000003), np.float64(0.9374999999999996)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.9488125000000004), np.float64(0.9160000000000003), np.float64(0.8963124999999996)]\n",
      "\t noisy fnstar sensitivity= [0.625, 0.625, 0.0, 0.0]\n",
      "\t noisy f sensitivity= [0.625, 0.625, 0.625, 0.625]\n",
      "\n",
      "signature= (1, 0, 0, 0, 1, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.8556024999999998), np.float64(0.8125000000000004), np.float64(0.8124999999999997)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.8556024999999998), np.float64(0.7742800000000003), np.float64(0.7301574999999995)]\n",
      "\t noisy fnstar sensitivity= [1.875, 1.875, 0.0, 0.0]\n",
      "\t noisy f sensitivity= [1.875, 1.875, 1.875, 1.875]\n",
      "\n",
      "signature= (1, 0, 0, 0, 1, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.8710156250000004), np.float64(0.7961000000000005), np.float64(0.7831656249999994)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.8710156250000004), np.float64(0.7835000000000003), np.float64(0.7234843749999998)]\n",
      "\t noisy fnstar sensitivity= [1.5625, 1.5625, 0.3125, 0.3125]\n",
      "\t noisy f sensitivity= [1.5625, 1.5625, 1.5625, 1.5625]\n",
      "\n",
      "signature= (1, 0, 0, 1, 0, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.7469806250000003), np.float64(0.6562500000000001), np.float64(0.6562499999999996)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.7469806250000003), np.float64(0.6233800000000002), np.float64(0.5709793749999998)]\n",
      "\t noisy fnstar sensitivity= [3.4375, 3.4375, 0.0, 0.0]\n",
      "\t noisy f sensitivity= [3.4375, 3.4375, 3.4375, 3.4375]\n",
      "\n",
      "signature= (1, 0, 0, 1, 0, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.7305000000000005), np.float64(0.6270500000000001), np.float64(0.6249999999999997)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.7305000000000005), np.float64(0.6070000000000002), np.float64(0.5578749999999998)]\n",
      "\t noisy fnstar sensitivity= [3.75, 3.75, 0.3125, 0.0]\n",
      "\t noisy f sensitivity= [3.75, 3.75, 3.75, 3.75]\n",
      "\n",
      "signature= (1, 0, 0, 1, 1, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.7952399999999998), np.float64(0.6780200000000001), np.float64(0.6042299999999998)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.7952399999999998), np.float64(0.6636800000000003), np.float64(0.5828199999999999)]\n",
      "\t noisy fnstar sensitivity= [2.5, 2.5, 1.875, 1.875]\n",
      "\t noisy f sensitivity= [2.5, 2.5, 2.5, 2.5]\n",
      "\n",
      "signature= (1, 0, 0, 1, 1, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.819765625), np.float64(0.7056499999999999), np.float64(0.6252875)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.819765625), np.float64(0.6984999999999999), np.float64(0.614734375)]\n",
      "\t noisy fnstar sensitivity= [2.1875, 2.1875, 1.875, 1.875]\n",
      "\t noisy f sensitivity= [2.1875, 2.1875, 2.1875, 2.1875]\n",
      "\n",
      "signature= (1, 0, 1, 0, 0, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.7550806250000002), np.float64(0.65828), np.float64(0.6562499999999998)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.7550806250000002), np.float64(0.6425800000000002), np.float64(0.5930293749999997)]\n",
      "\t noisy fnstar sensitivity= [3.4375, 3.4375, 0.3125, 0.0]\n",
      "\t noisy f sensitivity= [3.4375, 3.4375, 3.4375, 3.4375]\n",
      "\n",
      "signature= (1, 0, 1, 0, 0, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.7305), np.float64(0.6270500000000002), np.float64(0.6249999999999999)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.7305), np.float64(0.6070000000000003), np.float64(0.557875)]\n",
      "\t noisy fnstar sensitivity= [3.75, 3.75, 0.3125, 0.0]\n",
      "\t noisy f sensitivity= [3.75, 3.75, 3.75, 3.75]\n",
      "\n",
      "signature= (1, 0, 1, 0, 1, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.6638400000000004), np.float64(0.5388800000000002), np.float64(0.5051199999999998)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.6638400000000004), np.float64(0.5388800000000002), np.float64(0.5051199999999998)]\n",
      "\t noisy fnstar sensitivity= [5.0, 5.0, 5.0, 5.0]\n",
      "\t noisy f sensitivity= [5.0, 5.0, 5.0, 5.0]\n",
      "\n",
      "signature= (1, 0, 1, 0, 1, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.6802656250000001), np.float64(0.5545000000000002), np.float64(0.53125)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.6802656250000001), np.float64(0.5545000000000002), np.float64(0.5149843749999998)]\n",
      "\t noisy fnstar sensitivity= [4.6875, 4.6875, 4.6875, 0.0]\n",
      "\t noisy f sensitivity= [4.6875, 4.6875, 4.6875, 4.6875]\n",
      "\n",
      "signature= (1, 0, 1, 1, 0, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.7797806249999999), np.float64(0.6753300000000002), np.float64(0.6597856249999996)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.7797806249999999), np.float64(0.6539800000000001), np.float64(0.5883793749999995)]\n",
      "\t noisy fnstar sensitivity= [2.8125, 2.8125, 1.25, 0.3125]\n",
      "\t noisy f sensitivity= [2.8125, 2.8125, 2.8125, 2.8125]\n",
      "\n",
      "signature= (1, 0, 1, 1, 0, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.7643125000000005), np.float64(0.6875000000000004), np.float64(0.6874999999999993)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.7643125000000005), np.float64(0.6440000000000002), np.float64(0.5918124999999997)]\n",
      "\t noisy fnstar sensitivity= [3.125, 3.125, 0.0, 0.0]\n",
      "\t noisy f sensitivity= [3.125, 3.125, 3.125, 3.125]\n",
      "\n",
      "signature= (1, 0, 1, 1, 1, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.8556024999999998), np.float64(0.8125000000000007), np.float64(0.8124999999999996)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.8556024999999998), np.float64(0.7742800000000005), np.float64(0.7301574999999996)]\n",
      "\t noisy fnstar sensitivity= [1.875, 1.875, 0.0, 0.0]\n",
      "\t noisy f sensitivity= [1.875, 1.875, 1.875, 1.875]\n",
      "\n",
      "signature= (1, 0, 1, 1, 1, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.881140625), np.float64(0.8437500000000001), np.float64(0.8437499999999994)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.881140625), np.float64(0.8155), np.float64(0.7786093749999995)]\n",
      "\t noisy fnstar sensitivity= [1.5625, 1.5625, 0.0, 0.0]\n",
      "\t noisy f sensitivity= [1.5625, 1.5625, 1.5625, 1.5625]\n",
      "\n",
      "signature= (1, 1, 0, 0, 0, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.8965524999999995), np.float64(0.8273300000000006), np.float64(0.8142637499999996)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.8965524999999995), np.float64(0.8246800000000005), np.float64(0.7716324999999996)]\n",
      "\t noisy fnstar sensitivity= [1.25, 1.25, 0.3125, 0.3125]\n",
      "\t noisy f sensitivity= [1.25, 1.25, 1.25, 1.25]\n",
      "\n",
      "signature= (1, 1, 0, 0, 0, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.8710156250000001), np.float64(0.7961000000000003), np.float64(0.7831656249999993)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.8710156250000001), np.float64(0.7835000000000002), np.float64(0.7234843749999994)]\n",
      "\t noisy fnstar sensitivity= [1.5625, 1.5625, 0.3125, 0.3125]\n",
      "\t noisy f sensitivity= [1.5625, 1.5625, 1.5625, 1.5625]\n",
      "\n",
      "signature= (1, 1, 0, 0, 1, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.779780625), np.float64(0.6753300000000003), np.float64(0.6597856249999995)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.779780625), np.float64(0.6539800000000003), np.float64(0.5883793749999996)]\n",
      "\t noisy fnstar sensitivity= [2.8125, 2.8125, 1.25, 0.3125]\n",
      "\t noisy f sensitivity= [2.8125, 2.8125, 2.8125, 2.8125]\n",
      "\n",
      "signature= (1, 1, 0, 0, 1, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.7952500000000005), np.float64(0.6640000000000003), np.float64(0.6323749999999995)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.7952500000000005), np.float64(0.6640000000000003), np.float64(0.5852499999999997)]\n",
      "\t noisy fnstar sensitivity= [2.5, 2.5, 2.5, 0.625]\n",
      "\t noisy f sensitivity= [2.5, 2.5, 2.5, 2.5]\n",
      "\n",
      "signature= (1, 1, 0, 1, 0, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.6966900000000004), np.float64(0.57008), np.float64(0.540195)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.6966900000000004), np.float64(0.57008), np.float64(0.5245449999999997)]\n",
      "\t noisy fnstar sensitivity= [4.375, 4.375, 4.375, 1.875]\n",
      "\t noisy f sensitivity= [4.375, 4.375, 4.375, 4.375]\n",
      "\n",
      "signature= (1, 1, 0, 1, 0, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.6802656250000003), np.float64(0.5545000000000002), np.float64(0.53125)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.6802656250000003), np.float64(0.5545000000000002), np.float64(0.5149843749999997)]\n",
      "\t noisy fnstar sensitivity= [4.6875, 4.6875, 4.6875, 0.0]\n",
      "\t noisy f sensitivity= [4.6875, 4.6875, 4.6875, 4.6875]\n",
      "\n",
      "signature= (1, 1, 0, 1, 1, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.7469806250000003), np.float64(0.6562500000000001), np.float64(0.6562499999999999)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.7469806250000003), np.float64(0.6233799999999999), np.float64(0.570979375)]\n",
      "\t noisy fnstar sensitivity= [3.4375, 3.4375, 0.0, 0.0]\n",
      "\t noisy f sensitivity= [3.4375, 3.4375, 3.4375, 3.4375]\n",
      "\n",
      "signature= (1, 1, 0, 1, 1, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.7715625000000003), np.float64(0.6875), np.float64(0.6874999999999998)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.7715625000000003), np.float64(0.659), np.float64(0.6064375)]\n",
      "\t noisy fnstar sensitivity= [3.125, 3.125, 0.0, 0.0]\n",
      "\t noisy f sensitivity= [3.125, 3.125, 3.125, 3.125]\n",
      "\n",
      "signature= (1, 1, 1, 0, 0, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.8442900000000001), np.float64(0.7332800000000005), np.float64(0.6463449999999998)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.8442900000000001), np.float64(0.7332800000000005), np.float64(0.6463449999999998)]\n",
      "\t noisy fnstar sensitivity= [1.875, 1.875, 1.875, 1.875]\n",
      "\t noisy f sensitivity= [1.875, 1.875, 1.875, 1.875]\n",
      "\n",
      "signature= (1, 1, 1, 0, 0, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.8197656249999998), np.float64(0.7056500000000002), np.float64(0.6252874999999999)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.8197656249999998), np.float64(0.6985000000000002), np.float64(0.6147343749999999)]\n",
      "\t noisy fnstar sensitivity= [2.1875, 2.1875, 1.875, 1.875]\n",
      "\t noisy f sensitivity= [2.1875, 2.1875, 2.1875, 2.1875]\n",
      "\n",
      "signature= (1, 1, 1, 0, 1, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.7550806250000003), np.float64(0.6582800000000003), np.float64(0.6562499999999996)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.7550806250000003), np.float64(0.6425800000000003), np.float64(0.5930293749999997)]\n",
      "\t noisy fnstar sensitivity= [3.4375, 3.4375, 0.3125, 0.0]\n",
      "\t noisy f sensitivity= [3.4375, 3.4375, 3.4375, 3.4375]\n",
      "\n",
      "signature= (1, 1, 1, 0, 1, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.7715625000000003), np.float64(0.6875000000000003), np.float64(0.6874999999999998)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.7715625000000003), np.float64(0.6590000000000001), np.float64(0.6064374999999999)]\n",
      "\t noisy fnstar sensitivity= [3.125, 3.125, 0.0, 0.0]\n",
      "\t noisy f sensitivity= [3.125, 3.125, 3.125, 3.125]\n",
      "\n",
      "signature= (1, 1, 1, 1, 0, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.8965524999999998), np.float64(0.8273300000000001), np.float64(0.8142637499999997)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.8965524999999998), np.float64(0.8246800000000003), np.float64(0.7716324999999995)]\n",
      "\t noisy fnstar sensitivity= [1.25, 1.25, 0.3125, 0.3125]\n",
      "\t noisy f sensitivity= [1.25, 1.25, 1.25, 1.25]\n",
      "\n",
      "signature= (1, 1, 1, 1, 0, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.8811406250000001), np.float64(0.8437500000000003), np.float64(0.8437499999999993)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.8811406250000001), np.float64(0.8155000000000004), np.float64(0.7786093749999996)]\n",
      "\t noisy fnstar sensitivity= [1.5625, 1.5625, 0.0, 0.0]\n",
      "\t noisy f sensitivity= [1.5625, 1.5625, 1.5625, 1.5625]\n",
      "\n",
      "signature= (1, 1, 1, 1, 1, 0)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(0.9744056249999998), np.float64(0.9687500000000006), np.float64(0.9687499999999993)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(0.9744056249999998), np.float64(0.9579800000000006), np.float64(0.9480043749999993)]\n",
      "\t noisy fnstar sensitivity= [0.3125, 0.3125, 0.0, 0.0]\n",
      "\t noisy f sensitivity= [0.3125, 0.3125, 0.3125, 0.3125]\n",
      "\n",
      "signature= (1, 1, 1, 1, 1, 1)\n",
      "\t pvals= [0, 0.1, 0.2, 0.3]\n",
      "\t noisy fnstar acc= [np.float64(1.0), np.float64(1.0), np.float64(1.0000000000000002), np.float64(0.9999999999999998)]\n",
      "\t noisy f acc= [np.float64(1.0), np.float64(1.0), np.float64(1.0000000000000002), np.float64(0.9999999999999998)]\n",
      "\t noisy fnstar sensitivity= [0.0, 0.0, 0.0, 0.0]\n",
      "\t noisy f sensitivity= [0.0, 0.0, 0.0, 0.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# n = 5\n",
    "n = 5\n",
    "k = n\n",
    "pvals = [0, 0.1, 0.2, 0.3]\n",
    "\n",
    "X_arr = np.array(list(itertools.product([0, 1], repeat=n)))\n",
    "p_x = 1 / (2 ** k) # uniform distribution over x\n",
    "\n",
    "# counterexamples = [\n",
    "#     [1,0,0,1,1,0],\n",
    "#     [0,1,1,0,1,0],\n",
    "#     [0,1,1,0,0,0]\n",
    "# ]\n",
    "\n",
    "counterexamples = itertools.product([0, 1], repeat=n+1)\n",
    "\n",
    "# H = [[0, 1, 0, 0, 0]]\n",
    "print(\"boolean signature | imbal? | Pr(fN*=f) | taccfN* |naccfN*-naccf| S(f) | S(fN*) | S(f) - S(fN*)\")\n",
    "print(\"--------------------------------------------------------------------------\")\n",
    "for i, signature in enumerate(counterexamples):\n",
    "    print(\"signature=\", signature)\n",
    "    f_accs = []\n",
    "    fn_accs = []\n",
    "    f_sens = []\n",
    "    fn_sens = []\n",
    "    for p in pvals:\n",
    "        hash = dict(zip(range(n+1), signature))\n",
    "        func = lambda b: hash[sum(b)]\n",
    "        \n",
    "        # noisy_lookup[row,col] is the JOINT probability Pr(f(z)=row| x=col)\n",
    "        noisy_lookup = np.zeros((2, 2**n))\n",
    "        true_lookup = np.zeros((2, 2**n))\n",
    "        # simulate a noisy dataset essentially\n",
    "        for i, x in enumerate(product([0,1], repeat=k)):\n",
    "            func_value = func(x)\n",
    "            # true lookup is an array with 2 rows; there is a p_x at [row, column] if \n",
    "            # f[column] = row]. so, true_lookup[i, j] = pr(f(x) = i| x=j)\n",
    "            true_lookup[func(x), i] = 1\n",
    "            # iterate over all of the z values that contribute to \n",
    "            for e in product([0, 1], repeat=k):\n",
    "                z = np.array(x) ^ np.array(e)\n",
    "                p_x_given_z = p ** sum(e) * (1-p)**(k - sum(e))\n",
    "                # increment noisy_lookup at the binary index of z\n",
    "                # noisy_lookup[i, j] = pr(f(z) = i,  x=j) \n",
    "                noisy_lookup[func_value, int(''.join(map(str, z)), 2)] += p_x_given_z \n",
    "        \n",
    "        # the function is balanced if the sums of the two rows of true_lookup are equal\n",
    "        imbal = abs(true_lookup[0,:].sum() - true_lookup[1,:].sum())  / 2 ** n\n",
    "        \n",
    "        # if not balanced:\n",
    "        #     continue\n",
    "        # round up to get argmax \n",
    "        noisy_mle = np.round(noisy_lookup)  \n",
    "        out = np.multiply(noisy_mle, true_lookup) / 2 ** n # \"inner product\" of the functions\n",
    "        diff = out.sum()\n",
    "\n",
    "\n",
    "        fnstar_dct = {}\n",
    "        for i, x in enumerate(X_arr):\n",
    "            fnstar_dct[tuple(x)] = np.argmax(noisy_lookup[:, i])\n",
    "        def fnstar(x):\n",
    "            return fnstar_dct[tuple(x)]\n",
    "        \n",
    "        sensitivity_f = average_sensitivity(func, X_arr)\n",
    "        sensitivity_fnstar = average_sensitivity(fnstar, X_arr)\n",
    "        sensitivity_diff = sensitivity_f - sensitivity_fnstar\n",
    "        # accuracies on dataset\n",
    "        p_zy = generate_noisy_distr(k, p, func)\n",
    "        noisy_f_acc = compute_acc_noisytest(p_zy, func, n) # accuracy of f on noisy data\n",
    "        noiseless_fnstar_acc = compute_acc_test(fnstar, func, n) # accuracy of fN* on noiseless data\n",
    "        noisy_fnstar_acc = compute_acc_noisytest(p_zy, fnstar, n) # accuracy of fN* MLE on noisy data\n",
    "\n",
    "        acc_diff = noisy_fnstar_acc - noisy_f_acc\n",
    "        f_accs.append(noisy_f_acc)\n",
    "        fn_accs.append(noisy_fnstar_acc)\n",
    "        f_sens.append(sensitivity_f)\n",
    "        fn_sens.append(sensitivity_fnstar)\n",
    "\n",
    "    print(\"\\t pvals=\", pvals)\n",
    "    print(\"\\t noisy fnstar acc=\", fn_accs)\n",
    "    print(\"\\t noisy f acc=\", f_accs)\n",
    "    print(\"\\t noisy fnstar sensitivity=\", fn_sens)\n",
    "    print(\"\\t true f sensitivity=\", f_sens)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_boolean_function(n):\n",
    "    X_arr = list(itertools.product([0, 1], repeat=n))\n",
    "    X_arr = [tuple(x) for x in X_arr]\n",
    "    f = np.random.randint(2, size=2**n, dtype=int)\n",
    "    f = [x.item() for x in f]\n",
    "    lookup = dict(zip(X_arr, f))\n",
    "    def func(x):\n",
    "        return lookup[tuple(x)]\n",
    "    return lookup, func\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random search over boolean functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n = 6 # n=7 takes about .5 second per trial\n",
    "# p = 0.45\n",
    "p = np.random.random()/2\n",
    "trials = 100\n",
    "\n",
    "X_arr = np.array(list(itertools.product([0, 1], repeat=n)))\n",
    "p_x = 1 / (2 ** n) # uniform distribution over x\n",
    "out = []\n",
    "for j in range(trials):\n",
    "    dct = {\"j\": j}\n",
    "    lookup, func = random_boolean_function(n)\n",
    "    \n",
    "    # noisy_lookup[row,col] is the JOINT probability Pr(f(z)=row| x=col)\n",
    "    noisy_lookup = np.zeros((2, 2**n))\n",
    "    true_lookup = np.zeros((2, 2**n))\n",
    "    # simulate a noisy dataset essentially\n",
    "    for i, x in enumerate(product([0,1], repeat=n)):\n",
    "        x = tuple(x)\n",
    "        func_value = func(x)\n",
    "        # true lookup is an array with 2 rows; there is a p_x at [row, column] if \n",
    "        # f[column] = row]. so, true_lookup[i, j] = pr(f(x) = i| x=j)\n",
    "        true_lookup[func(x), i] = 1\n",
    "        # iterate over all of the z values that contribute to \n",
    "        for e in product([0, 1], repeat=n):\n",
    "            z = np.array(x) ^ np.array(e)\n",
    "            p_x_given_z = p ** sum(e) * (1-p)**(n - sum(e))\n",
    "            # increment noisy_lookup at the binary index of z\n",
    "            # noisy_lookup[i, j] = pr(f(z) = i,  x=j) \n",
    "            noisy_lookup[func_value, int(''.join(map(str, z)), 2)] += p_x_given_z \n",
    "    \n",
    "    # the function is balanced if the sums of the two rows of true_lookup are equal\n",
    "    imbal = abs(true_lookup[0,:].sum() - true_lookup[1,:].sum())  / 2 ** n\n",
    "\n",
    "    fnstar_dct = {}\n",
    "    for i, x in enumerate(X_arr):\n",
    "        fnstar_dct[tuple(x)] = np.argmax(noisy_lookup[:, i])\n",
    "    def fnstar(x):\n",
    "        return fnstar_dct[tuple(x)]\n",
    "    \n",
    "    sensitivity_f = average_sensitivity(func, X_arr)\n",
    "    sensitivity_fnstar = average_sensitivity(fnstar, X_arr)\n",
    "    sensitivity_diff = sensitivity_f - sensitivity_fnstar\n",
    "    # accuracies on dataset\n",
    "    noiseless_fnstar_acc  = compute_acc_test(fnstar, func, n) # accuracy of fN* on noiseless data\n",
    "\n",
    "    p_zy = generate_noisy_distr(n, p, func)\n",
    "    noisy_f_acc = compute_acc_noisytest(p_zy, func, n) # accuracy of f on noisy data\n",
    "    noisy_fnstar_acc = compute_acc_noisytest(p_zy, fnstar, n) # accuracy of fN* MLE on noisy data\n",
    "    nacc_diff = noisy_fnstar_acc - noisy_f_acc\n",
    "    dct.update({\n",
    "        \"imbal\": imbal,\n",
    "        \"noisy_acc_fnstar\": noisy_fnstar_acc,\n",
    "        \"noiseless_acc_fnstar\": noiseless_fnstar_acc,\n",
    "        \"sensitivity_f\": sensitivity_f,\n",
    "        \"sensitivity_fnstar\": sensitivity_fnstar,\n",
    "    })\n",
    "    out.append(dct)\n",
    "\n",
    "    if sensitivity_f < sensitivity_fnstar:\n",
    "        print(\"COUNTEREXAMPLE\")\n",
    "        print(\"sensitivity_f\", sensitivity_f)\n",
    "        print(\"sensitivity_fnstar\", sensitivity_fnstar)\n",
    "        print(\"f\", func)\n",
    "        print(\"fnstar\", fnstar)\n",
    "    # if sensitivity_f > sensitivity_fnstar and sensitivity_fnstar > 0:\n",
    "    #     for k, v in fnstar_dct.items():\n",
    "    #         fnstar_dct[tuple([int(x) for x in k])] = int(v)\n",
    "    #     for k in lookup.keys():\n",
    "    #         print(k, lookup[k], fnstar_dct[k])\n",
    "    #     print(sensitivity_f - sensitivity_fnstar)\n",
    "    #     print(\"fnstar sensitivity\", sensitivity_fnstar)\n",
    "    #     print(\"f sensitivity\", sensitivity_f)\n",
    "    #     print()\n",
    "\n",
    "df = pd.DataFrame(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.head()\n",
    "\n",
    "sf = df[\"sensitivity_f\"] \n",
    "sfn = df[\"sensitivity_fnstar\"] \n",
    "fn_noiseless_acc = df[\"noiseless_acc_fnstar\"]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axes[0].hist2d(sf, sfn, cmap='binary')\n",
    "\n",
    "# axes[0].scatter(sf, sfn)\n",
    "# use square aspect\n",
    "axes[0].set_xlabel(\"Sensitivity f\")\n",
    "axes[0].set_ylabel(\"Sensitivity fN*\")\n",
    "# axes[0].set_xlim(axes[0].get_ylim())\n",
    "axes[0].plot([min(sf), max(sf)], [min(sf), max(sf)], ls='--', c='k')\n",
    "\n",
    "# 2D histogram of sf-sfn vs fn_noiseless_acc\n",
    "axes[1].hist2d(sf-sfn, fn_noiseless_acc,  cmap='binary')\n",
    "axes[1].set_xlabel(\"Sensitivity f - Sensitivity fN*\")\n",
    "axes[1].set_ylabel(\"Noiseless Accuracy fN*\")\n",
    "# axes[1].axhline(1 - p/2)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(.319*100, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "!!!p=0\n",
      "boolean signature | imbal? | nacc(fN*) | taccfN* |naccfN*-naccf| S(f) | S(fN*) | S(f) - S(fN*)\n",
      "--------------------------------------------------------------------------\n",
      "  [0 0 0 0 0 0]   | 1.0000 |   1.0000  | 1.0000 | 0.0000    |0.0000|0.0000  |  0.0000 \n",
      "  [0 0 0 0 0 1]   | 0.9375 |   1.0000  | 1.0000 | 0.0000    |0.3125|0.3125  |  0.0000 \n",
      "  [0 0 0 0 1 0]   | 0.6875 |   1.0000  | 1.0000 | 0.0000    |1.5625|1.5625  |  0.0000 \n",
      "  [0 0 0 0 1 1]   | 0.6250 |   1.0000  | 1.0000 | 0.0000    |1.2500|1.2500  |  0.0000 \n",
      "  [0 0 0 1 0 0]   | 0.3750 |   1.0000  | 1.0000 | 0.0000    |3.1250|3.1250  |  0.0000 \n",
      "  [0 0 0 1 0 1]   | 0.3125 |   1.0000  | 1.0000 | 0.0000    |3.4375|3.4375  |  0.0000 \n",
      "  [0 0 0 1 1 0]   | 0.0625 |   1.0000  | 1.0000 | 0.0000    |2.1875|2.1875  |  0.0000 \n",
      "  [0 0 0 1 1 1]   | 0.0000 |   1.0000  | 1.0000 | 0.0000    |1.8750|1.8750  |  0.0000 \n",
      "  [0 0 1 0 0 0]   | 0.3750 |   1.0000  | 1.0000 | 0.0000    |3.1250|3.1250  |  0.0000 \n",
      "  [0 0 1 0 0 1]   | 0.3125 |   1.0000  | 1.0000 | 0.0000    |3.4375|3.4375  |  0.0000 \n",
      "  [0 0 1 0 1 0]   | 0.0625 |   1.0000  | 1.0000 | 0.0000    |4.6875|4.6875  |  0.0000 \n",
      "  [0 0 1 0 1 1]   | 0.0000 |   1.0000  | 1.0000 | 0.0000    |4.3750|4.3750  |  0.0000 \n",
      "  [0 0 1 1 0 0]   | 0.2500 |   1.0000  | 1.0000 | 0.0000    |2.5000|2.5000  |  0.0000 \n",
      "  [0 0 1 1 0 1]   | 0.3125 |   1.0000  | 1.0000 | 0.0000    |2.8125|2.8125  |  0.0000 \n",
      "  [0 0 1 1 1 0]   | 0.5625 |   1.0000  | 1.0000 | 0.0000    |1.5625|1.5625  |  0.0000 \n",
      "  [0 0 1 1 1 1]   | 0.6250 |   1.0000  | 1.0000 | 0.0000    |1.2500|1.2500  |  0.0000 \n",
      "  [0 1 0 0 0 0]   | 0.6875 |   1.0000  | 1.0000 | 0.0000    |1.5625|1.5625  |  0.0000 \n",
      "  [0 1 0 0 0 1]   | 0.6250 |   1.0000  | 1.0000 | 0.0000    |1.8750|1.8750  |  0.0000 \n",
      "  [0 1 0 0 1 0]   | 0.3750 |   1.0000  | 1.0000 | 0.0000    |3.1250|3.1250  |  0.0000 \n",
      "  [0 1 0 0 1 1]   | 0.3125 |   1.0000  | 1.0000 | 0.0000    |2.8125|2.8125  |  0.0000 \n",
      "  [0 1 0 1 0 0]   | 0.0625 |   1.0000  | 1.0000 | 0.0000    |4.6875|4.6875  |  0.0000 \n",
      "  [0 1 0 1 0 1]   | 0.0000 |   1.0000  | 1.0000 | 0.0000    |5.0000|5.0000  |  0.0000 \n",
      "  [0 1 0 1 1 0]   | 0.2500 |   1.0000  | 1.0000 | 0.0000    |3.7500|3.7500  |  0.0000 \n",
      "  [0 1 0 1 1 1]   | 0.3125 |   1.0000  | 1.0000 | 0.0000    |3.4375|3.4375  |  0.0000 \n",
      ">>[0 1 1 0 0 0]   | 0.0625 |   1.0000  | 1.0000 | 0.0000    |2.1875|2.1875  |  0.0000 \n",
      "  [0 1 1 0 0 1]   | 0.0000 |   1.0000  | 1.0000 | 0.0000    |2.5000|2.5000  |  0.0000 \n",
      ">>[0 1 1 0 1 0]   | 0.2500 |   1.0000  | 1.0000 | 0.0000    |3.7500|3.7500  |  0.0000 \n",
      "  [0 1 1 0 1 1]   | 0.3125 |   1.0000  | 1.0000 | 0.0000    |3.4375|3.4375  |  0.0000 \n",
      "  [0 1 1 1 0 0]   | 0.5625 |   1.0000  | 1.0000 | 0.0000    |1.5625|1.5625  |  0.0000 \n",
      "  [0 1 1 1 0 1]   | 0.6250 |   1.0000  | 1.0000 | 0.0000    |1.8750|1.8750  |  0.0000 \n",
      "  [0 1 1 1 1 0]   | 0.8750 |   1.0000  | 1.0000 | 0.0000    |0.6250|0.6250  |  0.0000 \n",
      "  [0 1 1 1 1 1]   | 0.9375 |   1.0000  | 1.0000 | 0.0000    |0.3125|0.3125  |  0.0000 \n",
      "  [1 0 0 0 0 0]   | 0.9375 |   1.0000  | 1.0000 | 0.0000    |0.3125|0.3125  |  0.0000 \n",
      "  [1 0 0 0 0 1]   | 0.8750 |   1.0000  | 1.0000 | 0.0000    |0.6250|0.6250  |  0.0000 \n",
      "  [1 0 0 0 1 0]   | 0.6250 |   1.0000  | 1.0000 | 0.0000    |1.8750|1.8750  |  0.0000 \n",
      "  [1 0 0 0 1 1]   | 0.5625 |   1.0000  | 1.0000 | 0.0000    |1.5625|1.5625  |  0.0000 \n",
      "  [1 0 0 1 0 0]   | 0.3125 |   1.0000  | 1.0000 | 0.0000    |3.4375|3.4375  |  0.0000 \n",
      "  [1 0 0 1 0 1]   | 0.2500 |   1.0000  | 1.0000 | 0.0000    |3.7500|3.7500  |  0.0000 \n",
      ">>[1 0 0 1 1 0]   | 0.0000 |   1.0000  | 1.0000 | 0.0000    |2.5000|2.5000  |  0.0000 \n",
      "  [1 0 0 1 1 1]   | 0.0625 |   1.0000  | 1.0000 | 0.0000    |2.1875|2.1875  |  0.0000 \n",
      "  [1 0 1 0 0 0]   | 0.3125 |   1.0000  | 1.0000 | 0.0000    |3.4375|3.4375  |  0.0000 \n",
      "  [1 0 1 0 0 1]   | 0.2500 |   1.0000  | 1.0000 | 0.0000    |3.7500|3.7500  |  0.0000 \n",
      "  [1 0 1 0 1 0]   | 0.0000 |   1.0000  | 1.0000 | 0.0000    |5.0000|5.0000  |  0.0000 \n",
      "  [1 0 1 0 1 1]   | 0.0625 |   1.0000  | 1.0000 | 0.0000    |4.6875|4.6875  |  0.0000 \n",
      "  [1 0 1 1 0 0]   | 0.3125 |   1.0000  | 1.0000 | 0.0000    |2.8125|2.8125  |  0.0000 \n",
      "  [1 0 1 1 0 1]   | 0.3750 |   1.0000  | 1.0000 | 0.0000    |3.1250|3.1250  |  0.0000 \n",
      "  [1 0 1 1 1 0]   | 0.6250 |   1.0000  | 1.0000 | 0.0000    |1.8750|1.8750  |  0.0000 \n",
      "  [1 0 1 1 1 1]   | 0.6875 |   1.0000  | 1.0000 | 0.0000    |1.5625|1.5625  |  0.0000 \n",
      "  [1 1 0 0 0 0]   | 0.6250 |   1.0000  | 1.0000 | 0.0000    |1.2500|1.2500  |  0.0000 \n",
      "  [1 1 0 0 0 1]   | 0.5625 |   1.0000  | 1.0000 | 0.0000    |1.5625|1.5625  |  0.0000 \n",
      "  [1 1 0 0 1 0]   | 0.3125 |   1.0000  | 1.0000 | 0.0000    |2.8125|2.8125  |  0.0000 \n",
      "  [1 1 0 0 1 1]   | 0.2500 |   1.0000  | 1.0000 | 0.0000    |2.5000|2.5000  |  0.0000 \n",
      "  [1 1 0 1 0 0]   | 0.0000 |   1.0000  | 1.0000 | 0.0000    |4.3750|4.3750  |  0.0000 \n",
      "  [1 1 0 1 0 1]   | 0.0625 |   1.0000  | 1.0000 | 0.0000    |4.6875|4.6875  |  0.0000 \n",
      "  [1 1 0 1 1 0]   | 0.3125 |   1.0000  | 1.0000 | 0.0000    |3.4375|3.4375  |  0.0000 \n",
      "  [1 1 0 1 1 1]   | 0.3750 |   1.0000  | 1.0000 | 0.0000    |3.1250|3.1250  |  0.0000 \n",
      "  [1 1 1 0 0 0]   | 0.0000 |   1.0000  | 1.0000 | 0.0000    |1.8750|1.8750  |  0.0000 \n",
      "  [1 1 1 0 0 1]   | 0.0625 |   1.0000  | 1.0000 | 0.0000    |2.1875|2.1875  |  0.0000 \n",
      "  [1 1 1 0 1 0]   | 0.3125 |   1.0000  | 1.0000 | 0.0000    |3.4375|3.4375  |  0.0000 \n",
      "  [1 1 1 0 1 1]   | 0.3750 |   1.0000  | 1.0000 | 0.0000    |3.1250|3.1250  |  0.0000 \n",
      "  [1 1 1 1 0 0]   | 0.6250 |   1.0000  | 1.0000 | 0.0000    |1.2500|1.2500  |  0.0000 \n",
      "  [1 1 1 1 0 1]   | 0.6875 |   1.0000  | 1.0000 | 0.0000    |1.5625|1.5625  |  0.0000 \n",
      "  [1 1 1 1 1 0]   | 0.9375 |   1.0000  | 1.0000 | 0.0000    |0.3125|0.3125  |  0.0000 \n",
      "  [1 1 1 1 1 1]   | 1.0000 |   1.0000  | 1.0000 | 0.0000    |0.0000|0.0000  |  0.0000 \n",
      "\n",
      "!!!p=0.1\n",
      "boolean signature | imbal? | nacc(fN*) | taccfN* |naccfN*-naccf| S(f) | S(fN*) | S(f) - S(fN*)\n",
      "--------------------------------------------------------------------------\n",
      "  [0 0 0 0 0 0]   | 1.0000 |   1.0000  | 1.0000 | 0.0000    |0.0000|0.0000  |  0.0000 \n",
      "  [0 0 0 0 0 1]   | 0.9375 |   0.9744  | 1.0000 | 0.0000    |0.3125|0.3125  |  0.0000 \n",
      "  [0 0 0 0 1 0]   | 0.6875 |   0.8811  | 1.0000 | 0.0000    |1.5625|1.5625  |  0.0000 \n",
      "  [0 0 0 0 1 1]   | 0.6250 |   0.8966  | 1.0000 | 0.0000    |1.2500|1.2500  |  0.0000 \n",
      "  [0 0 0 1 0 0]   | 0.3750 |   0.7716  | 1.0000 | 0.0000    |3.1250|3.1250  |  0.0000 \n",
      "  [0 0 0 1 0 1]   | 0.3125 |   0.7551  | 1.0000 | 0.0000    |3.4375|3.4375  |  0.0000 \n",
      "  [0 0 0 1 1 0]   | 0.0625 |   0.8198  | 1.0000 | 0.0000    |2.1875|2.1875  |  0.0000 \n",
      "  [0 0 0 1 1 1]   | 0.0000 |   0.8443  | 1.0000 | 0.0000    |1.8750|1.8750  |  0.0000 \n",
      "  [0 0 1 0 0 0]   | 0.3750 |   0.7716  | 1.0000 | 0.0000    |3.1250|3.1250  |  0.0000 \n",
      "  [0 0 1 0 0 1]   | 0.3125 |   0.7470  | 1.0000 | 0.0000    |3.4375|3.4375  |  0.0000 \n",
      "  [0 0 1 0 1 0]   | 0.0625 |   0.6803  | 1.0000 | 0.0000    |4.6875|4.6875  |  0.0000 \n",
      "  [0 0 1 0 1 1]   | 0.0000 |   0.6967  | 1.0000 | 0.0000    |4.3750|4.3750  |  0.0000 \n",
      "  [0 0 1 1 0 0]   | 0.2500 |   0.7953  | 1.0000 | 0.0000    |2.5000|2.5000  |  0.0000 \n",
      "  [0 0 1 1 0 1]   | 0.3125 |   0.7798  | 1.0000 | 0.0000    |2.8125|2.8125  |  0.0000 \n",
      "  [0 0 1 1 1 0]   | 0.5625 |   0.8710  | 1.0000 | 0.0000    |1.5625|1.5625  |  0.0000 \n",
      "  [0 0 1 1 1 1]   | 0.6250 |   0.8966  | 1.0000 | 0.0000    |1.2500|1.2500  |  0.0000 \n",
      "  [0 1 0 0 0 0]   | 0.6875 |   0.8811  | 1.0000 | 0.0000    |1.5625|1.5625  |  0.0000 \n",
      "  [0 1 0 0 0 1]   | 0.6250 |   0.8556  | 1.0000 | 0.0000    |1.8750|1.8750  |  0.0000 \n",
      "  [0 1 0 0 1 0]   | 0.3750 |   0.7643  | 1.0000 | 0.0000    |3.1250|3.1250  |  0.0000 \n",
      "  [0 1 0 0 1 1]   | 0.3125 |   0.7798  | 1.0000 | 0.0000    |2.8125|2.8125  |  0.0000 \n",
      "  [0 1 0 1 0 0]   | 0.0625 |   0.6803  | 1.0000 | 0.0000    |4.6875|4.6875  |  0.0000 \n",
      "  [0 1 0 1 0 1]   | 0.0000 |   0.6638  | 1.0000 | 0.0000    |5.0000|5.0000  |  0.0000 \n",
      "  [0 1 0 1 1 0]   | 0.2500 |   0.7305  | 1.0000 | 0.0000    |3.7500|3.7500  |  0.0000 \n",
      "  [0 1 0 1 1 1]   | 0.3125 |   0.7551  | 1.0000 | 0.0000    |3.4375|3.4375  |  0.0000 \n",
      ">>[0 1 1 0 0 0]   | 0.0625 |   0.8198  | 1.0000 | 0.0000    |2.1875|2.1875  |  0.0000 \n",
      "  [0 1 1 0 0 1]   | 0.0000 |   0.7952  | 1.0000 | 0.0000    |2.5000|2.5000  |  0.0000 \n",
      ">>[0 1 1 0 1 0]   | 0.2500 |   0.7305  | 1.0000 | 0.0000    |3.7500|3.7500  |  0.0000 \n",
      "  [0 1 1 0 1 1]   | 0.3125 |   0.7470  | 1.0000 | 0.0000    |3.4375|3.4375  |  0.0000 \n",
      "  [0 1 1 1 0 0]   | 0.5625 |   0.8710  | 1.0000 | 0.0000    |1.5625|1.5625  |  0.0000 \n",
      "  [0 1 1 1 0 1]   | 0.6250 |   0.8556  | 1.0000 | 0.0000    |1.8750|1.8750  |  0.0000 \n",
      "  [0 1 1 1 1 0]   | 0.8750 |   0.9488  | 1.0000 | 0.0000    |0.6250|0.6250  |  0.0000 \n",
      "  [0 1 1 1 1 1]   | 0.9375 |   0.9744  | 1.0000 | 0.0000    |0.3125|0.3125  |  0.0000 \n",
      "  [1 0 0 0 0 0]   | 0.9375 |   0.9744  | 1.0000 | 0.0000    |0.3125|0.3125  |  0.0000 \n",
      "  [1 0 0 0 0 1]   | 0.8750 |   0.9488  | 1.0000 | 0.0000    |0.6250|0.6250  |  0.0000 \n",
      "  [1 0 0 0 1 0]   | 0.6250 |   0.8556  | 1.0000 | 0.0000    |1.8750|1.8750  |  0.0000 \n",
      "  [1 0 0 0 1 1]   | 0.5625 |   0.8710  | 1.0000 | 0.0000    |1.5625|1.5625  |  0.0000 \n",
      "  [1 0 0 1 0 0]   | 0.3125 |   0.7470  | 1.0000 | 0.0000    |3.4375|3.4375  |  0.0000 \n",
      "  [1 0 0 1 0 1]   | 0.2500 |   0.7305  | 1.0000 | 0.0000    |3.7500|3.7500  |  0.0000 \n",
      ">>[1 0 0 1 1 0]   | 0.0000 |   0.7952  | 1.0000 | 0.0000    |2.5000|2.5000  |  0.0000 \n",
      "  [1 0 0 1 1 1]   | 0.0625 |   0.8198  | 1.0000 | 0.0000    |2.1875|2.1875  |  0.0000 \n",
      "  [1 0 1 0 0 0]   | 0.3125 |   0.7551  | 1.0000 | 0.0000    |3.4375|3.4375  |  0.0000 \n",
      "  [1 0 1 0 0 1]   | 0.2500 |   0.7305  | 1.0000 | 0.0000    |3.7500|3.7500  |  0.0000 \n",
      "  [1 0 1 0 1 0]   | 0.0000 |   0.6638  | 1.0000 | 0.0000    |5.0000|5.0000  |  0.0000 \n",
      "  [1 0 1 0 1 1]   | 0.0625 |   0.6803  | 1.0000 | 0.0000    |4.6875|4.6875  |  0.0000 \n",
      "  [1 0 1 1 0 0]   | 0.3125 |   0.7798  | 1.0000 | 0.0000    |2.8125|2.8125  |  0.0000 \n",
      "  [1 0 1 1 0 1]   | 0.3750 |   0.7643  | 1.0000 | 0.0000    |3.1250|3.1250  |  0.0000 \n",
      "  [1 0 1 1 1 0]   | 0.6250 |   0.8556  | 1.0000 | 0.0000    |1.8750|1.8750  |  0.0000 \n",
      "  [1 0 1 1 1 1]   | 0.6875 |   0.8811  | 1.0000 | 0.0000    |1.5625|1.5625  |  0.0000 \n",
      "  [1 1 0 0 0 0]   | 0.6250 |   0.8966  | 1.0000 | 0.0000    |1.2500|1.2500  |  0.0000 \n",
      "  [1 1 0 0 0 1]   | 0.5625 |   0.8710  | 1.0000 | 0.0000    |1.5625|1.5625  |  0.0000 \n",
      "  [1 1 0 0 1 0]   | 0.3125 |   0.7798  | 1.0000 | 0.0000    |2.8125|2.8125  |  0.0000 \n",
      "  [1 1 0 0 1 1]   | 0.2500 |   0.7953  | 1.0000 | 0.0000    |2.5000|2.5000  |  0.0000 \n",
      "  [1 1 0 1 0 0]   | 0.0000 |   0.6967  | 1.0000 | 0.0000    |4.3750|4.3750  |  0.0000 \n",
      "  [1 1 0 1 0 1]   | 0.0625 |   0.6803  | 1.0000 | 0.0000    |4.6875|4.6875  |  0.0000 \n",
      "  [1 1 0 1 1 0]   | 0.3125 |   0.7470  | 1.0000 | 0.0000    |3.4375|3.4375  |  0.0000 \n",
      "  [1 1 0 1 1 1]   | 0.3750 |   0.7716  | 1.0000 | 0.0000    |3.1250|3.1250  |  0.0000 \n",
      "  [1 1 1 0 0 0]   | 0.0000 |   0.8443  | 1.0000 | 0.0000    |1.8750|1.8750  |  0.0000 \n",
      "  [1 1 1 0 0 1]   | 0.0625 |   0.8198  | 1.0000 | 0.0000    |2.1875|2.1875  |  0.0000 \n",
      "  [1 1 1 0 1 0]   | 0.3125 |   0.7551  | 1.0000 | 0.0000    |3.4375|3.4375  |  0.0000 \n",
      "  [1 1 1 0 1 1]   | 0.3750 |   0.7716  | 1.0000 | 0.0000    |3.1250|3.1250  |  0.0000 \n",
      "  [1 1 1 1 0 0]   | 0.6250 |   0.8966  | 1.0000 | 0.0000    |1.2500|1.2500  |  0.0000 \n",
      "  [1 1 1 1 0 1]   | 0.6875 |   0.8811  | 1.0000 | 0.0000    |1.5625|1.5625  |  0.0000 \n",
      "  [1 1 1 1 1 0]   | 0.9375 |   0.9744  | 1.0000 | 0.0000    |0.3125|0.3125  |  0.0000 \n",
      "  [1 1 1 1 1 1]   | 1.0000 |   1.0000  | 1.0000 | 0.0000    |0.0000|0.0000  |  0.0000 \n",
      "\n",
      "!!!p=0.15\n",
      "boolean signature | imbal? | nacc(fN*) | taccfN* |naccfN*-naccf| S(f) | S(fN*) | S(f) - S(fN*)\n",
      "--------------------------------------------------------------------------\n",
      "  [0 0 0 0 0 0]   | 1.0000 |   1.0000  | 1.0000 | 0.0000    |0.0000|0.0000  |  0.0000 \n",
      "  [0 0 0 0 0 1]   | 0.9375 |   0.9687  | 0.9688 | 0.0035    |0.3125|0.0000  |  0.3125 \n",
      "  [0 0 0 0 1 0]   | 0.6875 |   0.8437  | 0.8438 | 0.0003    |1.5625|0.0000  |  1.5625 \n",
      "  [0 0 0 0 1 1]   | 0.6250 |   0.8576  | 1.0000 | 0.0000    |1.2500|1.2500  |  0.0000 \n",
      "  [0 0 0 1 0 0]   | 0.3750 |   0.7049  | 1.0000 | 0.0000    |3.1250|3.1250  |  0.0000 \n",
      "  [0 0 0 1 0 1]   | 0.3125 |   0.6874  | 1.0000 | 0.0000    |3.4375|3.4375  |  0.0000 \n",
      "  [0 0 0 1 1 0]   | 0.0625 |   0.7551  | 0.9688 | 0.0019    |2.1875|1.8750  |  0.3125 \n",
      "  [0 0 0 1 1 1]   | 0.0000 |   0.7847  | 1.0000 | 0.0000    |1.8750|1.8750  |  0.0000 \n",
      "  [0 0 1 0 0 0]   | 0.3750 |   0.7049  | 1.0000 | 0.0000    |3.1250|3.1250  |  0.0000 \n",
      "  [0 0 1 0 0 1]   | 0.3125 |   0.6752  | 0.9688 | 0.0020    |3.4375|3.1250  |  0.3125 \n",
      "  [0 0 1 0 1 0]   | 0.0625 |   0.6013  | 1.0000 | 0.0000    |4.6875|4.6875  |  0.0000 \n",
      "  [0 0 1 0 1 1]   | 0.0000 |   0.6185  | 1.0000 | 0.0000    |4.3750|4.3750  |  0.0000 \n",
      "  [0 0 1 1 0 0]   | 0.2500 |   0.7219  | 1.0000 | 0.0000    |2.5000|2.5000  |  0.0000 \n",
      "  [0 0 1 1 0 1]   | 0.3125 |   0.7074  | 1.0000 | 0.0000    |2.8125|2.8125  |  0.0000 \n",
      "  [0 0 1 1 1 0]   | 0.5625 |   0.8265  | 0.9688 | 0.0034    |1.5625|1.2500  |  0.3125 \n",
      "  [0 0 1 1 1 1]   | 0.6250 |   0.8576  | 1.0000 | 0.0000    |1.2500|1.2500  |  0.0000 \n",
      "  [0 1 0 0 0 0]   | 0.6875 |   0.8437  | 0.8438 | 0.0003    |1.5625|0.0000  |  1.5625 \n",
      "  [0 1 0 0 0 1]   | 0.6250 |   0.8125  | 0.8125 | 0.0036    |1.8750|0.0000  |  1.8750 \n",
      "  [0 1 0 0 1 0]   | 0.3750 |   0.6930  | 1.0000 | 0.0000    |3.1250|3.1250  |  0.0000 \n",
      "  [0 1 0 0 1 1]   | 0.3125 |   0.7074  | 1.0000 | 0.0000    |2.8125|2.8125  |  0.0000 \n",
      "  [0 1 0 1 0 0]   | 0.0625 |   0.6013  | 1.0000 | 0.0000    |4.6875|4.6875  |  0.0000 \n",
      "  [0 1 0 1 0 1]   | 0.0000 |   0.5840  | 1.0000 | 0.0000    |5.0000|5.0000  |  0.0000 \n",
      "  [0 1 0 1 1 0]   | 0.2500 |   0.6577  | 0.9688 | 0.0020    |3.7500|3.4375  |  0.3125 \n",
      "  [0 1 0 1 1 1]   | 0.3125 |   0.6874  | 1.0000 | 0.0000    |3.4375|3.4375  |  0.0000 \n",
      ">>[0 1 1 0 0 0]   | 0.0625 |   0.7551  | 0.9688 | 0.0019    |2.1875|1.8750  |  0.3125 \n",
      "  [0 1 1 0 0 1]   | 0.0000 |   0.7255  | 0.9375 | 0.0037    |2.5000|1.8750  |  0.6250 \n",
      ">>[0 1 1 0 1 0]   | 0.2500 |   0.6577  | 0.9688 | 0.0020    |3.7500|3.4375  |  0.3125 \n",
      "  [0 1 1 0 1 1]   | 0.3125 |   0.6752  | 0.9688 | 0.0020    |3.4375|3.1250  |  0.3125 \n",
      "  [0 1 1 1 0 0]   | 0.5625 |   0.8265  | 0.9688 | 0.0034    |1.5625|1.2500  |  0.3125 \n",
      "  [0 1 1 1 0 1]   | 0.6250 |   0.8125  | 0.8125 | 0.0036    |1.8750|0.0000  |  1.8750 \n",
      "  [0 1 1 1 1 0]   | 0.8750 |   0.9375  | 0.9375 | 0.0070    |0.6250|0.0000  |  0.6250 \n",
      "  [0 1 1 1 1 1]   | 0.9375 |   0.9687  | 0.9688 | 0.0035    |0.3125|0.0000  |  0.3125 \n",
      "  [1 0 0 0 0 0]   | 0.9375 |   0.9687  | 0.9688 | 0.0035    |0.3125|0.0000  |  0.3125 \n",
      "  [1 0 0 0 0 1]   | 0.8750 |   0.9375  | 0.9375 | 0.0070    |0.6250|0.0000  |  0.6250 \n",
      "  [1 0 0 0 1 0]   | 0.6250 |   0.8125  | 0.8125 | 0.0036    |1.8750|0.0000  |  1.8750 \n",
      "  [1 0 0 0 1 1]   | 0.5625 |   0.8265  | 0.9688 | 0.0034    |1.5625|1.2500  |  0.3125 \n",
      "  [1 0 0 1 0 0]   | 0.3125 |   0.6752  | 0.9688 | 0.0020    |3.4375|3.1250  |  0.3125 \n",
      "  [1 0 0 1 0 1]   | 0.2500 |   0.6577  | 0.9688 | 0.0020    |3.7500|3.4375  |  0.3125 \n",
      ">>[1 0 0 1 1 0]   | 0.0000 |   0.7255  | 0.9375 | 0.0037    |2.5000|1.8750  |  0.6250 \n",
      "  [1 0 0 1 1 1]   | 0.0625 |   0.7551  | 0.9688 | 0.0019    |2.1875|1.8750  |  0.3125 \n",
      "  [1 0 1 0 0 0]   | 0.3125 |   0.6874  | 1.0000 | 0.0000    |3.4375|3.4375  |  0.0000 \n",
      "  [1 0 1 0 0 1]   | 0.2500 |   0.6577  | 0.9688 | 0.0020    |3.7500|3.4375  |  0.3125 \n",
      "  [1 0 1 0 1 0]   | 0.0000 |   0.5840  | 1.0000 | 0.0000    |5.0000|5.0000  |  0.0000 \n",
      "  [1 0 1 0 1 1]   | 0.0625 |   0.6013  | 1.0000 | 0.0000    |4.6875|4.6875  |  0.0000 \n",
      "  [1 0 1 1 0 0]   | 0.3125 |   0.7074  | 1.0000 | 0.0000    |2.8125|2.8125  |  0.0000 \n",
      "  [1 0 1 1 0 1]   | 0.3750 |   0.6930  | 1.0000 | 0.0000    |3.1250|3.1250  |  0.0000 \n",
      "  [1 0 1 1 1 0]   | 0.6250 |   0.8125  | 0.8125 | 0.0036    |1.8750|0.0000  |  1.8750 \n",
      "  [1 0 1 1 1 1]   | 0.6875 |   0.8437  | 0.8438 | 0.0003    |1.5625|0.0000  |  1.5625 \n",
      "  [1 1 0 0 0 0]   | 0.6250 |   0.8576  | 1.0000 | 0.0000    |1.2500|1.2500  |  0.0000 \n",
      "  [1 1 0 0 0 1]   | 0.5625 |   0.8265  | 0.9688 | 0.0034    |1.5625|1.2500  |  0.3125 \n",
      "  [1 1 0 0 1 0]   | 0.3125 |   0.7074  | 1.0000 | 0.0000    |2.8125|2.8125  |  0.0000 \n",
      "  [1 1 0 0 1 1]   | 0.2500 |   0.7219  | 1.0000 | 0.0000    |2.5000|2.5000  |  0.0000 \n",
      "  [1 1 0 1 0 0]   | 0.0000 |   0.6185  | 1.0000 | 0.0000    |4.3750|4.3750  |  0.0000 \n",
      "  [1 1 0 1 0 1]   | 0.0625 |   0.6013  | 1.0000 | 0.0000    |4.6875|4.6875  |  0.0000 \n",
      "  [1 1 0 1 1 0]   | 0.3125 |   0.6752  | 0.9688 | 0.0020    |3.4375|3.1250  |  0.3125 \n",
      "  [1 1 0 1 1 1]   | 0.3750 |   0.7049  | 1.0000 | 0.0000    |3.1250|3.1250  |  0.0000 \n",
      "  [1 1 1 0 0 0]   | 0.0000 |   0.7847  | 1.0000 | 0.0000    |1.8750|1.8750  |  0.0000 \n",
      "  [1 1 1 0 0 1]   | 0.0625 |   0.7551  | 0.9688 | 0.0019    |2.1875|1.8750  |  0.3125 \n",
      "  [1 1 1 0 1 0]   | 0.3125 |   0.6874  | 1.0000 | 0.0000    |3.4375|3.4375  |  0.0000 \n",
      "  [1 1 1 0 1 1]   | 0.3750 |   0.7049  | 1.0000 | 0.0000    |3.1250|3.1250  |  0.0000 \n",
      "  [1 1 1 1 0 0]   | 0.6250 |   0.8576  | 1.0000 | 0.0000    |1.2500|1.2500  |  0.0000 \n",
      "  [1 1 1 1 0 1]   | 0.6875 |   0.8437  | 0.8438 | 0.0003    |1.5625|0.0000  |  1.5625 \n",
      "  [1 1 1 1 1 0]   | 0.9375 |   0.9687  | 0.9688 | 0.0035    |0.3125|0.0000  |  0.3125 \n",
      "  [1 1 1 1 1 1]   | 1.0000 |   1.0000  | 1.0000 | 0.0000    |0.0000|0.0000  |  0.0000 \n",
      "\n",
      "!!!p=0.2\n",
      "boolean signature | imbal? | nacc(fN*) | taccfN* |naccfN*-naccf| S(f) | S(fN*) | S(f) - S(fN*)\n",
      "--------------------------------------------------------------------------\n",
      "  [0 0 0 0 0 0]   | 1.0000 |   1.0000  | 1.0000 | 0.0000    |0.0000|0.0000  |  0.0000 \n",
      "  [0 0 0 0 0 1]   | 0.9375 |   0.9688  | 0.9688 | 0.0108    |0.3125|0.0000  |  0.3125 \n",
      "  [0 0 0 0 1 0]   | 0.6875 |   0.8438  | 0.8438 | 0.0282    |1.5625|0.0000  |  1.5625 \n",
      "  [0 0 0 0 1 1]   | 0.6250 |   0.8273  | 0.8438 | 0.0026    |1.2500|0.3125  |  0.9375 \n",
      "  [0 0 0 1 0 0]   | 0.3750 |   0.6875  | 0.6875 | 0.0285    |3.1250|0.0000  |  3.1250 \n",
      "  [0 0 0 1 0 1]   | 0.3125 |   0.6583  | 0.6875 | 0.0157    |3.4375|0.3125  |  3.1250 \n",
      "  [0 0 0 1 1 0]   | 0.0625 |   0.7057  | 0.9688 | 0.0071    |2.1875|1.8750  |  0.3125 \n",
      "  [0 0 0 1 1 1]   | 0.0000 |   0.7333  | 1.0000 | 0.0000    |1.8750|1.8750  |  0.0000 \n",
      "  [0 0 1 0 0 0]   | 0.3750 |   0.6875  | 0.6875 | 0.0285    |3.1250|0.0000  |  3.1250 \n",
      "  [0 0 1 0 0 1]   | 0.3125 |   0.6563  | 0.6562 | 0.0329    |3.4375|0.0000  |  3.4375 \n",
      "  [0 0 1 0 1 0]   | 0.0625 |   0.5545  | 1.0000 | 0.0000    |4.6875|4.6875  |  0.0000 \n",
      "  [0 0 1 0 1 1]   | 0.0000 |   0.5701  | 1.0000 | 0.0000    |4.3750|4.3750  |  0.0000 \n",
      "  [0 0 1 1 0 0]   | 0.2500 |   0.6640  | 1.0000 | 0.0000    |2.5000|2.5000  |  0.0000 \n",
      "  [0 0 1 1 0 1]   | 0.3125 |   0.6753  | 0.8438 | 0.0213    |2.8125|1.2500  |  1.5625 \n",
      "  [0 0 1 1 1 0]   | 0.5625 |   0.7961  | 0.8125 | 0.0126    |1.5625|0.3125  |  1.2500 \n",
      "  [0 0 1 1 1 1]   | 0.6250 |   0.8273  | 0.8438 | 0.0027    |1.2500|0.3125  |  0.9375 \n",
      "  [0 1 0 0 0 0]   | 0.6875 |   0.8438  | 0.8438 | 0.0283    |1.5625|0.0000  |  1.5625 \n",
      "  [0 1 0 0 0 1]   | 0.6250 |   0.8125  | 0.8125 | 0.0382    |1.8750|0.0000  |  1.8750 \n",
      "  [0 1 0 0 1 0]   | 0.3750 |   0.6875  | 0.6875 | 0.0435    |3.1250|0.0000  |  3.1250 \n",
      "  [0 1 0 0 1 1]   | 0.3125 |   0.6753  | 0.8438 | 0.0214    |2.8125|1.2500  |  1.5625 \n",
      "  [0 1 0 1 0 0]   | 0.0625 |   0.5545  | 1.0000 | 0.0000    |4.6875|4.6875  |  0.0000 \n",
      "  [0 1 0 1 0 1]   | 0.0000 |   0.5389  | 1.0000 | 0.0000    |5.0000|5.0000  |  0.0000 \n",
      "  [0 1 0 1 1 0]   | 0.2500 |   0.6271  | 0.6562 | 0.0200    |3.7500|0.3125  |  3.4375 \n",
      "  [0 1 0 1 1 1]   | 0.3125 |   0.6583  | 0.6875 | 0.0157    |3.4375|0.3125  |  3.1250 \n",
      ">>[0 1 1 0 0 0]   | 0.0625 |   0.7056  | 0.9688 | 0.0071    |2.1875|1.8750  |  0.3125 \n",
      "  [0 1 1 0 0 1]   | 0.0000 |   0.6780  | 0.9375 | 0.0143    |2.5000|1.8750  |  0.6250 \n",
      ">>[0 1 1 0 1 0]   | 0.2500 |   0.6271  | 0.6562 | 0.0200    |3.7500|0.3125  |  3.4375 \n",
      "  [0 1 1 0 1 1]   | 0.3125 |   0.6563  | 0.6562 | 0.0329    |3.4375|0.0000  |  3.4375 \n",
      "  [0 1 1 1 0 0]   | 0.5625 |   0.7961  | 0.8125 | 0.0126    |1.5625|0.3125  |  1.2500 \n",
      "  [0 1 1 1 0 1]   | 0.6250 |   0.8125  | 0.8125 | 0.0382    |1.8750|0.0000  |  1.8750 \n",
      "  [0 1 1 1 1 0]   | 0.8750 |   0.9375  | 0.9375 | 0.0215    |0.6250|0.0000  |  0.6250 \n",
      "  [0 1 1 1 1 1]   | 0.9375 |   0.9688  | 0.9688 | 0.0108    |0.3125|0.0000  |  0.3125 \n",
      "  [1 0 0 0 0 0]   | 0.9375 |   0.9688  | 0.9688 | 0.0108    |0.3125|0.0000  |  0.3125 \n",
      "  [1 0 0 0 0 1]   | 0.8750 |   0.9375  | 0.9375 | 0.0215    |0.6250|0.0000  |  0.6250 \n",
      "  [1 0 0 0 1 0]   | 0.6250 |   0.8125  | 0.8125 | 0.0382    |1.8750|0.0000  |  1.8750 \n",
      "  [1 0 0 0 1 1]   | 0.5625 |   0.7961  | 0.8125 | 0.0126    |1.5625|0.3125  |  1.2500 \n",
      "  [1 0 0 1 0 0]   | 0.3125 |   0.6563  | 0.6562 | 0.0329    |3.4375|0.0000  |  3.4375 \n",
      "  [1 0 0 1 0 1]   | 0.2500 |   0.6271  | 0.6562 | 0.0200    |3.7500|0.3125  |  3.4375 \n",
      ">>[1 0 0 1 1 0]   | 0.0000 |   0.6780  | 0.9375 | 0.0143    |2.5000|1.8750  |  0.6250 \n",
      "  [1 0 0 1 1 1]   | 0.0625 |   0.7056  | 0.9688 | 0.0071    |2.1875|1.8750  |  0.3125 \n",
      "  [1 0 1 0 0 0]   | 0.3125 |   0.6583  | 0.6875 | 0.0157    |3.4375|0.3125  |  3.1250 \n",
      "  [1 0 1 0 0 1]   | 0.2500 |   0.6271  | 0.6562 | 0.0200    |3.7500|0.3125  |  3.4375 \n",
      "  [1 0 1 0 1 0]   | 0.0000 |   0.5389  | 1.0000 | 0.0000    |5.0000|5.0000  |  0.0000 \n",
      "  [1 0 1 0 1 1]   | 0.0625 |   0.5545  | 1.0000 | 0.0000    |4.6875|4.6875  |  0.0000 \n",
      "  [1 0 1 1 0 0]   | 0.3125 |   0.6753  | 0.8438 | 0.0214    |2.8125|1.2500  |  1.5625 \n",
      "  [1 0 1 1 0 1]   | 0.3750 |   0.6875  | 0.6875 | 0.0435    |3.1250|0.0000  |  3.1250 \n",
      "  [1 0 1 1 1 0]   | 0.6250 |   0.8125  | 0.8125 | 0.0382    |1.8750|0.0000  |  1.8750 \n",
      "  [1 0 1 1 1 1]   | 0.6875 |   0.8438  | 0.8438 | 0.0283    |1.5625|0.0000  |  1.5625 \n",
      "  [1 1 0 0 0 0]   | 0.6250 |   0.8273  | 0.8438 | 0.0027    |1.2500|0.3125  |  0.9375 \n",
      "  [1 1 0 0 0 1]   | 0.5625 |   0.7961  | 0.8125 | 0.0126    |1.5625|0.3125  |  1.2500 \n",
      "  [1 1 0 0 1 0]   | 0.3125 |   0.6753  | 0.8438 | 0.0213    |2.8125|1.2500  |  1.5625 \n",
      "  [1 1 0 0 1 1]   | 0.2500 |   0.6640  | 1.0000 | 0.0000    |2.5000|2.5000  |  0.0000 \n",
      "  [1 1 0 1 0 0]   | 0.0000 |   0.5701  | 1.0000 | 0.0000    |4.3750|4.3750  |  0.0000 \n",
      "  [1 1 0 1 0 1]   | 0.0625 |   0.5545  | 1.0000 | 0.0000    |4.6875|4.6875  |  0.0000 \n",
      "  [1 1 0 1 1 0]   | 0.3125 |   0.6563  | 0.6562 | 0.0329    |3.4375|0.0000  |  3.4375 \n",
      "  [1 1 0 1 1 1]   | 0.3750 |   0.6875  | 0.6875 | 0.0285    |3.1250|0.0000  |  3.1250 \n",
      "  [1 1 1 0 0 0]   | 0.0000 |   0.7333  | 1.0000 | 0.0000    |1.8750|1.8750  |  0.0000 \n",
      "  [1 1 1 0 0 1]   | 0.0625 |   0.7057  | 0.9688 | 0.0071    |2.1875|1.8750  |  0.3125 \n",
      "  [1 1 1 0 1 0]   | 0.3125 |   0.6583  | 0.6875 | 0.0157    |3.4375|0.3125  |  3.1250 \n",
      "  [1 1 1 0 1 1]   | 0.3750 |   0.6875  | 0.6875 | 0.0285    |3.1250|0.0000  |  3.1250 \n",
      "  [1 1 1 1 0 0]   | 0.6250 |   0.8273  | 0.8438 | 0.0026    |1.2500|0.3125  |  0.9375 \n",
      "  [1 1 1 1 0 1]   | 0.6875 |   0.8438  | 0.8438 | 0.0282    |1.5625|0.0000  |  1.5625 \n",
      "  [1 1 1 1 1 0]   | 0.9375 |   0.9688  | 0.9688 | 0.0108    |0.3125|0.0000  |  0.3125 \n",
      "  [1 1 1 1 1 1]   | 1.0000 |   1.0000  | 1.0000 | 0.0000    |0.0000|0.0000  |  0.0000 \n",
      "\n",
      "!!!p=0.25\n",
      "boolean signature | imbal? | nacc(fN*) | taccfN* |naccfN*-naccf| S(f) | S(fN*) | S(f) - S(fN*)\n",
      "--------------------------------------------------------------------------\n",
      "  [0 0 0 0 0 0]   | 1.0000 |   1.0000  | 1.0000 | 0.0000    |0.0000|0.0000  |  0.0000 \n",
      "  [0 0 0 0 0 1]   | 0.9375 |   0.9688  | 0.9688 | 0.0164    |0.3125|0.0000  |  0.3125 \n",
      "  [0 0 0 0 1 0]   | 0.6875 |   0.8438  | 0.8438 | 0.0491    |1.5625|0.0000  |  1.5625 \n",
      "  [0 0 0 0 1 1]   | 0.6250 |   0.8208  | 0.8438 | 0.0244    |1.2500|0.3125  |  0.9375 \n",
      "  [0 0 0 1 0 0]   | 0.3750 |   0.6875  | 0.6875 | 0.0598    |3.1250|0.0000  |  3.1250 \n",
      "  [0 0 0 1 0 1]   | 0.3125 |   0.6563  | 0.6875 | 0.0433    |3.4375|0.3125  |  3.1250 \n",
      "  [0 0 0 1 1 0]   | 0.0625 |   0.6630  | 0.9688 | 0.0099    |2.1875|1.8750  |  0.3125 \n",
      "  [0 0 0 1 1 1]   | 0.0000 |   0.6877  | 1.0000 | 0.0000    |1.8750|1.8750  |  0.0000 \n",
      "  [0 0 1 0 0 0]   | 0.3750 |   0.6875  | 0.6875 | 0.0598    |3.1250|0.0000  |  3.1250 \n",
      "  [0 0 1 0 0 1]   | 0.3125 |   0.6562  | 0.6562 | 0.0652    |3.4375|0.0000  |  3.4375 \n",
      "  [0 0 1 0 1 0]   | 0.0625 |   0.5352  | 0.6875 | 0.0067    |4.6875|1.5625  |  3.1250 \n",
      "  [0 0 1 0 1 1]   | 0.0000 |   0.5437  | 0.3750 | 0.0024    |4.3750|1.8750  |  2.5000 \n",
      "  [0 0 1 1 0 0]   | 0.2500 |   0.6436  | 0.6875 | 0.0244    |2.5000|0.6250  |  1.8750 \n",
      "  [0 0 1 1 0 1]   | 0.3125 |   0.6655  | 0.6875 | 0.0500    |2.8125|0.3125  |  2.5000 \n",
      "  [0 0 1 1 1 0]   | 0.5625 |   0.7896  | 0.8125 | 0.0389    |1.5625|0.3125  |  1.2500 \n",
      "  [0 0 1 1 1 1]   | 0.6250 |   0.8208  | 0.8438 | 0.0244    |1.2500|0.3125  |  0.9375 \n",
      "  [0 1 0 0 0 0]   | 0.6875 |   0.8438  | 0.8438 | 0.0491    |1.5625|0.0000  |  1.5625 \n",
      "  [0 1 0 0 0 1]   | 0.6250 |   0.8125  | 0.8125 | 0.0637    |1.8750|0.0000  |  1.8750 \n",
      "  [0 1 0 0 1 0]   | 0.3750 |   0.6875  | 0.6875 | 0.0757    |3.1250|0.0000  |  3.1250 \n",
      "  [0 1 0 0 1 1]   | 0.3125 |   0.6655  | 0.6875 | 0.0500    |2.8125|0.3125  |  2.5000 \n",
      "  [0 1 0 1 0 0]   | 0.0625 |   0.5352  | 0.6875 | 0.0067    |4.6875|1.5625  |  3.1250 \n",
      "  [0 1 0 1 0 1]   | 0.0000 |   0.5156  | 1.0000 | 0.0000    |5.0000|5.0000  |  0.0000 \n",
      "  [0 1 0 1 1 0]   | 0.2500 |   0.6251  | 0.6562 | 0.0487    |3.7500|0.3125  |  3.4375 \n",
      "  [0 1 0 1 1 1]   | 0.3125 |   0.6563  | 0.6875 | 0.0433    |3.4375|0.3125  |  3.1250 \n",
      ">>[0 1 1 0 0 0]   | 0.0625 |   0.6630  | 0.9688 | 0.0099    |2.1875|1.8750  |  0.3125 \n",
      "  [0 1 1 0 0 1]   | 0.0000 |   0.6382  | 0.9375 | 0.0200    |2.5000|1.8750  |  0.6250 \n",
      ">>[0 1 1 0 1 0]   | 0.2500 |   0.6251  | 0.6562 | 0.0487    |3.7500|0.3125  |  3.4375 \n",
      "  [0 1 1 0 1 1]   | 0.3125 |   0.6562  | 0.6562 | 0.0652    |3.4375|0.0000  |  3.4375 \n",
      "  [0 1 1 1 0 0]   | 0.5625 |   0.7896  | 0.8125 | 0.0389    |1.5625|0.3125  |  1.2500 \n",
      "  [0 1 1 1 0 1]   | 0.6250 |   0.8125  | 0.8125 | 0.0637    |1.8750|0.0000  |  1.8750 \n",
      "  [0 1 1 1 1 0]   | 0.8750 |   0.9375  | 0.9375 | 0.0327    |0.6250|0.0000  |  0.6250 \n",
      "  [0 1 1 1 1 1]   | 0.9375 |   0.9688  | 0.9688 | 0.0164    |0.3125|0.0000  |  0.3125 \n",
      "  [1 0 0 0 0 0]   | 0.9375 |   0.9688  | 0.9688 | 0.0164    |0.3125|0.0000  |  0.3125 \n",
      "  [1 0 0 0 0 1]   | 0.8750 |   0.9375  | 0.9375 | 0.0327    |0.6250|0.0000  |  0.6250 \n",
      "  [1 0 0 0 1 0]   | 0.6250 |   0.8125  | 0.8125 | 0.0637    |1.8750|0.0000  |  1.8750 \n",
      "  [1 0 0 0 1 1]   | 0.5625 |   0.7896  | 0.8125 | 0.0389    |1.5625|0.3125  |  1.2500 \n",
      "  [1 0 0 1 0 0]   | 0.3125 |   0.6562  | 0.6562 | 0.0652    |3.4375|0.0000  |  3.4375 \n",
      "  [1 0 0 1 0 1]   | 0.2500 |   0.6251  | 0.6562 | 0.0487    |3.7500|0.3125  |  3.4375 \n",
      ">>[1 0 0 1 1 0]   | 0.0000 |   0.6382  | 0.9375 | 0.0200    |2.5000|1.8750  |  0.6250 \n",
      "  [1 0 0 1 1 1]   | 0.0625 |   0.6630  | 0.9688 | 0.0099    |2.1875|1.8750  |  0.3125 \n",
      "  [1 0 1 0 0 0]   | 0.3125 |   0.6563  | 0.6875 | 0.0433    |3.4375|0.3125  |  3.1250 \n",
      "  [1 0 1 0 0 1]   | 0.2500 |   0.6251  | 0.6562 | 0.0487    |3.7500|0.3125  |  3.4375 \n",
      "  [1 0 1 0 1 0]   | 0.0000 |   0.5156  | 1.0000 | 0.0000    |5.0000|5.0000  |  0.0000 \n",
      "  [1 0 1 0 1 1]   | 0.0625 |   0.5352  | 0.6875 | 0.0067    |4.6875|1.5625  |  3.1250 \n",
      "  [1 0 1 1 0 0]   | 0.3125 |   0.6655  | 0.6875 | 0.0500    |2.8125|0.3125  |  2.5000 \n",
      "  [1 0 1 1 0 1]   | 0.3750 |   0.6875  | 0.6875 | 0.0757    |3.1250|0.0000  |  3.1250 \n",
      "  [1 0 1 1 1 0]   | 0.6250 |   0.8125  | 0.8125 | 0.0637    |1.8750|0.0000  |  1.8750 \n",
      "  [1 0 1 1 1 1]   | 0.6875 |   0.8438  | 0.8438 | 0.0491    |1.5625|0.0000  |  1.5625 \n",
      "  [1 1 0 0 0 0]   | 0.6250 |   0.8208  | 0.8438 | 0.0244    |1.2500|0.3125  |  0.9375 \n",
      "  [1 1 0 0 0 1]   | 0.5625 |   0.7896  | 0.8125 | 0.0389    |1.5625|0.3125  |  1.2500 \n",
      "  [1 1 0 0 1 0]   | 0.3125 |   0.6655  | 0.6875 | 0.0500    |2.8125|0.3125  |  2.5000 \n",
      "  [1 1 0 0 1 1]   | 0.2500 |   0.6436  | 0.6875 | 0.0244    |2.5000|0.6250  |  1.8750 \n",
      "  [1 1 0 1 0 0]   | 0.0000 |   0.5437  | 0.3750 | 0.0024    |4.3750|1.8750  |  2.5000 \n",
      "  [1 1 0 1 0 1]   | 0.0625 |   0.5352  | 0.6875 | 0.0067    |4.6875|1.5625  |  3.1250 \n",
      "  [1 1 0 1 1 0]   | 0.3125 |   0.6562  | 0.6562 | 0.0652    |3.4375|0.0000  |  3.4375 \n",
      "  [1 1 0 1 1 1]   | 0.3750 |   0.6875  | 0.6875 | 0.0598    |3.1250|0.0000  |  3.1250 \n",
      "  [1 1 1 0 0 0]   | 0.0000 |   0.6877  | 1.0000 | 0.0000    |1.8750|1.8750  |  0.0000 \n",
      "  [1 1 1 0 0 1]   | 0.0625 |   0.6630  | 0.9688 | 0.0099    |2.1875|1.8750  |  0.3125 \n",
      "  [1 1 1 0 1 0]   | 0.3125 |   0.6563  | 0.6875 | 0.0433    |3.4375|0.3125  |  3.1250 \n",
      "  [1 1 1 0 1 1]   | 0.3750 |   0.6875  | 0.6875 | 0.0598    |3.1250|0.0000  |  3.1250 \n",
      "  [1 1 1 1 0 0]   | 0.6250 |   0.8208  | 0.8438 | 0.0244    |1.2500|0.3125  |  0.9375 \n",
      "  [1 1 1 1 0 1]   | 0.6875 |   0.8438  | 0.8438 | 0.0491    |1.5625|0.0000  |  1.5625 \n",
      "  [1 1 1 1 1 0]   | 0.9375 |   0.9688  | 0.9688 | 0.0164    |0.3125|0.0000  |  0.3125 \n",
      "  [1 1 1 1 1 1]   | 1.0000 |   1.0000  | 1.0000 | 0.0000    |0.0000|0.0000  |  0.0000 \n",
      "\n",
      "!!!p=0.3\n",
      "boolean signature | imbal? | nacc(fN*) | taccfN* |naccfN*-naccf| S(f) | S(fN*) | S(f) - S(fN*)\n",
      "--------------------------------------------------------------------------\n",
      "  [0 0 0 0 0 0]   | 1.0000 |   1.0000  | 1.0000 | 0.0000    |0.0000|0.0000  |  0.0000 \n",
      "  [0 0 0 0 0 1]   | 0.9375 |   0.9687  | 0.9688 | 0.0207    |0.3125|0.0000  |  0.3125 \n",
      "  [0 0 0 0 1 0]   | 0.6875 |   0.8437  | 0.8438 | 0.0651    |1.5625|0.0000  |  1.5625 \n",
      "  [0 0 0 0 1 1]   | 0.6250 |   0.8143  | 0.8438 | 0.0426    |1.2500|0.3125  |  0.9375 \n",
      "  [0 0 0 1 0 0]   | 0.3750 |   0.6875  | 0.6875 | 0.0811    |3.1250|0.0000  |  3.1250 \n",
      "  [0 0 0 1 0 1]   | 0.3125 |   0.6562  | 0.6562 | 0.0632    |3.4375|0.0000  |  3.4375 \n",
      "  [0 0 0 1 1 0]   | 0.0625 |   0.6253  | 0.9688 | 0.0106    |2.1875|1.8750  |  0.3125 \n",
      "  [0 0 0 1 1 1]   | 0.0000 |   0.6463  | 1.0000 | 0.0000    |1.8750|1.8750  |  0.0000 \n",
      "  [0 0 1 0 0 0]   | 0.3750 |   0.6875  | 0.6875 | 0.0811    |3.1250|0.0000  |  3.1250 \n",
      "  [0 0 1 0 0 1]   | 0.3125 |   0.6562  | 0.6562 | 0.0853    |3.4375|0.0000  |  3.4375 \n",
      "  [0 0 1 0 1 0]   | 0.0625 |   0.5312  | 0.5312 | 0.0163    |4.6875|0.0000  |  4.6875 \n",
      "  [0 0 1 0 1 1]   | 0.0000 |   0.5402  | 0.3750 | 0.0157    |4.3750|1.8750  |  2.5000 \n",
      "  [0 0 1 1 0 0]   | 0.2500 |   0.6324  | 0.6875 | 0.0471    |2.5000|0.6250  |  1.8750 \n",
      "  [0 0 1 1 0 1]   | 0.3125 |   0.6598  | 0.6875 | 0.0714    |2.8125|0.3125  |  2.5000 \n",
      "  [0 0 1 1 1 0]   | 0.5625 |   0.7832  | 0.8125 | 0.0597    |1.5625|0.3125  |  1.2500 \n",
      "  [0 0 1 1 1 1]   | 0.6250 |   0.8143  | 0.8438 | 0.0426    |1.2500|0.3125  |  0.9375 \n",
      "  [0 1 0 0 0 0]   | 0.6875 |   0.8437  | 0.8438 | 0.0651    |1.5625|0.0000  |  1.5625 \n",
      "  [0 1 0 0 0 1]   | 0.6250 |   0.8125  | 0.8125 | 0.0823    |1.8750|0.0000  |  1.8750 \n",
      "  [0 1 0 0 1 0]   | 0.3750 |   0.6875  | 0.6875 | 0.0957    |3.1250|0.0000  |  3.1250 \n",
      "  [0 1 0 0 1 1]   | 0.3125 |   0.6598  | 0.6875 | 0.0714    |2.8125|0.3125  |  2.5000 \n",
      "  [0 1 0 1 0 0]   | 0.0625 |   0.5312  | 0.5312 | 0.0163    |4.6875|0.0000  |  4.6875 \n",
      "  [0 1 0 1 0 1]   | 0.0000 |   0.5051  | 1.0000 | 0.0000    |5.0000|5.0000  |  0.0000 \n",
      "  [0 1 0 1 1 0]   | 0.2500 |   0.6250  | 0.6250 | 0.0671    |3.7500|0.0000  |  3.7500 \n",
      "  [0 1 0 1 1 1]   | 0.3125 |   0.6562  | 0.6562 | 0.0632    |3.4375|0.0000  |  3.4375 \n",
      ">>[0 1 1 0 0 0]   | 0.0625 |   0.6253  | 0.9688 | 0.0106    |2.1875|1.8750  |  0.3125 \n",
      "  [0 1 1 0 0 1]   | 0.0000 |   0.6042  | 0.9375 | 0.0214    |2.5000|1.8750  |  0.6250 \n",
      ">>[0 1 1 0 1 0]   | 0.2500 |   0.6250  | 0.6250 | 0.0671    |3.7500|0.0000  |  3.7500 \n",
      "  [0 1 1 0 1 1]   | 0.3125 |   0.6562  | 0.6562 | 0.0853    |3.4375|0.0000  |  3.4375 \n",
      "  [0 1 1 1 0 0]   | 0.5625 |   0.7832  | 0.8125 | 0.0597    |1.5625|0.3125  |  1.2500 \n",
      "  [0 1 1 1 0 1]   | 0.6250 |   0.8125  | 0.8125 | 0.0823    |1.8750|0.0000  |  1.8750 \n",
      "  [0 1 1 1 1 0]   | 0.8750 |   0.9375  | 0.9375 | 0.0412    |0.6250|0.0000  |  0.6250 \n",
      "  [0 1 1 1 1 1]   | 0.9375 |   0.9687  | 0.9688 | 0.0207    |0.3125|0.0000  |  0.3125 \n",
      "  [1 0 0 0 0 0]   | 0.9375 |   0.9687  | 0.9688 | 0.0207    |0.3125|0.0000  |  0.3125 \n",
      "  [1 0 0 0 0 1]   | 0.8750 |   0.9375  | 0.9375 | 0.0412    |0.6250|0.0000  |  0.6250 \n",
      "  [1 0 0 0 1 0]   | 0.6250 |   0.8125  | 0.8125 | 0.0823    |1.8750|0.0000  |  1.8750 \n",
      "  [1 0 0 0 1 1]   | 0.5625 |   0.7832  | 0.8125 | 0.0597    |1.5625|0.3125  |  1.2500 \n",
      "  [1 0 0 1 0 0]   | 0.3125 |   0.6562  | 0.6562 | 0.0853    |3.4375|0.0000  |  3.4375 \n",
      "  [1 0 0 1 0 1]   | 0.2500 |   0.6250  | 0.6250 | 0.0671    |3.7500|0.0000  |  3.7500 \n",
      ">>[1 0 0 1 1 0]   | 0.0000 |   0.6042  | 0.9375 | 0.0214    |2.5000|1.8750  |  0.6250 \n",
      "  [1 0 0 1 1 1]   | 0.0625 |   0.6253  | 0.9688 | 0.0106    |2.1875|1.8750  |  0.3125 \n",
      "  [1 0 1 0 0 0]   | 0.3125 |   0.6562  | 0.6562 | 0.0632    |3.4375|0.0000  |  3.4375 \n",
      "  [1 0 1 0 0 1]   | 0.2500 |   0.6250  | 0.6250 | 0.0671    |3.7500|0.0000  |  3.7500 \n",
      "  [1 0 1 0 1 0]   | 0.0000 |   0.5051  | 1.0000 | 0.0000    |5.0000|5.0000  |  0.0000 \n",
      "  [1 0 1 0 1 1]   | 0.0625 |   0.5312  | 0.5312 | 0.0163    |4.6875|0.0000  |  4.6875 \n",
      "  [1 0 1 1 0 0]   | 0.3125 |   0.6598  | 0.6875 | 0.0714    |2.8125|0.3125  |  2.5000 \n",
      "  [1 0 1 1 0 1]   | 0.3750 |   0.6875  | 0.6875 | 0.0957    |3.1250|0.0000  |  3.1250 \n",
      "  [1 0 1 1 1 0]   | 0.6250 |   0.8125  | 0.8125 | 0.0823    |1.8750|0.0000  |  1.8750 \n",
      "  [1 0 1 1 1 1]   | 0.6875 |   0.8437  | 0.8438 | 0.0651    |1.5625|0.0000  |  1.5625 \n",
      "  [1 1 0 0 0 0]   | 0.6250 |   0.8143  | 0.8438 | 0.0426    |1.2500|0.3125  |  0.9375 \n",
      "  [1 1 0 0 0 1]   | 0.5625 |   0.7832  | 0.8125 | 0.0597    |1.5625|0.3125  |  1.2500 \n",
      "  [1 1 0 0 1 0]   | 0.3125 |   0.6598  | 0.6875 | 0.0714    |2.8125|0.3125  |  2.5000 \n",
      "  [1 1 0 0 1 1]   | 0.2500 |   0.6324  | 0.6875 | 0.0471    |2.5000|0.6250  |  1.8750 \n",
      "  [1 1 0 1 0 0]   | 0.0000 |   0.5402  | 0.3750 | 0.0157    |4.3750|1.8750  |  2.5000 \n",
      "  [1 1 0 1 0 1]   | 0.0625 |   0.5312  | 0.5312 | 0.0163    |4.6875|0.0000  |  4.6875 \n",
      "  [1 1 0 1 1 0]   | 0.3125 |   0.6562  | 0.6562 | 0.0853    |3.4375|0.0000  |  3.4375 \n",
      "  [1 1 0 1 1 1]   | 0.3750 |   0.6875  | 0.6875 | 0.0811    |3.1250|0.0000  |  3.1250 \n",
      "  [1 1 1 0 0 0]   | 0.0000 |   0.6463  | 1.0000 | 0.0000    |1.8750|1.8750  |  0.0000 \n",
      "  [1 1 1 0 0 1]   | 0.0625 |   0.6253  | 0.9688 | 0.0106    |2.1875|1.8750  |  0.3125 \n",
      "  [1 1 1 0 1 0]   | 0.3125 |   0.6562  | 0.6562 | 0.0632    |3.4375|0.0000  |  3.4375 \n",
      "  [1 1 1 0 1 1]   | 0.3750 |   0.6875  | 0.6875 | 0.0811    |3.1250|0.0000  |  3.1250 \n",
      "  [1 1 1 1 0 0]   | 0.6250 |   0.8143  | 0.8438 | 0.0426    |1.2500|0.3125  |  0.9375 \n",
      "  [1 1 1 1 0 1]   | 0.6875 |   0.8437  | 0.8438 | 0.0651    |1.5625|0.0000  |  1.5625 \n",
      "  [1 1 1 1 1 0]   | 0.9375 |   0.9687  | 0.9688 | 0.0207    |0.3125|0.0000  |  0.3125 \n",
      "  [1 1 1 1 1 1]   | 1.0000 |   1.0000  | 1.0000 | 0.0000    |0.0000|0.0000  |  0.0000 \n"
     ]
    }
   ],
   "source": [
    "# n = 5\n",
    "n = 5\n",
    "k = n\n",
    "# p = 0.2\n",
    "pvals = [0, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "\n",
    "X_arr = np.array(list(itertools.product([0, 1], repeat=n)))\n",
    "\n",
    "\n",
    "p_x = 1 / (2 ** k) # uniform distribution over x\n",
    "\n",
    "H = np.array(list(itertools.product([0, 1], repeat=n+1)))\n",
    "mine = [\n",
    "    [1,0,0,1,1,0],\n",
    "    [0,1,1,0,1,0],\n",
    "    [0,1,1,0,0,0]\n",
    "]\n",
    "# H = [[0, 1, 0, 0, 0]]\n",
    "\n",
    "for p in pvals:\n",
    "    print()\n",
    "    print(f\"!!!p={p}\")\n",
    "    print(\"boolean signature | imbal? | nacc(fN*) | taccfN* |naccfN*-naccf| S(f) | S(fN*) | S(f) - S(fN*)\")\n",
    "    print(\"--------------------------------------------------------------------------\")\n",
    "    for i, signature in enumerate(H):\n",
    "        sss = \"  \"\n",
    "        if list(signature) in mine:\n",
    "            sss = \">>\"\n",
    "        hash = dict(zip(range(n+1), signature))\n",
    "        func = lambda b: hash[sum(b)]\n",
    "        \n",
    "        # noisy_lookup[row,col] is the JOINT probability Pr(f(z)=row| x=col)\n",
    "        noisy_lookup = np.zeros((2, 2**n))\n",
    "        true_lookup = np.zeros((2, 2**n))\n",
    "        # simulate a noisy dataset essentially\n",
    "        for i, x in enumerate(product([0,1], repeat=k)):\n",
    "            func_value = func(x)\n",
    "            # true lookup is an array with 2 rows; there is a p_x at [row, column] if \n",
    "            # f[column] = row]. so, true_lookup[i, j] = pr(f(x) = i| x=j)\n",
    "            true_lookup[func(x), i] = 1\n",
    "            # iterate over all of the z values that contribute to \n",
    "            for e in product([0, 1], repeat=k):\n",
    "                z = np.array(x) ^ np.array(e)\n",
    "                p_x_given_z = p ** sum(e) * (1-p)**(k - sum(e))\n",
    "                # increment noisy_lookup at the binary index of z\n",
    "                # noisy_lookup[i, j] = pr(f(z) = i,  x=j) \n",
    "                noisy_lookup[func_value, int(''.join(map(str, z)), 2)] += p_x_given_z \n",
    "        \n",
    "        # the function is balanced if the sums of the two rows of true_lookup are equal\n",
    "        imbal = abs(true_lookup[0,:].sum() - true_lookup[1,:].sum())  / 2 ** n\n",
    "        \n",
    "        # if not balanced:\n",
    "        #     continue\n",
    "        # round up to get argmax \n",
    "        noisy_mle = np.round(noisy_lookup)  \n",
    "        out = np.multiply(noisy_mle, true_lookup) / 2 ** n # \"inner product\" of the functions\n",
    "        noiseless_fnstar_acc = out.sum()\n",
    "\n",
    "\n",
    "        fnstar_dct = {}\n",
    "        for i, x in enumerate(X_arr):\n",
    "            fnstar_dct[tuple(x)] = np.argmax(noisy_lookup[:, i])\n",
    "        def fnstar(x):\n",
    "            return fnstar_dct[tuple(x)]\n",
    "        \n",
    "        sensitivity_f = average_sensitivity(func, X_arr)\n",
    "        sensitivity_fnstar = average_sensitivity(fnstar, X_arr)\n",
    "        sensitivity_diff = sensitivity_f - sensitivity_fnstar\n",
    "        # accuracies on dataset\n",
    "        #  = compute_acc_test(fnstar, func, n) # accuracy of fN* on noiseless data\n",
    "\n",
    "        p_zy = generate_noisy_distr(k, p, func)\n",
    "        noisy_f_acc = compute_acc_noisytest(p_zy, func, n) # accuracy of f on noisy data\n",
    "        noisy_fnstar_acc = compute_acc_noisytest(p_zy, fnstar, n) # accuracy of fN* MLE on noisy data\n",
    "        nacc_diff = noisy_fnstar_acc - noisy_f_acc\n",
    "\n",
    "\n",
    "\n",
    "        print(f\"{sss}{signature}   | {imbal:0.4f} |   {noisy_fnstar_acc:1.4f}  | {noiseless_fnstar_acc:1.4f} | {nacc_diff:1.4f}    |{sensitivity_f:1.4f}|{sensitivity_fnstar:1.4f}  |  {sensitivity_diff:1.4f} \")\n",
    "        if sensitivity_fnstar == 0:\n",
    "            topr = sum(noisy_mle[0,:])\n",
    "            botr = sum(noisy_mle[1,:])\n",
    "            assert (np.allclose(topr, 0) or np.allclose(topr, 1 << n))\n",
    "            assert (np.allclose(botr, 0) or np.allclose(botr, 1 << n))\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(noisy_lookup[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(noisy_mle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01562539 0.         0.         0.01562539 0.         0.01562539\n",
      "  0.01562539 0.         0.         0.01562539 0.01562539 0.\n",
      "  0.01562539 0.         0.         0.01562539 0.         0.01562539\n",
      "  0.01562539 0.         0.01562539 0.         0.         0.01562539\n",
      "  0.01562539 0.         0.         0.01562539 0.         0.01562539\n",
      "  0.01562539 0.        ]\n",
      " [0.         0.01562539 0.01562539 0.         0.01562539 0.\n",
      "  0.         0.01562539 0.01562539 0.         0.         0.01562539\n",
      "  0.         0.01562539 0.01562539 0.         0.01562539 0.\n",
      "  0.         0.01562539 0.         0.01562539 0.01562539 0.\n",
      "  0.         0.01562539 0.01562539 0.         0.01562539 0.\n",
      "  0.         0.01562539]]\n",
      "0.5000124416\n",
      "[[0.03125 0.      0.      0.03125 0.      0.03125 0.03125 0.      0.\n",
      "  0.03125 0.03125 0.      0.03125 0.      0.      0.03125 0.      0.03125\n",
      "  0.03125 0.      0.03125 0.      0.      0.03125 0.03125 0.      0.\n",
      "  0.03125 0.      0.03125 0.03125 0.     ]\n",
      " [0.      0.03125 0.03125 0.      0.03125 0.      0.      0.03125 0.03125\n",
      "  0.      0.      0.03125 0.      0.03125 0.03125 0.      0.03125 0.\n",
      "  0.      0.03125 0.      0.03125 0.03125 0.      0.      0.03125 0.03125\n",
      "  0.      0.03125 0.      0.      0.03125]]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# We will build the conditional p_{X|Z^{n-1}}(x|z^{n-1}) one at a time for each x and z^{n-1}\n",
    "k = 5\n",
    "p = 0.44\n",
    "\n",
    "noisy_lookup = np.zeros((2, 2**k))\n",
    "true_lookup = np.zeros((2, 2**k))\n",
    "func = parity\n",
    "# simulate a noisy dataset essentially\n",
    "for i, x in enumerate(product([0,1], repeat=k)):\n",
    "    p_x = 1 / (2 ** k) \n",
    "    func_value = func(x)\n",
    "    true_lookup[func(x), i] = p_x\n",
    "    for e in product([0, 1], repeat=k):\n",
    "        p_e = p ** sum(e) * (1-p)**(k - sum(e))\n",
    "        z = np.array(x) ^ np.array(e)\n",
    "        p_z = p_x * p_e\n",
    "        # increment noisy_lookup at the binary index of z\n",
    "        noisy_lookup[func_value, int(''.join(map(str, z)), 2)] += p_e\n",
    "\n",
    "\n",
    "out = np.multiply(np.round(noisy_lookup), true_lookup)\n",
    "print(out)\n",
    "print(out.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03125 0.      0.      0.03125 0.      0.03125 0.03125 0.      0.\n",
      "  0.03125 0.03125 0.      0.03125 0.      0.      0.03125 0.      0.03125\n",
      "  0.03125 0.      0.03125 0.      0.      0.03125 0.03125 0.      0.\n",
      "  0.03125 0.      0.03125 0.03125 0.     ]\n",
      " [0.      0.03125 0.03125 0.      0.03125 0.      0.      0.03125 0.03125\n",
      "  0.      0.      0.03125 0.      0.03125 0.03125 0.      0.03125 0.\n",
      "  0.      0.03125 0.      0.03125 0.03125 0.      0.      0.03125 0.03125\n",
      "  0.      0.03125 0.      0.      0.03125]]\n",
      "1.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
