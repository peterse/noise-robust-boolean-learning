{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will study the convergence and \"error bars\" for the accuracy of $f_N^*$, the optimal function for noisy inputs, by evaluating these functions on finite datasets.\n",
    "\n",
    "The current experiment for counterexamples trains an SAN to find the lowest error function on a training (or validation!) set, call this $\\hat{f}_{train}$ or $\\hat{f}_{test}$. The definition of $\\hat{f}_{test}$ is that it minimizes error on a _specific_ test set, call it $D_N$ where $N$ is the number of data. This immediately gives two statistical fluctuations:\n",
    " 1. For different $D_N$, we get different $\\hat{f}_{test}$\n",
    " 2. Any specific $\\hat{f}_{test}$ will achieve different accuracies when evaluated on different $D_N$\n",
    "\n",
    "We want to show the (error, sensitivity) point that $\\hat{f}_{test}$ is _attracted towards_. What does this mean? Suppose that we have a function $h$ that we are computing (error, sens) for.\n",
    " - this coordinate should be(?) specific for the dataset: $\\hat{f}$ cannot \"see\" alternative datasets.\n",
    " - This coordinate could therefore be the error of the $h$ that achieves optimal performance on $D_N$\n",
    " - This coordinate _might not_ be the same as the coordinate for $f_N^*$\n",
    " - A plausible choice for $h$ should be the optimal function for the specific dataset Matheus is looking at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "counterexample = \"00100110\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
