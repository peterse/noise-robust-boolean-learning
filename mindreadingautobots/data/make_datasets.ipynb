{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make datasets to analyze\n",
    "\n",
    "Each dataset should have a training set `train.pkl`, a validation set `val.pkl`. If there is noise involved, there should be an additional `train_noiselss.pkl`, `dev_noiseless.pkl`.\n",
    "\n",
    "This notebook serves as a manifest for reproducing precisely the datasets in this directory. Data access notes:\n",
    " - The data directory name is how you will access this data using `dataloader.py` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload magic\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os \n",
    "from mindreadingautobots.sequence_generators import make_datasets, data_io\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hamilton path datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0]\n",
      "[0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0]\n",
      "[0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0]\n",
      "[0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0]\n",
      "[0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0]\n",
      "[0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0]\n",
      "[0 1 2 5]\n"
     ]
    }
   ],
   "source": [
    "out = make_datasets.k_choose_m_hamilton_forecast_dataset(k=6, m=4, n_data=300, n_bits=30, p_bitflip=0, seed=1237)\n",
    "X, Z, idx = out\n",
    "for x in X:\n",
    "    # find two entries with the first 6 bits the same\n",
    "    if np.all(x[0:6] == X[0,0:6]):\n",
    "        # print(X[0])\n",
    "        print(x)\n",
    "    # print(x)\n",
    "# tester = out[0]\n",
    "# dct = {}\n",
    "# for i in range(len(tester)):\n",
    "\n",
    "# print(out[0])\n",
    "print(out[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating hamilton_6_choose_4_nbits16_n2000_bf20_seed1234 with p_bitflip=0.2\n",
      "idx for sparse parity: save these: [1 2 3 5]\n"
     ]
    }
   ],
   "source": [
    "# Generate data with bitflip values\n",
    "n_val = 10000 # number of validation examples\n",
    "seed = 1234\n",
    "n_train = 2000\n",
    "n_bits = 16 # number of TOTAL bits\n",
    "# variables\n",
    "# p_bitflips = [0, 0.05, 0.1]\n",
    "p_bitflips = [0.2]\n",
    "\n",
    "def hamilton_6_choose_4(n_data, n_bits, p_bitflip, seed):\n",
    "    return make_datasets.k_choose_m_hamilton_forecast_dataset(k=6, m=4, n_data=n_data, n_bits=n_bits, p_bitflip=p_bitflip, seed=seed)\n",
    "\n",
    "generators = {\n",
    "    # \"parity_4lookback\": make_datasets.parity_4lookback,\n",
    "    # \"not_majority_4lookback\": make_datasets.not_majority_4lookback,\n",
    "    # \"sparse_parity_k4\": make_datasets.sparity_k4,\n",
    "    \"hamilton_6_choose_4\": hamilton_6_choose_4\n",
    "}\n",
    "\n",
    "for p_bitflip in p_bitflips:\n",
    "    p100 = int(p_bitflip*100)\n",
    "    suffix = f\"_nbits{n_bits}_n{n_train}_bf{p100}_seed{seed}\"\n",
    "\n",
    "    for gen_name, generator in generators.items():\n",
    "        dirname = gen_name + suffix\n",
    "        print(f\"Generating {dirname} with p_bitflip={p_bitflip}\")\n",
    "        # If your dataset has a hidden subset, update this list:\n",
    "        if gen_name in [\"sparse_parity_k4\", \"hamilton_6_choose_4\"]: \n",
    "            X, Z, idx = generator(n_train + n_val, n_bits, p_bitflip, seed)\n",
    "            print(\"idx for sparse parity: save these:\", idx)\n",
    "        else:\n",
    "            X, Z = generator(n_train + n_val, n_bits, p_bitflip, seed)\n",
    "\n",
    "        if p_bitflip == 0:\n",
    "            Z = X\n",
    "        Z_train = Z[:n_train]\n",
    "        Z_val = Z[n_train:]\n",
    "\n",
    "        # Check if the data directory exists, if not create it\n",
    "        if not os.path.exists(dirname):\n",
    "            os.makedirs(dirname)\n",
    "\n",
    "        train_path = f\"{dirname}/train.pkl\"\n",
    "        val_path = f\"{dirname}/val.pkl\"\n",
    "        data_io.save_numpy_as_dict(Z_train, train_path)\n",
    "        data_io.save_numpy_as_dict(Z_val, val_path)\n",
    "\n",
    "        if p_bitflip != 0:\n",
    "            X_train = X[:n_train]\n",
    "            X_val = X[n_train:]\n",
    "            noiseless_train_path = f\"{dirname}/noiseless_train.pkl\"\n",
    "            noiseless_val_path = f\"{dirname}/noiseless_val.pkl\"\n",
    "            data_io.save_numpy_as_dict(X_train, noiseless_train_path)\n",
    "            data_io.save_numpy_as_dict(X_val, noiseless_val_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-lookback datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sparse_parity_k4_nbits10_n5000_bf0_seed1234 with nondeterm=0.0\n",
      "Generating sparse_parity_k4_nbits10_n5000_bf10_seed1234 with nondeterm=0.1\n",
      "Generating sparse_parity_k4_nbits10_n5000_bf20_seed1234 with nondeterm=0.2\n"
     ]
    }
   ],
   "source": [
    "# Generate data with bitflip values\n",
    "n_val = 10000 # number of validation examples\n",
    "seed = 1234 \n",
    "n_train = 5000 # number of training examples\n",
    "n_bits = 10 # number of TOTAL bits (including final bit)\n",
    "\n",
    "# Create a different dataset for every 'nondeterministic' value in this list\n",
    "# Note that _sometimes_ this means bitflip rate, but not always\n",
    "nondeterms = [0.0, 0.1, 0.2]\n",
    "\n",
    "# We will create datasets for every entry in this dictionary of data generators \n",
    "# Data generating functions must all have signature (n_data, n_bits, p_bitflip, seed)\n",
    "generators = {\n",
    "    # \"parity_4lookback\": make_datasets.parity_4lookback_nondeterministic,\n",
    "    # \"not_majority_4lookback\": make_datasets.not_majority_4lookback_nondeterministic,\n",
    "    \"sparse_parity_k4\": make_datasets.sparity_k4,\n",
    "}\n",
    "\n",
    "for nondeterm in nondeterms:\n",
    "    p100 = int(nondeterm*100)\n",
    "    suffix = f\"_nbits{n_bits}_n{n_train}_bf{p100}_seed{seed}\"\n",
    "\n",
    "    for gen_name, generator in generators.items():\n",
    "        dirname = gen_name + suffix\n",
    "        print(f\"Generating {dirname} with nondeterm={nondeterm}\")\n",
    "        X, Z, idx = generator(n_train + n_val, n_bits, nondeterm, seed)\n",
    "        X_train = X[:n_train]\n",
    "        X_val = X[n_train:]\n",
    "\n",
    "        if nondeterm == 0:\n",
    "            Z = X\n",
    "        Z_train = Z[:n_train]\n",
    "        Z_val = Z[n_train:]\n",
    "\n",
    "        # Check if the data directory exists, if not create it\n",
    "        if not os.path.exists(dirname):\n",
    "            os.makedirs(dirname)\n",
    "\n",
    "        train_path = f\"{dirname}/train.pkl\"\n",
    "        val_path = f\"{dirname}/val.pkl\"\n",
    "        data_io.save_numpy_as_dict(Z_train, train_path)\n",
    "        data_io.save_numpy_as_dict(Z_val, val_path)\n",
    "    \n",
    "        # whether or not there are bitflips, we will save a 'noiseless'\n",
    "        # version of the data just for consistency\n",
    "        X_train = X[:n_train]\n",
    "        X_val = X[n_train:]\n",
    "        noiseless_train_path = f\"{dirname}/noiseless_train.pkl\"\n",
    "        noiseless_val_path = f\"{dirname}/noiseless_val.pkl\"\n",
    "        data_io.save_numpy_as_dict(X_train, noiseless_train_path)\n",
    "        data_io.save_numpy_as_dict(X_val, noiseless_val_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
