{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload magic\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ray import tune\n",
    "\n",
    "from mindreadingautobots.sequence_generators import make_datasets\n",
    "from mindreadingautobots.models import rnn, hyperparameters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 9, 1])\n"
     ]
    }
   ],
   "source": [
    "# DATA LOADING\n",
    "seed = 334\n",
    "n_train = 800 \n",
    "n_data = int(n_train * 5/4) # downstream we have a 80/20 train/val split\n",
    "n_bits = 8 + 1\n",
    "# k = 4\n",
    "# noisy_not_majority_transition_matrix = {0: 0.05, 1: 0.05, 2: 0.05, 3: 0.95, 4: 0.95}\n",
    "# noisy_not_majority_transition_matrix = {0: 0.00, 1: 0.00, 2: 0.00, 3: 1, 4: 1}\n",
    "k = 2\n",
    "noisy_not_majority_transition_matrix = {0: 0.00, 1: 1, 2: 1}\n",
    "\n",
    "X, _ = make_datasets.k_lookback_weight_dataset(noisy_not_majority_transition_matrix, k, n_data, n_bits, 0, seed)\n",
    "# Insert feature dimension (1 for scalar bits), and convert to tensor\n",
    "stochastic_majority_data = torch.tensor(X).float().unsqueeze(-1) \n",
    "print(stochastic_majority_data.shape)\n",
    "config = {\n",
    "        \"hidden_size\": 64,\n",
    "        \"num_layers\": 6,\n",
    "        \"lr\": 5e-5,\n",
    "        \"epochs\": 1000,\n",
    "        \"n_eval\": 4,\n",
    "    } \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_size': 64, 'num_layers': 6, 'lr': 5e-05, 'epochs': 1000, 'n_eval': 4}\n",
      "{'loss': 0.6959856152534485, 'mean_accuracy': 0.2}\n",
      "{'loss': 0.6422483325004578, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.49859529733657837, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.39164572954177856, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.473330557346344, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.44217056035995483, 'mean_accuracy': 0.7111111111111111}\n",
      "{'loss': 0.3741046190261841, 'mean_accuracy': 0.7555555555555555}\n",
      "{'loss': 0.33776533603668213, 'mean_accuracy': 0.7777777777777778}\n",
      "{'loss': 0.34727007150650024, 'mean_accuracy': 0.7666666666666667}\n",
      "{'loss': 0.3648398816585541, 'mean_accuracy': 0.7666666666666667}\n",
      "{'loss': 0.42478519678115845, 'mean_accuracy': 0.7222222222222222}\n",
      "{'loss': 0.35592901706695557, 'mean_accuracy': 0.7666666666666667}\n",
      "{'loss': 0.3906911313533783, 'mean_accuracy': 0.7333333333333333}\n",
      "{'loss': 0.26873207092285156, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.6168738007545471, 'mean_accuracy': 0.6111111111111112}\n",
      "{'loss': 0.3752978444099426, 'mean_accuracy': 0.7555555555555555}\n",
      "{'loss': 0.3921874165534973, 'mean_accuracy': 0.7666666666666667}\n",
      "{'loss': 0.3834065794944763, 'mean_accuracy': 0.7666666666666667}\n",
      "{'loss': 0.29551875591278076, 'mean_accuracy': 0.8}\n",
      "{'loss': 0.40405526757240295, 'mean_accuracy': 0.7333333333333333}\n",
      "{'loss': 0.49451780319213867, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.3615429997444153, 'mean_accuracy': 0.7888888888888889}\n",
      "{'loss': 0.5393754243850708, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.5221959352493286, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.4908844828605652, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.5460118055343628, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.5366314649581909, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.5212021470069885, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.5209454894065857, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.49225878715515137, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.5220845937728882, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.564473032951355, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.5519704222679138, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.5312711596488953, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.5426589250564575, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.5429195165634155, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.5097013711929321, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.528067946434021, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.5238998532295227, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.5117652416229248, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.5275117754936218, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.49259862303733826, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.47896260023117065, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.43409985303878784, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.3144366145133972, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.3189390003681183, 'mean_accuracy': 0.8222222222222222}\n",
      "{'loss': 0.5364886522293091, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.7074908018112183, 'mean_accuracy': 0.2}\n",
      "{'loss': 0.5815345048904419, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.563861072063446, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.45001021027565, 'mean_accuracy': 0.8222222222222222}\n",
      "{'loss': 0.4561616778373718, 'mean_accuracy': 0.8}\n",
      "{'loss': 0.3969971239566803, 'mean_accuracy': 0.7555555555555555}\n",
      "{'loss': 0.35047227144241333, 'mean_accuracy': 0.7777777777777778}\n",
      "{'loss': 0.3300807774066925, 'mean_accuracy': 0.7888888888888889}\n",
      "{'loss': 0.26350435614585876, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 1.2088261842727661, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.5442434549331665, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.523224949836731, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.5164978504180908, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.5080713629722595, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.5243785977363586, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.5214327573776245, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.5065038204193115, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.5291513204574585, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.5965707898139954, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.5316592454910278, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.5307151675224304, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.5175113677978516, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.5444610714912415, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.5151422619819641, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.53150475025177, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.6752079129219055, 'mean_accuracy': 0.6666666666666666}\n",
      "{'loss': 0.5141355991363525, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.5746738314628601, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.5341938734054565, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.5160611271858215, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.48438844084739685, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.5053191184997559, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.49352145195007324, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.48841381072998047, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.4800799787044525, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.43471193313598633, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.44935011863708496, 'mean_accuracy': 0.7333333333333333}\n",
      "{'loss': 0.40985363721847534, 'mean_accuracy': 0.7444444444444445}\n",
      "{'loss': 0.3592950701713562, 'mean_accuracy': 0.8111111111111111}\n",
      "{'loss': 0.45125526189804077, 'mean_accuracy': 0.7888888888888889}\n",
      "{'loss': 0.6097639799118042, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.31345346570014954, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.4075446128845215, 'mean_accuracy': 0.7222222222222222}\n",
      "{'loss': 0.3347198963165283, 'mean_accuracy': 0.7777777777777778}\n",
      "{'loss': 0.25178369879722595, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.4863148629665375, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.33645641803741455, 'mean_accuracy': 0.7777777777777778}\n",
      "{'loss': 0.34368032217025757, 'mean_accuracy': 0.7666666666666667}\n",
      "{'loss': 0.3287126421928406, 'mean_accuracy': 0.8}\n",
      "{'loss': 0.29147782921791077, 'mean_accuracy': 0.8}\n",
      "{'loss': 0.4169102609157562, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 0.2703425884246826, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.27641770243644714, 'mean_accuracy': 0.8111111111111111}\n",
      "{'loss': 0.3026147782802582, 'mean_accuracy': 0.7777777777777778}\n",
      "{'loss': 0.24875330924987793, 'mean_accuracy': 0.8111111111111111}\n",
      "{'loss': 0.2544083595275879, 'mean_accuracy': 0.8222222222222222}\n",
      "{'loss': 0.36551883816719055, 'mean_accuracy': 0.7666666666666667}\n",
      "{'loss': 0.2661501169204712, 'mean_accuracy': 0.8222222222222222}\n",
      "{'loss': 0.30282968282699585, 'mean_accuracy': 0.7777777777777778}\n",
      "{'loss': 0.20637977123260498, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.33707040548324585, 'mean_accuracy': 0.7888888888888889}\n",
      "{'loss': 0.26219436526298523, 'mean_accuracy': 0.8222222222222222}\n",
      "{'loss': 0.2393970936536789, 'mean_accuracy': 0.8222222222222222}\n",
      "{'loss': 0.26147112250328064, 'mean_accuracy': 0.8222222222222222}\n",
      "{'loss': 0.3124746084213257, 'mean_accuracy': 0.7777777777777778}\n",
      "{'loss': 0.2191036492586136, 'mean_accuracy': 0.8222222222222222}\n",
      "{'loss': 0.2923254370689392, 'mean_accuracy': 0.8}\n",
      "{'loss': 0.27311989665031433, 'mean_accuracy': 0.8111111111111111}\n",
      "{'loss': 0.20757131278514862, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.21336989104747772, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.32444027066230774, 'mean_accuracy': 0.7666666666666667}\n",
      "{'loss': 0.28230923414230347, 'mean_accuracy': 0.8111111111111111}\n",
      "{'loss': 0.27912771701812744, 'mean_accuracy': 0.8}\n",
      "{'loss': 0.26319828629493713, 'mean_accuracy': 0.8222222222222222}\n",
      "{'loss': 0.21035487949848175, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.1770087480545044, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.18481704592704773, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.35002151131629944, 'mean_accuracy': 0.7888888888888889}\n",
      "{'loss': 0.1984645277261734, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.20582525432109833, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.28675001859664917, 'mean_accuracy': 0.7888888888888889}\n",
      "{'loss': 0.27805644273757935, 'mean_accuracy': 0.8111111111111111}\n",
      "{'loss': 0.17704087495803833, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.2469506710767746, 'mean_accuracy': 0.8222222222222222}\n",
      "{'loss': 0.20060011744499207, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.20618681609630585, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.2341691553592682, 'mean_accuracy': 0.8111111111111111}\n",
      "{'loss': 0.24062852561473846, 'mean_accuracy': 0.8222222222222222}\n",
      "{'loss': 0.23508092761039734, 'mean_accuracy': 0.8111111111111111}\n",
      "{'loss': 0.19221936166286469, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.23477640748023987, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.19901113212108612, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.22289064526557922, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.18770131468772888, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.19284677505493164, 'mean_accuracy': 0.8222222222222222}\n",
      "{'loss': 0.19841089844703674, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.22916527092456818, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.17748019099235535, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.1840192973613739, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.1514304280281067, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.17886154353618622, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.14721523225307465, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.14925256371498108, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.19928808510303497, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.20615613460540771, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.20912329852581024, 'mean_accuracy': 0.8222222222222222}\n",
      "{'loss': 0.1973274052143097, 'mean_accuracy': 0.8222222222222222}\n",
      "{'loss': 0.20911923050880432, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.2073969542980194, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.2429054230451584, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.20704448223114014, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.1766919195652008, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.207101508975029, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.20737691223621368, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.18470339477062225, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.1979990452528, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.19107231497764587, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.1439676582813263, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.2197534143924713, 'mean_accuracy': 0.8222222222222222}\n",
      "{'loss': 0.15040364861488342, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.2063199281692505, 'mean_accuracy': 0.8111111111111111}\n",
      "{'loss': 0.20247772336006165, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.18479208648204803, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.1494140326976776, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.1899872124195099, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.22628910839557648, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.19024530053138733, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.13378547132015228, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.20786535739898682, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.1829017698764801, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.20622536540031433, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.12981386482715607, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.20279726386070251, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.19855140149593353, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.19927147030830383, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.17618811130523682, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.17381513118743896, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.17195124924182892, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.2272651642560959, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.23503749072551727, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.21757034957408905, 'mean_accuracy': 0.8222222222222222}\n",
      "{'loss': 0.15691471099853516, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.1543586254119873, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.18044057488441467, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.19487354159355164, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.14152571558952332, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.16261345148086548, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.12304948270320892, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.2147398293018341, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.1737866997718811, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.18323731422424316, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.16590550541877747, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.15409532189369202, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.22278745472431183, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.2006949633359909, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.17014430463314056, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.20233699679374695, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.24049511551856995, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.1668090671300888, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.16157595813274384, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.17943403124809265, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.24310985207557678, 'mean_accuracy': 0.8222222222222222}\n",
      "{'loss': 0.16610397398471832, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.13739445805549622, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.15532150864601135, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.11344043165445328, 'mean_accuracy': 0.8666666666666667}\n",
      "{'loss': 0.2152830809354782, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.1914595663547516, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.17185285687446594, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.24878060817718506, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.14391615986824036, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.16023442149162292, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.21052315831184387, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.1512865126132965, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.12892669439315796, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.1673259139060974, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.18740247189998627, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.10323967784643173, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.18783225119113922, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.22564247250556946, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.17269106209278107, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.20183086395263672, 'mean_accuracy': 0.8222222222222222}\n",
      "{'loss': 0.17170608043670654, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.1318206787109375, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.19815151393413544, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.20613828301429749, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.2084018886089325, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.22285714745521545, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.20204095542430878, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.18553045392036438, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.14897194504737854, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.18486899137496948, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.16776452958583832, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.19654616713523865, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.15579691529273987, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.19173792004585266, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.15425005555152893, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.18720468878746033, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.1801171749830246, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.1650964617729187, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.23920516669750214, 'mean_accuracy': 0.8222222222222222}\n",
      "{'loss': 0.12622937560081482, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.1471990942955017, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.15308594703674316, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.16580060124397278, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.17696331441402435, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.167469322681427, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.17101389169692993, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.22879378497600555, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.1893894076347351, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.2143227756023407, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.1811675876379013, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.14153991639614105, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.172901913523674, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.23159809410572052, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.18075165152549744, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.22773626446723938, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.13116466999053955, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.10912775993347168, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.1811792552471161, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.17833974957466125, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.15242743492126465, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.13304267823696136, 'mean_accuracy': 0.8555555555555555}\n",
      "{'loss': 0.18401791155338287, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.2087252140045166, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.1632559597492218, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.21495473384857178, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.19077007472515106, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.14539113640785217, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.179842010140419, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 0.192560613155365, 'mean_accuracy': 0.8333333333333334}\n",
      "{'loss': 0.19483456015586853, 'mean_accuracy': 0.8444444444444444}\n",
      "{'loss': 1.340464472770691, 'mean_accuracy': 0.7888888888888889}\n",
      "{'loss': 8.889520645141602, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 78.39842224121094, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 5139.21826171875, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 33326.1171875, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 1714505.625, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 76793472.0, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 18557173760.0, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 268923437056.0, 'mean_accuracy': 0.2}\n",
      "{'loss': 5952556032.0, 'mean_accuracy': 0.3333333333333333}\n",
      "{'loss': 9224544321536.0, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 7649558003712.0, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 55352892588032.0, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 7004017917952.0, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 764193330429952.0, 'mean_accuracy': 0.2}\n",
      "{'loss': 599247795781632.0, 'mean_accuracy': 0.2}\n",
      "{'loss': 405139533856768.0, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 5977995864965120.0, 'mean_accuracy': 0.2}\n",
      "{'loss': 1900261840781312.0, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 1.6217521631789056e+16, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 4.6718513634187674e+17, 'mean_accuracy': 0.6888888888888889}\n",
      "{'loss': 2.565250237799072e+18, 'mean_accuracy': 0.6666666666666666}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n",
      "{'loss': nan, 'mean_accuracy': 0.2}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m (model, hidden) \u001b[38;5;241m=\u001b[39m \u001b[43mrnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_binary_rnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstochastic_majority_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\projects\\MindReadingAutobot\\mindreadingautobots\\src\\mindreadingautobots\\models\\rnn.py:108\u001b[0m, in \u001b[0;36mtrain_binary_rnn\u001b[1;34m(config, data, checkpoint_dir, verbose, return_model)\u001b[0m\n\u001b[0;32m    106\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m criterion(output, target\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    107\u001b[0m     train_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 108\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Validation loss gets reported to raytune\u001b[39;00m\n\u001b[0;32m    112\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\MindReadingAutobot\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m             )\n\u001b[1;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\MindReadingAutobot\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\MindReadingAutobot\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    160\u001b[0m         group,\n\u001b[0;32m    161\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    166\u001b[0m         state_steps)\n\u001b[1;32m--> 168\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\MindReadingAutobot\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 318\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\MindReadingAutobot\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:394\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    393\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[1;32m--> 394\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[0;32m    397\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "(model, hidden) = rnn.train_binary_rnn(config, stochastic_majority_data, verbose=True, return_model=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 9)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "xx = stochastic_majority_data[:10,:-1]\n",
    "yy = stochastic_majority_data[:10,1:]\n",
    "print(xx.reshape(10, -1))\n",
    "preds = model.predict(xx, hidden).reshape(10, -1)\n",
    "print(preds)\n",
    "print(yy.reshape(10, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
