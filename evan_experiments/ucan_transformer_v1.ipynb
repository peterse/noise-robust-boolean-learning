{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload magic\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import deterministic.py using local file path\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# DEVICE = torch.device('cpu')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('../mindreadingautobots/src/mindreadingautobots')\n",
    "from sequence_generators import deterministic\n",
    "from ucan import ucan, ucan_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UCAN dataset\n",
    "\n",
    "The UCAN dataset has inputs $(Y \\oplus \\Gamma, \\Delta)$, targets $Y$, and 'hidden variable' $\\Gamma$, all of which are length-n bitstrings. Note that I'm changing my notation from whats in the writeup to match input/target sequence labels better.\n",
    "\n",
    "This has to be done in a few steps:\n",
    "1. Generate a dataset for $Y$, array $(n_{data}, n)$\n",
    "2. Generate a matched dataset $(\\Gamma, \\Delta)$, array $(N, n, 2)$ for whatever version of UCAN\n",
    "3. Compose and discard, i.e. $X = Y \\oplus \\Gamma$, data = $[Z, \\Delta]$ array $(N, 2n)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a deterministic set of sequences of length N_BITS\n",
    "# with the sos and eos tokens, this becomes length 2 + 2N_BITS\n",
    "# This is why the number of bits looks funny\n",
    "N_BITS = 7 \n",
    "n_data = 100\n",
    "\n",
    "gen = deterministic.SequenceGen(lookback=3, seed=228, number_of_generating_methods=1)\n",
    "Y, _ = gen.deterministically_generate_sequences(length=N_BITS, num_seq=n_data, save=False)\n",
    "Y = np.array(Y, dtype=np.int32)\n",
    "\n",
    "# Generate our UCAN. For the first experiment, gamma=delta (so p_diff = 0)\n",
    "p_diff = 0\n",
    "p0_delta = 0.5 # if this is too hard, change to 1 (i.e. ignore delta)\n",
    "out = ucan.bitwise_ucan_v1(N_BITS, n_data, p0_delta, p_diff, seed=228)\n",
    "gammas = out[:,:,0]\n",
    "deltas = out[:,:,1]\n",
    "\n",
    "# Generate the noise and concatenate the data\n",
    "Z = Y ^ gammas\n",
    "X = np.concatenate((Z, deltas), axis=1) # (n_data, 2*N_BITS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input data:  torch.Size([100, 16])\n",
      "Shape of output data:  torch.Size([100, 9])\n"
     ]
    }
   ],
   "source": [
    "# Encoding the entire text into integers\n",
    "data = torch.tensor(X, dtype=torch.int) # (n_data, 2*N_BITS)\n",
    "targets = torch.tensor(Y, dtype=torch.int) # (n_data, N_BITS)\n",
    "\n",
    "# SOS is needed to get reasonable marginals for first token / seed generator\n",
    "# I'm not sure if EOS is necessary for anything\n",
    "SOS_TOKEN = 2\n",
    "EOS_TOKEN = 3\n",
    "data = torch.cat((torch.ones(data.size(0), 1, dtype=torch.int) * SOS_TOKEN, data, torch.ones(data.size(0), 1, dtype=torch.int) * EOS_TOKEN), dim=1)\n",
    "targets = torch.cat((torch.ones(targets.size(0), 1, dtype=torch.int) * SOS_TOKEN, targets, torch.ones(targets.size(0), 1, dtype=torch.int) * EOS_TOKEN), dim=1)\n",
    "\n",
    "print(\"Shape of input data: \", data.shape)\n",
    "print(\"Shape of output data: \", targets.shape)\n",
    "\n",
    "n_train = int(len(data) * 0.9)\n",
    "train_data = data[:n_train]\n",
    "val_data = data[n_train:]\n",
    "train_targets = targets[:n_train]\n",
    "val_targets = targets[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337) # to get what andrej karpathy got\n",
    "\n",
    "# I have not hard-coded a context window into this data.\n",
    "# For now, the entire data will be included in the context window\n",
    "# block_size = 8 \n",
    "batch_size = 4\n",
    "\n",
    "def get_batch(split):\n",
    "    \"\"\"Generate a small batch of data with inputs x, targets y.\n",
    "    \n",
    "    Note that there's no interesting block structure going on here,\n",
    "    since we're learning a map from x \\in \\{0,1\\}^{2n} -> y \\in \\{0,1\\}^n.\n",
    "\n",
    "    Outputs are shaped (batch_size, 2n) and (batch_size, n) respectively.\n",
    "    This corresponds to `batch_first` in the torch transformer\n",
    "    \"\"\"\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    targets = train_targets if split == 'train' else val_targets\n",
    "\n",
    "    ix = torch.randint(0, len(data), (batch_size,)) # indices for batch sample\n",
    "    x = torch.stack([data[i] for i in ix]) # `block_size` many data points\n",
    "    y = torch.stack([targets[i] for i in ix]) # target for each input (the Y string)\n",
    "    # CUDA has a problem with short int. see: https://stackoverflow.com/questions/69742930/runtimeerror-nll-loss-forward-reduce-cuda-kernel-2d-index-not-implemented-for\n",
    "    y = y.type(torch.LongTensor)\n",
    "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture notes:\n",
    "\n",
    " - Do we want/need a causal mask in our decoder??? This is not an autoregressive task\n",
    " - Reminder to modify the mask for the encoder to reflect the position-wise dependence of $\\Delta$, $Z$\n",
    " - I do not have a tokenizer nor plans for one???\n",
    " - Do we need blocksize for context window eventually?|\n",
    "\n",
    "\n",
    " #### Data notes:\n",
    "\n",
    "  <!-- - I am avoiding using EOS and  BOS for sequence data partly because I can get away with bool-type data right now. I don't know how smart that actually is, since these values get cast regardless... -->\n",
    "\n",
    "  Notes from nn.Transformer https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
    "\n",
    " - If a boolean tensor is provided for any of the [src/tgt/memory]_mask arguments, positions with a True value are not allowed to participate in the attention, which is the opposite of the definition for attn_mask in torch.nn.functional.scaled_dot_product_attention().\n",
    " - src, tgt, memory mask are masks applied to the x input seq, the y target seq, and the last layer of encoder seq resp.\n",
    "\n",
    "Things I'm confused about:\n",
    " - at training time, we have targets, so we can embed (positional and vector) the targets to feed into the decoder. At evaluation time, we start with a source vector, encode into memory, then autoregressively build the target I guess?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 47876\n"
     ]
    }
   ],
   "source": [
    "# Training deck\n",
    "\n",
    "num_encoder_layers = 4\n",
    "num_decoder_layers = 4\n",
    "emb_size = 16\n",
    "nhead = 4\n",
    "src_vocab_size = 4\n",
    "tgt_vocab_size = 4\n",
    "dim_feedforward = 128\n",
    "# For deterministic task, dropout doesn't make sense\n",
    "dropout = 0.0\n",
    "\n",
    "# hyperparameters\n",
    "eval_iters = 200\n",
    "LEARNING_RATE = 0.0001\n",
    "# BATCH_SIZE = 32\n",
    "\n",
    "# Train loop\n",
    "eval_interval = 50\n",
    "max_iters = 5000\n",
    "\n",
    "model = ucan_transformer.Seq2SeqTransformer(\n",
    "    num_encoder_layers, \n",
    "    num_decoder_layers, \n",
    "    emb_size, \n",
    "    nhead, \n",
    "    src_vocab_size, \n",
    "    tgt_vocab_size, \n",
    "    dim_feedforward, \n",
    "    dropout\n",
    ").to(DEVICE)\n",
    "# `forward` signature: (src, trg, src_mask, tgt_mask, **kwargs)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Assuming you have a model called 'model'\n",
    "total_params = count_parameters(model)\n",
    "print(f\"Total trainable parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a custom source mask\n",
    "\n",
    "We want to build a source mask that forces self-attention elementwise between the $Z$ and $\\Delta$ parts of the input. For the toy example, a huge benefit here is that we don't need to do any self-attention within the $z$ block or within the $\\Delta$ block - we aren't modelling any distribution! If the CAN relationship is deterministic, the transformer is literally just learning an elementwise XOR.\n",
    "\n",
    "FIXME: Is there ever a need for self-attention then?? Yes, if $Y|Z,\\Delta$ is the output of a stochastic channel.\n",
    "\n",
    "1. Start with an `(2*N_BITS + 2, 2*N_BITS+2)` array of `True` (this disables all attention)\n",
    "2. Set diagonal to `False`\n",
    "3. Skipping the first and last indices, for `i=1, \\dots, N_BITS` SET `mask[i, i+N_BITS] = mask[i+N_BITS, i] = False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_src_mask(n):\n",
    "    mask = np.ones((2*n + 2, 2*n + 2), dtype=bool)\n",
    "    for i in range(2*n + 2):\n",
    "        mask[i, i] = False\n",
    "        if i > 0 and i <= n:\n",
    "            mask[i, i + n] = 0\n",
    "            mask[i + n, i] = 0\n",
    "    return torch.tensor(mask, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    " - clean up train and val epochs a bit\n",
    " - need to train closer to convergence\n",
    " - I think I want non-autoregressive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(verbose=False):\n",
    "    # Average the loss over many batches. Hardcoded cross_entropy loss\n",
    "    # Needs to be in same namespace as model and get_batch\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    tgt_mask = ucan_transformer.generate_square_subsequent_mask(N_BITS + 1, device=DEVICE)\n",
    "    src_mask = make_src_mask(N_BITS)\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            tgt_input = Y[:, :-1]\n",
    "            tgt_out = Y[:, 1:]\n",
    "            logits = model(X, tgt_input, src_mask, tgt_mask)\n",
    "\n",
    "            if verbose:\n",
    "                preds = torch.argmax(logits.reshape(batch_size, N_BITS + 1, tgt_vocab_size), dim=2)\n",
    "                for i in range(len(X)):\n",
    "                    print(f\"input: {X[i]}\")\n",
    "                    print(f\"target: {tgt_out[i]}\")\n",
    "                    print(f\"predicted: {preds[i]}\")\n",
    "                    print()\n",
    "            loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 1.2971, val loss 1.3453\n",
      "step 50: train loss 0.9851, val loss 1.0000\n",
      "step 100: train loss 0.9735, val loss 0.9731\n",
      "step 150: train loss 0.9617, val loss 0.9637\n",
      "step 200: train loss 0.9425, val loss 0.9582\n",
      "step 250: train loss 0.9195, val loss 0.9464\n",
      "step 300: train loss 0.9048, val loss 0.9319\n",
      "step 350: train loss 0.8898, val loss 0.9300\n",
      "step 400: train loss 0.8838, val loss 0.9101\n",
      "step 450: train loss 0.8610, val loss 0.8779\n",
      "step 500: train loss 0.8401, val loss 0.8529\n",
      "step 550: train loss 0.8178, val loss 0.8225\n",
      "step 600: train loss 0.7875, val loss 0.7952\n",
      "step 650: train loss 0.7748, val loss 0.7780\n",
      "step 700: train loss 0.7587, val loss 0.7604\n",
      "step 750: train loss 0.7405, val loss 0.7336\n",
      "step 800: train loss 0.7059, val loss 0.6892\n",
      "step 850: train loss 0.6963, val loss 0.6737\n",
      "step 900: train loss 0.6720, val loss 0.6634\n",
      "step 950: train loss 0.6470, val loss 0.6313\n",
      "step 1000: train loss 0.6236, val loss 0.5993\n",
      "step 1050: train loss 0.6231, val loss 0.6045\n",
      "step 1100: train loss 0.5980, val loss 0.5679\n",
      "step 1150: train loss 0.5686, val loss 0.5387\n",
      "step 1200: train loss 0.5654, val loss 0.5418\n",
      "step 1250: train loss 0.5283, val loss 0.5001\n",
      "step 1300: train loss 0.5074, val loss 0.4739\n",
      "step 1350: train loss 0.5003, val loss 0.4763\n",
      "step 1400: train loss 0.4865, val loss 0.4458\n",
      "step 1450: train loss 0.4559, val loss 0.4157\n",
      "step 1500: train loss 0.4673, val loss 0.4361\n",
      "step 1550: train loss 0.4318, val loss 0.4032\n",
      "step 1600: train loss 0.4302, val loss 0.3890\n",
      "step 1650: train loss 0.4238, val loss 0.3779\n",
      "step 1700: train loss 0.3918, val loss 0.3498\n",
      "step 1750: train loss 0.4055, val loss 0.3747\n",
      "step 1800: train loss 0.3824, val loss 0.3602\n",
      "step 1850: train loss 0.3817, val loss 0.3401\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# every once in a while evaluate the loss on train and val sets\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m eval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 10\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# sample a batch of data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 15\u001b[0m, in \u001b[0;36mestimate_loss\u001b[1;34m(verbose)\u001b[0m\n\u001b[0;32m     13\u001b[0m tgt_input \u001b[38;5;241m=\u001b[39m Y[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     14\u001b[0m tgt_out \u001b[38;5;241m=\u001b[39m Y[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m---> 15\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m     18\u001b[0m     preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits\u001b[38;5;241m.\u001b[39mreshape(batch_size, N_BITS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, tgt_vocab_size), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\MindReadingAutobot\\evan_experiments\\../mindreadingautobots/src/mindreadingautobots\\ucan\\ucan_transformer.py:121\u001b[0m, in \u001b[0;36mSeq2SeqTransformer.forward\u001b[1;34m(self, src, trg, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    119\u001b[0m src_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_tok_emb(src))\n\u001b[0;32m    120\u001b[0m tgt_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtgt_tok_emb(trg))\n\u001b[1;32m--> 121\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m                        \u001b[49m\u001b[43msrc_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator(outs)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# Forward is only called during training/validation, so this is fine\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:278\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    269\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    270\u001b[0m     )\n\u001b[0;32m    272\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m    273\u001b[0m     src,\n\u001b[0;32m    274\u001b[0m     mask\u001b[38;5;241m=\u001b[39msrc_mask,\n\u001b[0;32m    275\u001b[0m     src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask,\n\u001b[0;32m    276\u001b[0m     is_causal\u001b[38;5;241m=\u001b[39msrc_is_causal,\n\u001b[0;32m    277\u001b[0m )\n\u001b[1;32m--> 278\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_is_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:602\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[0;32m    599\u001b[0m tgt_is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 602\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_is_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    614\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(output)\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:1087\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[0;32m   1084\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x))\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1086\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(\n\u001b[1;32m-> 1087\u001b[0m         x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1088\u001b[0m     )\n\u001b[0;32m   1089\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(\n\u001b[0;32m   1090\u001b[0m         x\n\u001b[0;32m   1091\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mha_block(\n\u001b[0;32m   1092\u001b[0m             x, memory, memory_mask, memory_key_padding_mask, memory_is_causal\n\u001b[0;32m   1093\u001b[0m         )\n\u001b[0;32m   1094\u001b[0m     )\n\u001b[0;32m   1095\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:1107\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1102\u001b[0m     x: Tensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1105\u001b[0m     is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1106\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1107\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1368\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1343\u001b[0m         query,\n\u001b[0;32m   1344\u001b[0m         key,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1365\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[0;32m   1366\u001b[0m     )\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1368\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1380\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1388\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m   1390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:6097\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   6093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_separate_proj_weight:\n\u001b[0;32m   6094\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m   6095\u001b[0m         in_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   6096\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is False but in_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 6097\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[43m_in_projection_packed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6098\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6099\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m   6100\u001b[0m         q_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   6101\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is True but q_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:5504\u001b[0m, in \u001b[0;36m_in_projection_packed\u001b[1;34m(q, k, v, w, b)\u001b[0m\n\u001b[0;32m   5501\u001b[0m     proj \u001b[38;5;241m=\u001b[39m linear(q, w, b)\n\u001b[0;32m   5502\u001b[0m     \u001b[38;5;66;03m# reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\u001b[39;00m\n\u001b[0;32m   5503\u001b[0m     proj \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 5504\u001b[0m         \u001b[43mproj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5505\u001b[0m         \u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   5506\u001b[0m         \u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m   5507\u001b[0m         \u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m   5508\u001b[0m         \u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m   5509\u001b[0m     )\n\u001b[0;32m   5510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m proj[\u001b[38;5;241m0\u001b[39m], proj[\u001b[38;5;241m1\u001b[39m], proj[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m   5511\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5512\u001b[0m     \u001b[38;5;66;03m# encoder-decoder attention\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\_tensor.py:1376\u001b[0m, in \u001b[0;36mTensor.unflatten\u001b[1;34m(self, dim, sizes)\u001b[0m\n\u001b[0;32m   1374\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39munflatten(dim, sizes, names)\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msizes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    # we're doing that kind of sketchy off-by-one thing\n",
    "    # the input is missing the final token (it's always the EOS token....)\n",
    "    tgt_input = yb[:, :-1]\n",
    "    tgt_mask = ucan_transformer.generate_square_subsequent_mask(tgt_input.size(1), device=DEVICE)\n",
    "    src_mask = make_src_mask(N_BITS)\n",
    "    logits = model(xb, tgt_input, src_mask, tgt_mask)\n",
    "\n",
    "    # and the loss is based on the output, which is missing the first token (it's always the SOS token....)\n",
    "    tgt_out = yb[:, 1:]\n",
    "    loss = F.cross_entropy(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "    loss_history.append(loss.item())\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log\n",
    "\n",
    "Enc |dec | dmodel | nhead | dim_ff | final train/val loss\n",
    "8,4,16,4,128 .21 / .16\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x165b8760550>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # function to generate output sequence using greedy algorithm\n",
    "# def greedy_decode(model, src, src_mask, max_len):\n",
    "#     \"\"\"First attempt at non-autoregressive decoding.\"\"\"\n",
    "    \n",
    "#     ys = torch.cat((src[:9], torch.tensor([EOS_TOKEN]).to(DEVICE)), dim=0).unsqueeze(0)\n",
    "#     src = src.to(DEVICE)\n",
    "#     src_mask = None  \n",
    "    \n",
    "#     # For a single example evaluation, we need to add a dummy batch dimension (1, *) with unsqueeze(0)\n",
    "#     memory = model.encode(src.unsqueeze(0), src_mask)    \n",
    "#     memory = memory.to(DEVICE)\n",
    "\n",
    "#     out = model.decode(tgt=ys, memory=memory, tgt_mask=None) # (1, tgt_seq_len, emb_dim)\n",
    "#     prob = model.generator(out) # (1, tgt_seq_len, num_tokens)\n",
    "#     _, pred = torch.max(prob, dim=2)\n",
    "\n",
    "#     return pred.squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16])\n",
      "torch.Size([1, 16])\n",
      "input tensor([[2, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 3]], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "target tensor([[2, 0, 0, 0, 1, 1, 1, 0, 3]], device='cuda:0')\n",
      "final output tensor([[2, 0, 0, 0, 1, 1, 1, 0, 3]], device='cuda:0')\n",
      "\n",
      "input tensor([[2, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 3]], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "target tensor([[2, 0, 1, 0, 1, 1, 1, 0, 3]], device='cuda:0')\n",
      "final output tensor([[2, 0, 1, 0, 1, 1, 1, 0, 3]], device='cuda:0')\n",
      "\n",
      "input tensor([[2, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 3]], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "target tensor([[2, 1, 0, 1, 1, 1, 0, 1, 3]], device='cuda:0')\n",
      "final output tensor([[2, 1, 0, 1, 1, 1, 0, 1, 3]], device='cuda:0')\n",
      "\n",
      "input tensor([[2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 3]], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "target tensor([[2, 1, 0, 1, 1, 1, 0, 1, 3]], device='cuda:0')\n",
      "final output tensor([[2, 1, 0, 1, 1, 1, 0, 1, 3]], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "# sample a batch of data\n",
    "torch.manual_seed(1337) # to get what andrej karpathy got\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "for i in range(batch_size):\n",
    "    x, y = xb[i], yb[i]\n",
    "    print(x.shape)\n",
    "    x, y = x.unsqueeze(0), y.unsqueeze(0)\n",
    "    print(x.shape)\n",
    "    break\n",
    "\n",
    "    src_mask = make_src_mask(N_BITS)\n",
    "    tgt_mask = ucan_transformer.generate_square_subsequent_mask(N_BITS + 2, device=DEVICE)\n",
    "    # evaluate the loss\n",
    "    logits = model(x, y, src_mask, tgt_mask)\n",
    "    print('logits:', logits.shape)\n",
    "    print('loss:', loss)\n",
    "\n",
    "    # sample the output from the logits\n",
    "    _, preds = torch.max(logits, dim=1)\n",
    "    print(\"x    \", x.squeeze(0))\n",
    "    print(\"logits\", logits)\n",
    "    print(\"preds\", preds)\n",
    "    print(\"y    \", y.squeeze(0))\n",
    "    print()\n",
    "\n",
    "# -----------------------------------------------\n",
    "# GENERATING SEQUENCES\n",
    "max_len = N_BITS + 2\n",
    "\n",
    "for i in range(batch_size):\n",
    "    x, y = xb[i], yb[i]\n",
    "    x, y = x.unsqueeze(0), y.unsqueeze(0)\n",
    "    src_mask = make_src_mask(N_BITS)\n",
    "\n",
    "    memory = model.encode(x, src_mask).to(DEVICE) # now this is phi(x), shape (1, 2*N_BITS + 2, emb_dim)\n",
    "    ys = torch.ones(1, 1).fill_(SOS_TOKEN).type(torch.long).to(DEVICE)\n",
    "\n",
    "\n",
    "    print(\"input\", x)\n",
    "    print(\"target\", y)\n",
    "    for i in range(max_len - 1): # -1 since we start with SOS\n",
    "        tgt_mask = (ucan_transformer.generate_square_subsequent_mask(ys.size(1), device=DEVICE).type(torch.bool)).to(DEVICE) \n",
    "        out = model.decode(tgt=ys, memory=memory, tgt_mask=tgt_mask) # (1, tgt_seq_len, emb_dim)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "        ys = torch.cat([ys, torch.ones(1, 1).type_as(x.data).fill_(next_word)], dim=1)\n",
    "        # print(out, \"out\")\n",
    "        # print(ys, \"ys\")\n",
    "        # print(prob, \"prob\")\n",
    "        # print()\n",
    "    print(\"final output\", ys)\n",
    "    print()\n",
    "\n",
    "\n",
    "# tgt_mask = (ucan_transformer.generate_square_subsequent_mask(ys.size(1), device=DEVICE).type(torch.bool)).to(DEVICE) \n",
    "\n",
    "# # This mask sets -inf to True, which is correct according to `Transformer` docs\n",
    "# i = 0\n",
    "# out = model.decode(tgt=ys, memory=memory, tgt_mask=tgt_mask) # (1, tgt_seq_len, emb_dim)\n",
    "# print(\"out.shape\", out.shape)\n",
    "# logits = model.generator(out[:, -1])\n",
    "# # logits = model.generator(out)\n",
    "# print(\"logits.shape\", logits.shape)\n",
    "\n",
    "# _, preds = torch.max(logits, dim=-1)\n",
    "# print(\"ys  \", ys)\n",
    "# print(\"pred:\", preds)\n",
    "\n",
    "# print(memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-2.7868,  0.3571,  0.2683, -0.3423,  0.8015,  0.9599, -0.7208,\n",
      "           0.0275,  0.1133,  0.3672, -0.5141,  1.3417],\n",
      "         [-1.1109,  0.3338,  0.3859, -1.4481, -0.9562,  2.0624,  1.2049,\n",
      "          -0.6362,  0.2811,  0.6141, -0.8774,  0.0184],\n",
      "         [-1.1241,  0.2551,  0.4137, -1.4547, -0.9476,  2.0578,  1.2247,\n",
      "          -0.6118,  0.2941,  0.6032, -0.8785,  0.0387],\n",
      "         [-1.1677,  0.2572,  0.3904, -1.4032, -0.9617,  2.0293,  1.3103,\n",
      "          -0.5942,  0.2802,  0.5733, -0.8782,  0.0352],\n",
      "         [-1.2104,  0.3464,  0.3609, -1.3546, -0.9713,  1.9936,  1.3736,\n",
      "          -0.5989,  0.2561,  0.5441, -0.8755,  0.0084],\n",
      "         [-1.9315, -0.6380,  1.4399,  0.3022,  1.0120,  0.4055, -1.1799,\n",
      "           0.0860, -0.1640,  0.7048, -1.2249,  1.0884],\n",
      "         [-1.9339, -0.5638,  1.4427,  0.2349,  1.0007,  0.4492, -1.2247,\n",
      "           0.1311, -0.1612,  0.6739, -1.2345,  1.0857],\n",
      "         [-1.1189,  0.3520,  0.4719, -1.5429, -0.8868,  2.0529,  1.1527,\n",
      "          -0.6148,  0.2778,  0.5736, -0.8794,  0.0322],\n",
      "         [-1.1117,  0.2646,  0.5001, -1.5900, -0.8689,  2.0464,  1.1558,\n",
      "          -0.5867,  0.2963,  0.5636, -0.8710,  0.0700],\n",
      "         [-1.9095, -0.5974,  1.4829, -0.0930,  1.0984,  0.4114, -1.0502,\n",
      "           0.0903, -0.2330,  0.6108, -1.1770,  1.2612],\n",
      "         [-1.9435, -0.6004,  1.4701, -0.0041,  1.0862,  0.4062, -1.0842,\n",
      "           0.1034, -0.1932,  0.6371, -1.1818,  1.1989],\n",
      "         [-1.2430,  0.3476,  0.3917, -1.4999, -0.8243,  2.0242,  1.2668,\n",
      "          -0.5913,  0.2803,  0.5179, -0.8564,  0.0545],\n",
      "         [-1.2094,  0.3929,  0.4051, -1.5551, -0.8058,  2.0320,  1.2100,\n",
      "          -0.6102,  0.2813,  0.5232, -0.8479,  0.0524],\n",
      "         [-1.1513,  0.3679,  0.4192, -1.6296, -0.8038,  2.0398,  1.1666,\n",
      "          -0.6014,  0.2920,  0.5258, -0.8384,  0.0815],\n",
      "         [-1.1365,  0.2896,  0.4380, -1.6601, -0.8011,  2.0405,  1.1583,\n",
      "          -0.5843,  0.3131,  0.5239, -0.8356,  0.1214],\n",
      "         [-1.9291, -0.5465,  1.4353, -0.0473,  1.0993,  0.4311, -1.1221,\n",
      "           0.1307, -0.1737,  0.5498, -1.1950,  1.2641],\n",
      "         [-1.2328,  0.2497,  0.3409, -1.5551, -0.7896,  2.0227,  1.2831,\n",
      "          -0.5716,  0.3177,  0.5150, -0.8404,  0.1268],\n",
      "         [-1.1644, -0.8199,  1.5520, -0.6495, -0.8009,  1.1600, -0.1977,\n",
      "           0.1803, -0.0248,  1.3462, -1.5414,  0.8374]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[0.]], device='cuda:0')\n",
      "tensor([[[-0.6215,  2.6652, -0.3210, -0.9126,  0.9668,  0.4516, -1.5679,\n",
      "           2.2724, -0.7793, -1.2046,  0.5827, -0.9221]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2]], device='cuda:0') ys\n",
      "tensor([[ 1.7953,  1.8705, -3.4646, -2.9662]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[0., -inf],\n",
      "        [0., 0.]], device='cuda:0')\n",
      "tensor([[[-0.6215,  2.6652, -0.3210, -0.9126,  0.9668,  0.4516, -1.5679,\n",
      "           2.2724, -0.7793, -1.2046,  0.5827, -0.9221],\n",
      "         [-0.3644,  2.4371, -0.2941, -0.5195,  0.6165,  0.3531, -2.0415,\n",
      "           2.3806, -0.8867, -0.9838,  0.8195, -0.9697]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1]], device='cuda:0') ys\n",
      "tensor([[ 1.8422,  1.6185, -3.4436, -2.9632]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[0., -inf, -inf],\n",
      "        [0., 0., -inf],\n",
      "        [0., 0., 0.]], device='cuda:0')\n",
      "tensor([[[-0.6215,  2.6652, -0.3210, -0.9126,  0.9668,  0.4516, -1.5679,\n",
      "           2.2724, -0.7793, -1.2046,  0.5827, -0.9221],\n",
      "         [-0.3644,  2.4371, -0.2941, -0.5195,  0.6165,  0.3531, -2.0415,\n",
      "           2.3806, -0.8867, -0.9838,  0.8195, -0.9697],\n",
      "         [ 0.1441,  2.2480, -0.6197, -0.9697,  1.1294,  0.5845, -1.7319,\n",
      "           2.3668, -0.9564, -1.0624,  0.5569, -1.1021]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 0]], device='cuda:0') ys\n",
      "tensor([[ 1.7423,  2.0293, -3.6887, -2.7872]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[0., -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf],\n",
      "        [0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0.]], device='cuda:0')\n",
      "tensor([[[-0.6215,  2.6652, -0.3210, -0.9126,  0.9668,  0.4516, -1.5679,\n",
      "           2.2724, -0.7793, -1.2046,  0.5827, -0.9221],\n",
      "         [-0.3644,  2.4371, -0.2941, -0.5195,  0.6165,  0.3531, -2.0415,\n",
      "           2.3806, -0.8867, -0.9838,  0.8195, -0.9697],\n",
      "         [ 0.1441,  2.2480, -0.6197, -0.9697,  1.1294,  0.5845, -1.7319,\n",
      "           2.3668, -0.9564, -1.0624,  0.5569, -1.1021],\n",
      "         [ 0.8528,  1.4241, -0.3003, -0.1138,  0.2227,  0.7528, -2.3854,\n",
      "           2.7800, -0.7787, -1.0486,  0.0081, -1.0010]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 0, 1]], device='cuda:0') ys\n",
      "tensor([[ 1.2520,  1.6394, -3.5942, -2.3785]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0.]], device='cuda:0')\n",
      "tensor([[[-0.6215,  2.6652, -0.3210, -0.9126,  0.9668,  0.4516, -1.5679,\n",
      "           2.2724, -0.7793, -1.2046,  0.5827, -0.9221],\n",
      "         [-0.3644,  2.4371, -0.2941, -0.5195,  0.6165,  0.3531, -2.0415,\n",
      "           2.3806, -0.8867, -0.9838,  0.8195, -0.9697],\n",
      "         [ 0.1441,  2.2480, -0.6197, -0.9697,  1.1294,  0.5845, -1.7319,\n",
      "           2.3668, -0.9564, -1.0624,  0.5569, -1.1021],\n",
      "         [ 0.8528,  1.4241, -0.3003, -0.1138,  0.2227,  0.7528, -2.3854,\n",
      "           2.7800, -0.7787, -1.0486,  0.0081, -1.0010],\n",
      "         [-0.2352,  0.8998, -0.2121, -0.5112,  0.6165,  0.6870, -2.3933,\n",
      "           3.1812, -0.3517, -0.5731,  0.3524, -1.1253]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 0, 1, 1]], device='cuda:0') ys\n",
      "tensor([[ 1.9334,  1.7536, -3.4672, -2.4327]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
      "tensor([[[-0.6215,  2.6652, -0.3210, -0.9126,  0.9668,  0.4516, -1.5679,\n",
      "           2.2724, -0.7793, -1.2046,  0.5827, -0.9221],\n",
      "         [-0.3644,  2.4371, -0.2941, -0.5195,  0.6165,  0.3531, -2.0415,\n",
      "           2.3806, -0.8867, -0.9838,  0.8195, -0.9697],\n",
      "         [ 0.1441,  2.2480, -0.6197, -0.9697,  1.1294,  0.5845, -1.7319,\n",
      "           2.3668, -0.9564, -1.0624,  0.5569, -1.1021],\n",
      "         [ 0.8528,  1.4241, -0.3003, -0.1138,  0.2227,  0.7528, -2.3854,\n",
      "           2.7800, -0.7787, -1.0486,  0.0081, -1.0010],\n",
      "         [-0.2352,  0.8998, -0.2121, -0.5112,  0.6165,  0.6870, -2.3933,\n",
      "           3.1812, -0.3517, -0.5731,  0.3524, -1.1253],\n",
      "         [ 0.4362,  1.6598, -0.9374, -1.7154,  1.5094,  0.9382, -0.9895,\n",
      "           2.4549, -0.7743, -0.5954, -0.1595, -1.2763]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 0, 1, 1, 0]], device='cuda:0') ys\n",
      "tensor([[ 1.5140,  2.5645, -3.7196, -2.0801]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
      "tensor([[[-0.6215,  2.6652, -0.3210, -0.9126,  0.9668,  0.4516, -1.5679,\n",
      "           2.2724, -0.7793, -1.2046,  0.5827, -0.9221],\n",
      "         [-0.3644,  2.4371, -0.2941, -0.5195,  0.6165,  0.3531, -2.0415,\n",
      "           2.3806, -0.8867, -0.9838,  0.8195, -0.9697],\n",
      "         [ 0.1441,  2.2480, -0.6197, -0.9697,  1.1294,  0.5845, -1.7319,\n",
      "           2.3668, -0.9564, -1.0624,  0.5569, -1.1021],\n",
      "         [ 0.8528,  1.4241, -0.3003, -0.1138,  0.2227,  0.7528, -2.3854,\n",
      "           2.7800, -0.7787, -1.0486,  0.0081, -1.0010],\n",
      "         [-0.2352,  0.8998, -0.2121, -0.5112,  0.6165,  0.6870, -2.3933,\n",
      "           3.1812, -0.3517, -0.5731,  0.3524, -1.1253],\n",
      "         [ 0.4362,  1.6598, -0.9374, -1.7154,  1.5094,  0.9382, -0.9895,\n",
      "           2.4549, -0.7743, -0.5954, -0.1595, -1.2763],\n",
      "         [-0.6462,  1.0183,  0.0338, -1.4558,  1.0744,  0.9041, -1.7509,\n",
      "           2.9368,  0.1237, -0.1320, -0.5279, -1.2435]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 0, 1, 1, 0, 1]], device='cuda:0') ys\n",
      "tensor([[ 1.8322,  2.2890, -3.3510, -2.0556]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
      "tensor([[[-0.6215,  2.6652, -0.3210, -0.9126,  0.9668,  0.4516, -1.5679,\n",
      "           2.2724, -0.7793, -1.2046,  0.5827, -0.9221],\n",
      "         [-0.3644,  2.4371, -0.2941, -0.5195,  0.6165,  0.3531, -2.0415,\n",
      "           2.3806, -0.8867, -0.9838,  0.8195, -0.9697],\n",
      "         [ 0.1441,  2.2480, -0.6197, -0.9697,  1.1294,  0.5845, -1.7319,\n",
      "           2.3668, -0.9564, -1.0624,  0.5569, -1.1021],\n",
      "         [ 0.8528,  1.4241, -0.3003, -0.1138,  0.2227,  0.7528, -2.3854,\n",
      "           2.7800, -0.7787, -1.0486,  0.0081, -1.0010],\n",
      "         [-0.2352,  0.8998, -0.2121, -0.5112,  0.6165,  0.6870, -2.3933,\n",
      "           3.1812, -0.3517, -0.5731,  0.3524, -1.1253],\n",
      "         [ 0.4362,  1.6598, -0.9374, -1.7154,  1.5094,  0.9382, -0.9895,\n",
      "           2.4549, -0.7743, -0.5954, -0.1595, -1.2763],\n",
      "         [-0.6462,  1.0183,  0.0338, -1.4558,  1.0744,  0.9041, -1.7509,\n",
      "           2.9368,  0.1237, -0.1320, -0.5279, -1.2435],\n",
      "         [ 0.4092,  0.4639, -0.8515, -0.2431,  0.0593, -0.2076, -1.1747,\n",
      "           2.7772,  1.3246,  0.3161, -0.4826, -2.2395]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 0, 1, 1, 0, 1, 1]], device='cuda:0') ys\n",
      "tensor([[ 1.8240,  0.7871, -3.7078,  0.0735]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
      "tensor([[[-0.6215,  2.6652, -0.3210, -0.9126,  0.9668,  0.4516, -1.5679,\n",
      "           2.2724, -0.7793, -1.2046,  0.5827, -0.9221],\n",
      "         [-0.3644,  2.4371, -0.2941, -0.5195,  0.6165,  0.3531, -2.0415,\n",
      "           2.3806, -0.8867, -0.9838,  0.8195, -0.9697],\n",
      "         [ 0.1441,  2.2480, -0.6197, -0.9697,  1.1294,  0.5845, -1.7319,\n",
      "           2.3668, -0.9564, -1.0624,  0.5569, -1.1021],\n",
      "         [ 0.8528,  1.4241, -0.3003, -0.1138,  0.2227,  0.7528, -2.3854,\n",
      "           2.7800, -0.7787, -1.0486,  0.0081, -1.0010],\n",
      "         [-0.2352,  0.8998, -0.2121, -0.5112,  0.6165,  0.6870, -2.3933,\n",
      "           3.1812, -0.3517, -0.5731,  0.3524, -1.1253],\n",
      "         [ 0.4362,  1.6598, -0.9374, -1.7154,  1.5094,  0.9382, -0.9895,\n",
      "           2.4549, -0.7743, -0.5954, -0.1595, -1.2763],\n",
      "         [-0.6462,  1.0183,  0.0338, -1.4558,  1.0744,  0.9041, -1.7509,\n",
      "           2.9368,  0.1237, -0.1320, -0.5279, -1.2435],\n",
      "         [ 0.4092,  0.4639, -0.8515, -0.2431,  0.0593, -0.2076, -1.1747,\n",
      "           2.7772,  1.3246,  0.3161, -0.4826, -2.2395],\n",
      "         [ 1.5561,  0.5076, -2.4817, -0.9651, -0.3973, -0.3769,  1.4353,\n",
      "           0.4173,  0.9514,  0.8456, -0.4853, -1.0076]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 0, 1, 1, 0, 1, 1, 0]], device='cuda:0') ys\n",
      "tensor([[-3.5366e-04,  1.2898e-01, -2.2501e+00,  2.1610e+00]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xv, yv = get_batch('val')\n",
    "src = xv[0]\n",
    "truth = yv[0]\n",
    "\n",
    "src = src.to(DEVICE)\n",
    "src_mask = None\n",
    "max_len = 10\n",
    "\n",
    "# For a single example evaluation, we need to add a dummy batch dimension (1, *) with unsqueeze(0)\n",
    "memory = model.encode(src.unsqueeze(0), src_mask)\n",
    "# The [1, 1] shape starts us off with a dummy batch dimension \n",
    "ys = torch.ones(1, 1).fill_(SOS_TOKEN).type(torch.long).to(DEVICE)\n",
    "\n",
    "print(memory)\n",
    "\n",
    "for i in range(max_len - 1): # -1 since we start with SOS\n",
    "    memory = memory.to(DEVICE)\n",
    "    tgt_mask = (ucan_transformer.generate_square_subsequent_mask(ys.size(1), device=DEVICE)).to(DEVICE)\n",
    "    print(tgt_mask)\n",
    "    out = model.decode(tgt=ys, memory=memory, tgt_mask=tgt_mask) # (1, tgt_seq_len, emb_dim)\n",
    "\n",
    "\n",
    "    prob = model.generator(out[:, -1])\n",
    "    print(out, \"out\")\n",
    "    print(ys, \"ys\")\n",
    "    print(prob, \"prob\")\n",
    "    _, next_word = torch.max(prob, dim=1)\n",
    "    next_word = next_word.item()\n",
    "    ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xv_i tensor([2, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 3], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "tensor([[[-0.5963,  1.6153,  1.8461, -0.1170,  1.5781,  0.5869, -1.1237,\n",
      "          -0.3987, -0.7045, -2.1012,  0.1954, -0.7587]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2]], device='cuda:0') ys\n",
      "tensor([[ 1.9841,  2.0354, -0.7844, -2.5241]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-0.5963,  1.6153,  1.8461, -0.1170,  1.5781,  0.5869, -1.1237,\n",
      "          -0.3987, -0.7045, -2.1012,  0.1954, -0.7587],\n",
      "         [ 0.1536,  0.5132,  1.7468,  0.0826,  2.0158, -0.1218, -1.1890,\n",
      "          -0.9465, -1.5047, -2.0150,  0.5374,  0.9175]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1]], device='cuda:0') ys\n",
      "tensor([[ 2.3561,  2.4091, -2.6605, -1.6287]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-0.5963,  1.6153,  1.8461, -0.1170,  1.5781,  0.5869, -1.1237,\n",
      "          -0.3987, -0.7045, -2.1012,  0.1954, -0.7587],\n",
      "         [ 0.1536,  0.5132,  1.7468,  0.0826,  2.0158, -0.1218, -1.1890,\n",
      "          -0.9465, -1.5047, -2.0150,  0.5374,  0.9175],\n",
      "         [ 0.4186,  0.2751,  1.8131,  0.0022,  1.6174, -0.2270, -1.0423,\n",
      "          -1.1356, -1.5871, -1.9409,  0.5506,  1.4812]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 1]], device='cuda:0') ys\n",
      "tensor([[ 2.3298,  2.3269, -3.3170, -1.2850]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-0.5963,  1.6153,  1.8461, -0.1170,  1.5781,  0.5869, -1.1237,\n",
      "          -0.3987, -0.7045, -2.1012,  0.1954, -0.7587],\n",
      "         [ 0.1536,  0.5132,  1.7468,  0.0826,  2.0158, -0.1218, -1.1890,\n",
      "          -0.9465, -1.5047, -2.0150,  0.5374,  0.9175],\n",
      "         [ 0.4186,  0.2751,  1.8131,  0.0022,  1.6174, -0.2270, -1.0423,\n",
      "          -1.1356, -1.5871, -1.9409,  0.5506,  1.4812],\n",
      "         [ 0.9501, -0.3080,  1.9143, -0.1483,  0.8867, -0.1874, -1.0859,\n",
      "          -1.0961, -1.6832, -1.4982,  0.4141,  2.0918]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 1, 0]], device='cuda:0') ys\n",
      "tensor([[ 1.9824,  2.1173, -4.0767, -0.6527]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-0.5963,  1.6153,  1.8461, -0.1170,  1.5781,  0.5869, -1.1237,\n",
      "          -0.3987, -0.7045, -2.1012,  0.1954, -0.7587],\n",
      "         [ 0.1536,  0.5132,  1.7468,  0.0826,  2.0158, -0.1218, -1.1890,\n",
      "          -0.9465, -1.5047, -2.0150,  0.5374,  0.9175],\n",
      "         [ 0.4186,  0.2751,  1.8131,  0.0022,  1.6174, -0.2270, -1.0423,\n",
      "          -1.1356, -1.5871, -1.9409,  0.5506,  1.4812],\n",
      "         [ 0.9501, -0.3080,  1.9143, -0.1483,  0.8867, -0.1874, -1.0859,\n",
      "          -1.0961, -1.6832, -1.4982,  0.4141,  2.0918],\n",
      "         [ 0.7867, -0.0294,  1.7032,  0.0402,  1.0964, -0.4294, -0.7761,\n",
      "          -1.3463, -1.6947, -1.6382,  0.3869,  2.1547]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 1, 0, 1]], device='cuda:0') ys\n",
      "tensor([[ 2.0436,  2.2062, -3.9386, -0.6360]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-5.9634e-01,  1.6153e+00,  1.8461e+00, -1.1703e-01,  1.5781e+00,\n",
      "           5.8687e-01, -1.1237e+00, -3.9868e-01, -7.0449e-01, -2.1012e+00,\n",
      "           1.9538e-01, -7.5867e-01],\n",
      "         [ 1.5361e-01,  5.1322e-01,  1.7468e+00,  8.2632e-02,  2.0158e+00,\n",
      "          -1.2180e-01, -1.1890e+00, -9.4654e-01, -1.5047e+00, -2.0150e+00,\n",
      "           5.3739e-01,  9.1754e-01],\n",
      "         [ 4.1859e-01,  2.7507e-01,  1.8131e+00,  2.2498e-03,  1.6174e+00,\n",
      "          -2.2698e-01, -1.0423e+00, -1.1356e+00, -1.5871e+00, -1.9409e+00,\n",
      "           5.5064e-01,  1.4812e+00],\n",
      "         [ 9.5014e-01, -3.0804e-01,  1.9143e+00, -1.4826e-01,  8.8666e-01,\n",
      "          -1.8743e-01, -1.0859e+00, -1.0961e+00, -1.6832e+00, -1.4982e+00,\n",
      "           4.1413e-01,  2.0918e+00],\n",
      "         [ 7.8672e-01, -2.9418e-02,  1.7032e+00,  4.0243e-02,  1.0964e+00,\n",
      "          -4.2943e-01, -7.7613e-01, -1.3463e+00, -1.6947e+00, -1.6382e+00,\n",
      "           3.8687e-01,  2.1547e+00],\n",
      "         [ 9.1019e-01, -1.4873e-01,  1.6014e+00,  1.3995e-02,  6.8649e-01,\n",
      "          -4.6609e-01, -6.1476e-01, -1.4606e+00, -1.6964e+00, -1.4245e+00,\n",
      "           3.8821e-01,  2.4695e+00]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 1, 0, 1, 1]], device='cuda:0') ys\n",
      "tensor([[ 1.8821,  1.9886, -4.2026, -0.3244]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-5.9634e-01,  1.6153e+00,  1.8461e+00, -1.1703e-01,  1.5781e+00,\n",
      "           5.8687e-01, -1.1237e+00, -3.9868e-01, -7.0449e-01, -2.1012e+00,\n",
      "           1.9538e-01, -7.5867e-01],\n",
      "         [ 1.5361e-01,  5.1322e-01,  1.7468e+00,  8.2632e-02,  2.0158e+00,\n",
      "          -1.2180e-01, -1.1890e+00, -9.4654e-01, -1.5047e+00, -2.0150e+00,\n",
      "           5.3739e-01,  9.1754e-01],\n",
      "         [ 4.1859e-01,  2.7507e-01,  1.8131e+00,  2.2498e-03,  1.6174e+00,\n",
      "          -2.2698e-01, -1.0423e+00, -1.1356e+00, -1.5871e+00, -1.9409e+00,\n",
      "           5.5064e-01,  1.4812e+00],\n",
      "         [ 9.5014e-01, -3.0804e-01,  1.9143e+00, -1.4826e-01,  8.8666e-01,\n",
      "          -1.8743e-01, -1.0859e+00, -1.0961e+00, -1.6832e+00, -1.4982e+00,\n",
      "           4.1413e-01,  2.0918e+00],\n",
      "         [ 7.8672e-01, -2.9418e-02,  1.7032e+00,  4.0243e-02,  1.0964e+00,\n",
      "          -4.2943e-01, -7.7613e-01, -1.3463e+00, -1.6947e+00, -1.6382e+00,\n",
      "           3.8687e-01,  2.1547e+00],\n",
      "         [ 9.1019e-01, -1.4873e-01,  1.6014e+00,  1.3995e-02,  6.8649e-01,\n",
      "          -4.6609e-01, -6.1476e-01, -1.4606e+00, -1.6964e+00, -1.4245e+00,\n",
      "           3.8821e-01,  2.4695e+00],\n",
      "         [ 1.0201e+00, -3.0966e-01,  1.4013e+00,  1.1447e-02,  2.3091e-01,\n",
      "          -4.8021e-01, -4.9354e-01, -1.4684e+00, -1.6752e+00, -1.1515e+00,\n",
      "           3.6571e-01,  2.8064e+00]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 1, 0, 1, 1, 1]], device='cuda:0') ys\n",
      "tensor([[ 1.6112,  1.7149, -4.3948,  0.0614]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-5.9634e-01,  1.6153e+00,  1.8461e+00, -1.1703e-01,  1.5781e+00,\n",
      "           5.8687e-01, -1.1237e+00, -3.9868e-01, -7.0449e-01, -2.1012e+00,\n",
      "           1.9538e-01, -7.5867e-01],\n",
      "         [ 1.5361e-01,  5.1322e-01,  1.7468e+00,  8.2632e-02,  2.0158e+00,\n",
      "          -1.2180e-01, -1.1890e+00, -9.4654e-01, -1.5047e+00, -2.0150e+00,\n",
      "           5.3739e-01,  9.1754e-01],\n",
      "         [ 4.1859e-01,  2.7507e-01,  1.8131e+00,  2.2498e-03,  1.6174e+00,\n",
      "          -2.2698e-01, -1.0423e+00, -1.1356e+00, -1.5871e+00, -1.9409e+00,\n",
      "           5.5064e-01,  1.4812e+00],\n",
      "         [ 9.5014e-01, -3.0804e-01,  1.9143e+00, -1.4826e-01,  8.8666e-01,\n",
      "          -1.8743e-01, -1.0859e+00, -1.0961e+00, -1.6832e+00, -1.4982e+00,\n",
      "           4.1413e-01,  2.0918e+00],\n",
      "         [ 7.8672e-01, -2.9418e-02,  1.7032e+00,  4.0243e-02,  1.0964e+00,\n",
      "          -4.2943e-01, -7.7613e-01, -1.3463e+00, -1.6947e+00, -1.6382e+00,\n",
      "           3.8687e-01,  2.1547e+00],\n",
      "         [ 9.1019e-01, -1.4873e-01,  1.6014e+00,  1.3995e-02,  6.8649e-01,\n",
      "          -4.6609e-01, -6.1476e-01, -1.4606e+00, -1.6964e+00, -1.4245e+00,\n",
      "           3.8821e-01,  2.4695e+00],\n",
      "         [ 1.0201e+00, -3.0966e-01,  1.4013e+00,  1.1447e-02,  2.3091e-01,\n",
      "          -4.8021e-01, -4.9354e-01, -1.4684e+00, -1.6752e+00, -1.1515e+00,\n",
      "           3.6571e-01,  2.8064e+00],\n",
      "         [ 1.1100e+00, -4.7984e-01,  1.1571e+00,  2.8058e-02, -1.9557e-01,\n",
      "          -4.9524e-01, -3.9795e-01, -1.3771e+00, -1.6039e+00, -8.8289e-01,\n",
      "           3.0370e-01,  3.0829e+00]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 1, 0, 1, 1, 1, 1]], device='cuda:0') ys\n",
      "tensor([[ 1.2812,  1.4429, -4.4632,  0.4574]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-5.9634e-01,  1.6153e+00,  1.8461e+00, -1.1703e-01,  1.5781e+00,\n",
      "           5.8687e-01, -1.1237e+00, -3.9868e-01, -7.0449e-01, -2.1012e+00,\n",
      "           1.9538e-01, -7.5867e-01],\n",
      "         [ 1.5361e-01,  5.1322e-01,  1.7468e+00,  8.2632e-02,  2.0158e+00,\n",
      "          -1.2180e-01, -1.1890e+00, -9.4654e-01, -1.5047e+00, -2.0150e+00,\n",
      "           5.3739e-01,  9.1754e-01],\n",
      "         [ 4.1859e-01,  2.7507e-01,  1.8131e+00,  2.2498e-03,  1.6174e+00,\n",
      "          -2.2698e-01, -1.0423e+00, -1.1356e+00, -1.5871e+00, -1.9409e+00,\n",
      "           5.5064e-01,  1.4812e+00],\n",
      "         [ 9.5014e-01, -3.0804e-01,  1.9143e+00, -1.4826e-01,  8.8666e-01,\n",
      "          -1.8743e-01, -1.0859e+00, -1.0961e+00, -1.6832e+00, -1.4982e+00,\n",
      "           4.1413e-01,  2.0918e+00],\n",
      "         [ 7.8672e-01, -2.9418e-02,  1.7032e+00,  4.0243e-02,  1.0964e+00,\n",
      "          -4.2943e-01, -7.7613e-01, -1.3463e+00, -1.6947e+00, -1.6382e+00,\n",
      "           3.8687e-01,  2.1547e+00],\n",
      "         [ 9.1019e-01, -1.4873e-01,  1.6014e+00,  1.3995e-02,  6.8649e-01,\n",
      "          -4.6609e-01, -6.1476e-01, -1.4606e+00, -1.6964e+00, -1.4245e+00,\n",
      "           3.8821e-01,  2.4695e+00],\n",
      "         [ 1.0201e+00, -3.0966e-01,  1.4013e+00,  1.1447e-02,  2.3091e-01,\n",
      "          -4.8021e-01, -4.9354e-01, -1.4684e+00, -1.6752e+00, -1.1515e+00,\n",
      "           3.6571e-01,  2.8064e+00],\n",
      "         [ 1.1100e+00, -4.7984e-01,  1.1571e+00,  2.8058e-02, -1.9557e-01,\n",
      "          -4.9524e-01, -3.9795e-01, -1.3771e+00, -1.6039e+00, -8.8289e-01,\n",
      "           3.0370e-01,  3.0829e+00],\n",
      "         [ 1.1480e+00, -6.0037e-01,  9.4935e-01,  3.7641e-02, -5.3161e-01,\n",
      "          -5.1014e-01, -3.1719e-01, -1.2556e+00, -1.5157e+00, -6.6215e-01,\n",
      "           2.4044e-01,  3.2564e+00]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 1, 0, 1, 1, 1, 1, 1]], device='cuda:0') ys\n",
      "tensor([[ 0.9948,  1.2125, -4.4292,  0.7690]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "yv_i tensor([2, 1, 0, 1, 0, 1, 1, 0, 1, 3], device='cuda:0')\n",
      "prediction tensor([2, 1, 1, 0, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "\n",
      "xv_i tensor([2, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 3], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "tensor([[[-0.6280,  1.6423,  1.8768, -0.2662,  1.5226,  0.6821, -1.1435,\n",
      "          -0.3153, -0.7511, -2.0310,  0.1452, -0.6937]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2]], device='cuda:0') ys\n",
      "tensor([[ 1.9492,  2.0136, -0.7787, -2.5063]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-0.6280,  1.6423,  1.8768, -0.2662,  1.5226,  0.6821, -1.1435,\n",
      "          -0.3153, -0.7511, -2.0310,  0.1452, -0.6937],\n",
      "         [ 0.0872,  0.5035,  1.7554, -0.0619,  2.0380,  0.0854, -1.1793,\n",
      "          -0.8998, -1.5070, -2.0250,  0.5180,  0.8824]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1]], device='cuda:0') ys\n",
      "tensor([[ 2.4251,  2.3518, -2.5846, -1.6940]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-0.6280,  1.6423,  1.8768, -0.2662,  1.5226,  0.6821, -1.1435,\n",
      "          -0.3153, -0.7511, -2.0310,  0.1452, -0.6937],\n",
      "         [ 0.0872,  0.5035,  1.7554, -0.0619,  2.0380,  0.0854, -1.1793,\n",
      "          -0.8998, -1.5070, -2.0250,  0.5180,  0.8824],\n",
      "         [ 0.5726,  0.0512,  2.0009, -0.1943,  1.4484,  0.0675, -1.2951,\n",
      "          -0.9454, -1.7281, -1.6693,  0.4393,  1.4868]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 0]], device='cuda:0') ys\n",
      "tensor([[ 2.2131,  2.2936, -3.4552, -1.2203]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-0.6280,  1.6423,  1.8768, -0.2662,  1.5226,  0.6821, -1.1435,\n",
      "          -0.3153, -0.7511, -2.0310,  0.1452, -0.6937],\n",
      "         [ 0.0872,  0.5035,  1.7554, -0.0619,  2.0380,  0.0854, -1.1793,\n",
      "          -0.8998, -1.5070, -2.0250,  0.5180,  0.8824],\n",
      "         [ 0.5726,  0.0512,  2.0009, -0.1943,  1.4484,  0.0675, -1.2951,\n",
      "          -0.9454, -1.7281, -1.6693,  0.4393,  1.4868],\n",
      "         [ 0.5727,  0.0968,  1.7434, -0.0642,  1.5184, -0.1479, -1.0103,\n",
      "          -1.1843, -1.7148, -1.7651,  0.4416,  1.7640]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 0, 1]], device='cuda:0') ys\n",
      "tensor([[ 2.2335,  2.2957, -3.5508, -1.0374]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-0.6280,  1.6423,  1.8768, -0.2662,  1.5226,  0.6821, -1.1435,\n",
      "          -0.3153, -0.7511, -2.0310,  0.1452, -0.6937],\n",
      "         [ 0.0872,  0.5035,  1.7554, -0.0619,  2.0380,  0.0854, -1.1793,\n",
      "          -0.8998, -1.5070, -2.0250,  0.5180,  0.8824],\n",
      "         [ 0.5726,  0.0512,  2.0009, -0.1943,  1.4484,  0.0675, -1.2951,\n",
      "          -0.9454, -1.7281, -1.6693,  0.4393,  1.4868],\n",
      "         [ 0.5727,  0.0968,  1.7434, -0.0642,  1.5184, -0.1479, -1.0103,\n",
      "          -1.1843, -1.7148, -1.7651,  0.4416,  1.7640],\n",
      "         [ 0.7556, -0.0744,  1.7128, -0.0919,  1.0895, -0.1839, -0.8164,\n",
      "          -1.3479, -1.7584, -1.5864,  0.4151,  2.1462]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 0, 1, 1]], device='cuda:0') ys\n",
      "tensor([[ 2.1177,  2.1232, -3.9537, -0.6977]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-0.6280,  1.6423,  1.8768, -0.2662,  1.5226,  0.6821, -1.1435,\n",
      "          -0.3153, -0.7511, -2.0310,  0.1452, -0.6937],\n",
      "         [ 0.0872,  0.5035,  1.7554, -0.0619,  2.0380,  0.0854, -1.1793,\n",
      "          -0.8998, -1.5070, -2.0250,  0.5180,  0.8824],\n",
      "         [ 0.5726,  0.0512,  2.0009, -0.1943,  1.4484,  0.0675, -1.2951,\n",
      "          -0.9454, -1.7281, -1.6693,  0.4393,  1.4868],\n",
      "         [ 0.5727,  0.0968,  1.7434, -0.0642,  1.5184, -0.1479, -1.0103,\n",
      "          -1.1843, -1.7148, -1.7651,  0.4416,  1.7640],\n",
      "         [ 0.7556, -0.0744,  1.7128, -0.0919,  1.0895, -0.1839, -0.8164,\n",
      "          -1.3479, -1.7584, -1.5864,  0.4151,  2.1462],\n",
      "         [ 0.8993, -0.2161,  1.5968, -0.1160,  0.6255, -0.2078, -0.6724,\n",
      "          -1.4753, -1.7642, -1.3056,  0.3982,  2.5035]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 0, 1, 1, 1]], device='cuda:0') ys\n",
      "tensor([[ 1.9085,  1.8740, -4.2471, -0.3412]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-0.6280,  1.6423,  1.8768, -0.2662,  1.5226,  0.6821, -1.1435,\n",
      "          -0.3153, -0.7511, -2.0310,  0.1452, -0.6937],\n",
      "         [ 0.0872,  0.5035,  1.7554, -0.0619,  2.0380,  0.0854, -1.1793,\n",
      "          -0.8998, -1.5070, -2.0250,  0.5180,  0.8824],\n",
      "         [ 0.5726,  0.0512,  2.0009, -0.1943,  1.4484,  0.0675, -1.2951,\n",
      "          -0.9454, -1.7281, -1.6693,  0.4393,  1.4868],\n",
      "         [ 0.5727,  0.0968,  1.7434, -0.0642,  1.5184, -0.1479, -1.0103,\n",
      "          -1.1843, -1.7148, -1.7651,  0.4416,  1.7640],\n",
      "         [ 0.7556, -0.0744,  1.7128, -0.0919,  1.0895, -0.1839, -0.8164,\n",
      "          -1.3479, -1.7584, -1.5864,  0.4151,  2.1462],\n",
      "         [ 0.8993, -0.2161,  1.5968, -0.1160,  0.6255, -0.2078, -0.6724,\n",
      "          -1.4753, -1.7642, -1.3056,  0.3982,  2.5035],\n",
      "         [ 1.1288, -0.6601,  1.3922, -0.1988, -0.3689, -0.1136, -0.7826,\n",
      "          -1.0776, -1.6617, -0.6556,  0.2344,  3.0170]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 0, 1, 1, 1, 0]], device='cuda:0') ys\n",
      "tensor([[ 1.1865,  1.3552, -4.4923,  0.3852]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-0.6280,  1.6423,  1.8768, -0.2662,  1.5226,  0.6821, -1.1435,\n",
      "          -0.3153, -0.7511, -2.0310,  0.1452, -0.6937],\n",
      "         [ 0.0872,  0.5035,  1.7554, -0.0619,  2.0380,  0.0854, -1.1793,\n",
      "          -0.8998, -1.5070, -2.0250,  0.5180,  0.8824],\n",
      "         [ 0.5726,  0.0512,  2.0009, -0.1943,  1.4484,  0.0675, -1.2951,\n",
      "          -0.9454, -1.7281, -1.6693,  0.4393,  1.4868],\n",
      "         [ 0.5727,  0.0968,  1.7434, -0.0642,  1.5184, -0.1479, -1.0103,\n",
      "          -1.1843, -1.7148, -1.7651,  0.4416,  1.7640],\n",
      "         [ 0.7556, -0.0744,  1.7128, -0.0919,  1.0895, -0.1839, -0.8164,\n",
      "          -1.3479, -1.7584, -1.5864,  0.4151,  2.1462],\n",
      "         [ 0.8993, -0.2161,  1.5968, -0.1160,  0.6255, -0.2078, -0.6724,\n",
      "          -1.4753, -1.7642, -1.3056,  0.3982,  2.5035],\n",
      "         [ 1.1288, -0.6601,  1.3922, -0.1988, -0.3689, -0.1136, -0.7826,\n",
      "          -1.0776, -1.6617, -0.6556,  0.2344,  3.0170],\n",
      "         [ 1.1614, -0.5539,  1.1018, -0.0831, -0.2899, -0.2808, -0.5148,\n",
      "          -1.2700, -1.6579, -0.7209,  0.2225,  3.1418]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 0, 1, 1, 1, 0, 1]], device='cuda:0') ys\n",
      "tensor([[ 1.1497,  1.3716, -4.4711,  0.5277]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-0.6280,  1.6423,  1.8768, -0.2662,  1.5226,  0.6821, -1.1435,\n",
      "          -0.3153, -0.7511, -2.0310,  0.1452, -0.6937],\n",
      "         [ 0.0872,  0.5035,  1.7554, -0.0619,  2.0380,  0.0854, -1.1793,\n",
      "          -0.8998, -1.5070, -2.0250,  0.5180,  0.8824],\n",
      "         [ 0.5726,  0.0512,  2.0009, -0.1943,  1.4484,  0.0675, -1.2951,\n",
      "          -0.9454, -1.7281, -1.6693,  0.4393,  1.4868],\n",
      "         [ 0.5727,  0.0968,  1.7434, -0.0642,  1.5184, -0.1479, -1.0103,\n",
      "          -1.1843, -1.7148, -1.7651,  0.4416,  1.7640],\n",
      "         [ 0.7556, -0.0744,  1.7128, -0.0919,  1.0895, -0.1839, -0.8164,\n",
      "          -1.3479, -1.7584, -1.5864,  0.4151,  2.1462],\n",
      "         [ 0.8993, -0.2161,  1.5968, -0.1160,  0.6255, -0.2078, -0.6724,\n",
      "          -1.4753, -1.7642, -1.3056,  0.3982,  2.5035],\n",
      "         [ 1.1288, -0.6601,  1.3922, -0.1988, -0.3689, -0.1136, -0.7826,\n",
      "          -1.0776, -1.6617, -0.6556,  0.2344,  3.0170],\n",
      "         [ 1.1614, -0.5539,  1.1018, -0.0831, -0.2899, -0.2808, -0.5148,\n",
      "          -1.2700, -1.6579, -0.7209,  0.2225,  3.1418],\n",
      "         [ 1.1841, -0.6730,  0.8869, -0.0701, -0.6337, -0.3089, -0.4273,\n",
      "          -1.1356, -1.5578, -0.4835,  0.1608,  3.3023]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 0, 1, 1, 1, 0, 1, 1]], device='cuda:0') ys\n",
      "tensor([[ 0.8511,  1.1259, -4.4046,  0.8478]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "yv_i tensor([2, 1, 1, 1, 0, 0, 1, 1, 1, 3], device='cuda:0')\n",
      "prediction tensor([2, 1, 0, 1, 1, 1, 0, 1, 1, 1], device='cuda:0')\n",
      "\n",
      "xv_i tensor([2, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 3], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "tensor([[[-0.6070,  1.6234,  1.8250, -0.1175,  1.5908,  0.5833, -1.1168,\n",
      "          -0.3921, -0.6869, -2.1044,  0.2005, -0.7778]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2]], device='cuda:0') ys\n",
      "tensor([[ 1.9812,  2.0265, -0.7469, -2.5291]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-0.6070,  1.6234,  1.8250, -0.1175,  1.5908,  0.5833, -1.1168,\n",
      "          -0.3921, -0.6869, -2.1044,  0.2005, -0.7778],\n",
      "         [ 0.1442,  0.5207,  1.7357,  0.0805,  2.0328, -0.1220, -1.1808,\n",
      "          -0.9456, -1.4918, -2.0271,  0.5451,  0.8974]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1]], device='cuda:0') ys\n",
      "tensor([[ 2.3653,  2.4058, -2.6330, -1.6411]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-6.0704e-01,  1.6234e+00,  1.8250e+00, -1.1749e-01,  1.5908e+00,\n",
      "           5.8328e-01, -1.1168e+00, -3.9209e-01, -6.8694e-01, -2.1044e+00,\n",
      "           2.0055e-01, -7.7783e-01],\n",
      "         [ 1.4419e-01,  5.2069e-01,  1.7357e+00,  8.0455e-02,  2.0328e+00,\n",
      "          -1.2204e-01, -1.1808e+00, -9.4557e-01, -1.4918e+00, -2.0271e+00,\n",
      "           5.4510e-01,  8.9738e-01],\n",
      "         [ 4.1321e-01,  2.7984e-01,  1.8045e+00, -9.7541e-04,  1.6324e+00,\n",
      "          -2.2652e-01, -1.0346e+00, -1.1372e+00, -1.5768e+00, -1.9556e+00,\n",
      "           5.5983e-01,  1.4668e+00]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 1]], device='cuda:0') ys\n",
      "tensor([[ 2.3416,  2.3251, -3.2999, -1.2967]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-6.0704e-01,  1.6234e+00,  1.8250e+00, -1.1749e-01,  1.5908e+00,\n",
      "           5.8328e-01, -1.1168e+00, -3.9209e-01, -6.8694e-01, -2.1044e+00,\n",
      "           2.0055e-01, -7.7783e-01],\n",
      "         [ 1.4419e-01,  5.2069e-01,  1.7357e+00,  8.0455e-02,  2.0328e+00,\n",
      "          -1.2204e-01, -1.1808e+00, -9.4557e-01, -1.4918e+00, -2.0271e+00,\n",
      "           5.4510e-01,  8.9738e-01],\n",
      "         [ 4.1321e-01,  2.7984e-01,  1.8045e+00, -9.7541e-04,  1.6324e+00,\n",
      "          -2.2652e-01, -1.0346e+00, -1.1372e+00, -1.5768e+00, -1.9556e+00,\n",
      "           5.5983e-01,  1.4668e+00],\n",
      "         [ 9.5279e-01, -3.1606e-01,  1.9061e+00, -1.4970e-01,  8.9758e-01,\n",
      "          -1.9291e-01, -1.0787e+00, -1.0993e+00, -1.6770e+00, -1.5072e+00,\n",
      "           4.2395e-01,  2.0906e+00]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 1, 0]], device='cuda:0') ys\n",
      "tensor([[ 1.9923,  2.1150, -4.0742, -0.6525]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-6.0704e-01,  1.6234e+00,  1.8250e+00, -1.1749e-01,  1.5908e+00,\n",
      "           5.8328e-01, -1.1168e+00, -3.9209e-01, -6.8694e-01, -2.1044e+00,\n",
      "           2.0055e-01, -7.7783e-01],\n",
      "         [ 1.4419e-01,  5.2069e-01,  1.7357e+00,  8.0456e-02,  2.0328e+00,\n",
      "          -1.2204e-01, -1.1808e+00, -9.4557e-01, -1.4918e+00, -2.0271e+00,\n",
      "           5.4510e-01,  8.9738e-01],\n",
      "         [ 4.1321e-01,  2.7984e-01,  1.8045e+00, -9.7540e-04,  1.6324e+00,\n",
      "          -2.2652e-01, -1.0346e+00, -1.1372e+00, -1.5768e+00, -1.9556e+00,\n",
      "           5.5983e-01,  1.4668e+00],\n",
      "         [ 9.5279e-01, -3.1606e-01,  1.9061e+00, -1.4970e-01,  8.9758e-01,\n",
      "          -1.9291e-01, -1.0787e+00, -1.0993e+00, -1.6770e+00, -1.5072e+00,\n",
      "           4.2394e-01,  2.0906e+00],\n",
      "         [ 7.8101e-01, -3.2726e-02,  1.6981e+00,  3.9859e-02,  1.1088e+00,\n",
      "          -4.3134e-01, -7.6294e-01, -1.3534e+00, -1.6890e+00, -1.6492e+00,\n",
      "           3.9688e-01,  2.1477e+00]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 1, 0, 1]], device='cuda:0') ys\n",
      "tensor([[ 2.0598,  2.2021, -3.9303, -0.6412]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-6.0704e-01,  1.6234e+00,  1.8250e+00, -1.1749e-01,  1.5908e+00,\n",
      "           5.8328e-01, -1.1168e+00, -3.9209e-01, -6.8694e-01, -2.1044e+00,\n",
      "           2.0055e-01, -7.7783e-01],\n",
      "         [ 1.4419e-01,  5.2069e-01,  1.7357e+00,  8.0456e-02,  2.0328e+00,\n",
      "          -1.2204e-01, -1.1808e+00, -9.4557e-01, -1.4918e+00, -2.0271e+00,\n",
      "           5.4510e-01,  8.9738e-01],\n",
      "         [ 4.1321e-01,  2.7984e-01,  1.8045e+00, -9.7540e-04,  1.6324e+00,\n",
      "          -2.2652e-01, -1.0346e+00, -1.1372e+00, -1.5768e+00, -1.9556e+00,\n",
      "           5.5983e-01,  1.4668e+00],\n",
      "         [ 9.5279e-01, -3.1606e-01,  1.9061e+00, -1.4970e-01,  8.9758e-01,\n",
      "          -1.9291e-01, -1.0787e+00, -1.0993e+00, -1.6770e+00, -1.5072e+00,\n",
      "           4.2394e-01,  2.0906e+00],\n",
      "         [ 7.8101e-01, -3.2726e-02,  1.6981e+00,  3.9859e-02,  1.1088e+00,\n",
      "          -4.3134e-01, -7.6294e-01, -1.3534e+00, -1.6890e+00, -1.6492e+00,\n",
      "           3.9688e-01,  2.1477e+00],\n",
      "         [ 9.0782e-01, -1.5274e-01,  1.5971e+00,  1.2994e-02,  6.9517e-01,\n",
      "          -4.6773e-01, -5.9824e-01, -1.4705e+00, -1.6903e+00, -1.4362e+00,\n",
      "           3.9783e-01,  2.4630e+00]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 1, 0, 1, 1]], device='cuda:0') ys\n",
      "tensor([[ 1.8986,  1.9839, -4.1978, -0.3281]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-6.0704e-01,  1.6234e+00,  1.8250e+00, -1.1749e-01,  1.5908e+00,\n",
      "           5.8328e-01, -1.1168e+00, -3.9209e-01, -6.8694e-01, -2.1044e+00,\n",
      "           2.0055e-01, -7.7783e-01],\n",
      "         [ 1.4419e-01,  5.2069e-01,  1.7357e+00,  8.0456e-02,  2.0328e+00,\n",
      "          -1.2204e-01, -1.1808e+00, -9.4557e-01, -1.4918e+00, -2.0271e+00,\n",
      "           5.4510e-01,  8.9738e-01],\n",
      "         [ 4.1321e-01,  2.7984e-01,  1.8045e+00, -9.7540e-04,  1.6324e+00,\n",
      "          -2.2652e-01, -1.0346e+00, -1.1372e+00, -1.5768e+00, -1.9556e+00,\n",
      "           5.5983e-01,  1.4668e+00],\n",
      "         [ 9.5279e-01, -3.1606e-01,  1.9061e+00, -1.4970e-01,  8.9758e-01,\n",
      "          -1.9291e-01, -1.0787e+00, -1.0993e+00, -1.6770e+00, -1.5072e+00,\n",
      "           4.2394e-01,  2.0906e+00],\n",
      "         [ 7.8101e-01, -3.2726e-02,  1.6981e+00,  3.9859e-02,  1.1088e+00,\n",
      "          -4.3134e-01, -7.6294e-01, -1.3534e+00, -1.6890e+00, -1.6492e+00,\n",
      "           3.9688e-01,  2.1477e+00],\n",
      "         [ 9.0782e-01, -1.5274e-01,  1.5971e+00,  1.2994e-02,  6.9517e-01,\n",
      "          -4.6773e-01, -5.9824e-01, -1.4705e+00, -1.6903e+00, -1.4362e+00,\n",
      "           3.9783e-01,  2.4630e+00],\n",
      "         [ 1.0216e+00, -3.1296e-01,  1.3935e+00,  9.3145e-03,  2.3609e-01,\n",
      "          -4.8057e-01, -4.7731e-01, -1.4794e+00, -1.6705e+00, -1.1587e+00,\n",
      "           3.7227e-01,  2.8040e+00]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 1, 0, 1, 1, 1]], device='cuda:0') ys\n",
      "tensor([[ 1.6221,  1.7098, -4.3926,  0.0619]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-6.0704e-01,  1.6234e+00,  1.8250e+00, -1.1749e-01,  1.5908e+00,\n",
      "           5.8328e-01, -1.1168e+00, -3.9209e-01, -6.8694e-01, -2.1044e+00,\n",
      "           2.0055e-01, -7.7783e-01],\n",
      "         [ 1.4419e-01,  5.2069e-01,  1.7357e+00,  8.0456e-02,  2.0328e+00,\n",
      "          -1.2204e-01, -1.1808e+00, -9.4557e-01, -1.4918e+00, -2.0271e+00,\n",
      "           5.4510e-01,  8.9738e-01],\n",
      "         [ 4.1321e-01,  2.7984e-01,  1.8045e+00, -9.7540e-04,  1.6324e+00,\n",
      "          -2.2652e-01, -1.0346e+00, -1.1372e+00, -1.5768e+00, -1.9556e+00,\n",
      "           5.5983e-01,  1.4668e+00],\n",
      "         [ 9.5279e-01, -3.1606e-01,  1.9061e+00, -1.4970e-01,  8.9758e-01,\n",
      "          -1.9291e-01, -1.0787e+00, -1.0993e+00, -1.6770e+00, -1.5072e+00,\n",
      "           4.2394e-01,  2.0906e+00],\n",
      "         [ 7.8101e-01, -3.2726e-02,  1.6981e+00,  3.9859e-02,  1.1088e+00,\n",
      "          -4.3134e-01, -7.6294e-01, -1.3534e+00, -1.6890e+00, -1.6492e+00,\n",
      "           3.9688e-01,  2.1477e+00],\n",
      "         [ 9.0782e-01, -1.5274e-01,  1.5971e+00,  1.2994e-02,  6.9517e-01,\n",
      "          -4.6773e-01, -5.9824e-01, -1.4705e+00, -1.6903e+00, -1.4362e+00,\n",
      "           3.9783e-01,  2.4630e+00],\n",
      "         [ 1.0216e+00, -3.1296e-01,  1.3935e+00,  9.3145e-03,  2.3609e-01,\n",
      "          -4.8057e-01, -4.7731e-01, -1.4794e+00, -1.6705e+00, -1.1587e+00,\n",
      "           3.7227e-01,  2.8040e+00],\n",
      "         [ 1.1123e+00, -4.8484e-01,  1.1475e+00,  2.6358e-02, -1.9454e-01,\n",
      "          -4.9542e-01, -3.8158e-01, -1.3865e+00, -1.5990e+00, -8.8747e-01,\n",
      "           3.0932e-01,  3.0829e+00]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 1, 0, 1, 1, 1, 1]], device='cuda:0') ys\n",
      "tensor([[ 1.2887,  1.4356, -4.4616,  0.4615]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-6.0704e-01,  1.6234e+00,  1.8250e+00, -1.1749e-01,  1.5908e+00,\n",
      "           5.8328e-01, -1.1168e+00, -3.9209e-01, -6.8694e-01, -2.1044e+00,\n",
      "           2.0055e-01, -7.7783e-01],\n",
      "         [ 1.4419e-01,  5.2069e-01,  1.7357e+00,  8.0456e-02,  2.0328e+00,\n",
      "          -1.2204e-01, -1.1808e+00, -9.4557e-01, -1.4918e+00, -2.0271e+00,\n",
      "           5.4510e-01,  8.9738e-01],\n",
      "         [ 4.1321e-01,  2.7984e-01,  1.8045e+00, -9.7540e-04,  1.6324e+00,\n",
      "          -2.2652e-01, -1.0346e+00, -1.1372e+00, -1.5768e+00, -1.9556e+00,\n",
      "           5.5983e-01,  1.4668e+00],\n",
      "         [ 9.5279e-01, -3.1606e-01,  1.9061e+00, -1.4970e-01,  8.9758e-01,\n",
      "          -1.9291e-01, -1.0787e+00, -1.0993e+00, -1.6770e+00, -1.5072e+00,\n",
      "           4.2394e-01,  2.0906e+00],\n",
      "         [ 7.8101e-01, -3.2726e-02,  1.6981e+00,  3.9859e-02,  1.1088e+00,\n",
      "          -4.3134e-01, -7.6294e-01, -1.3534e+00, -1.6890e+00, -1.6492e+00,\n",
      "           3.9688e-01,  2.1477e+00],\n",
      "         [ 9.0782e-01, -1.5274e-01,  1.5971e+00,  1.2994e-02,  6.9517e-01,\n",
      "          -4.6773e-01, -5.9824e-01, -1.4705e+00, -1.6903e+00, -1.4362e+00,\n",
      "           3.9783e-01,  2.4630e+00],\n",
      "         [ 1.0216e+00, -3.1296e-01,  1.3935e+00,  9.3145e-03,  2.3609e-01,\n",
      "          -4.8057e-01, -4.7731e-01, -1.4794e+00, -1.6705e+00, -1.1587e+00,\n",
      "           3.7227e-01,  2.8040e+00],\n",
      "         [ 1.1123e+00, -4.8484e-01,  1.1475e+00,  2.6358e-02, -1.9454e-01,\n",
      "          -4.9542e-01, -3.8158e-01, -1.3865e+00, -1.5990e+00, -8.8747e-01,\n",
      "           3.0932e-01,  3.0829e+00],\n",
      "         [ 1.1500e+00, -6.0611e-01,  9.3864e-01,  3.5227e-02, -5.3297e-01,\n",
      "          -5.1070e-01, -3.0090e-01, -1.2630e+00, -1.5097e+00, -6.6450e-01,\n",
      "           2.4515e-01,  3.2576e+00]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 1, 0, 1, 1, 1, 1, 1]], device='cuda:0') ys\n",
      "tensor([[ 0.9996,  1.2035, -4.4264,  0.7754]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "yv_i tensor([2, 0, 0, 0, 0, 1, 1, 1, 0, 3], device='cuda:0')\n",
      "prediction tensor([2, 1, 1, 0, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "\n",
      "xv_i tensor([2, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 3], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "tensor([[[-0.6280,  1.6423,  1.8768, -0.2662,  1.5226,  0.6821, -1.1435,\n",
      "          -0.3153, -0.7511, -2.0310,  0.1452, -0.6937]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2]], device='cuda:0') ys\n",
      "tensor([[ 1.9492,  2.0136, -0.7787, -2.5063]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-0.6280,  1.6423,  1.8768, -0.2662,  1.5226,  0.6821, -1.1435,\n",
      "          -0.3153, -0.7511, -2.0310,  0.1452, -0.6937],\n",
      "         [ 0.0872,  0.5035,  1.7554, -0.0619,  2.0380,  0.0854, -1.1793,\n",
      "          -0.8998, -1.5070, -2.0250,  0.5180,  0.8824]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1]], device='cuda:0') ys\n",
      "tensor([[ 2.4251,  2.3518, -2.5846, -1.6940]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-0.6280,  1.6423,  1.8768, -0.2662,  1.5226,  0.6821, -1.1435,\n",
      "          -0.3153, -0.7511, -2.0310,  0.1452, -0.6937],\n",
      "         [ 0.0872,  0.5035,  1.7554, -0.0619,  2.0380,  0.0854, -1.1793,\n",
      "          -0.8998, -1.5070, -2.0250,  0.5180,  0.8824],\n",
      "         [ 0.5726,  0.0512,  2.0009, -0.1943,  1.4484,  0.0675, -1.2951,\n",
      "          -0.9454, -1.7281, -1.6693,  0.4393,  1.4868]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 0]], device='cuda:0') ys\n",
      "tensor([[ 2.2131,  2.2936, -3.4552, -1.2203]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-0.6280,  1.6423,  1.8768, -0.2662,  1.5226,  0.6821, -1.1435,\n",
      "          -0.3153, -0.7511, -2.0310,  0.1452, -0.6937],\n",
      "         [ 0.0872,  0.5035,  1.7554, -0.0619,  2.0380,  0.0854, -1.1793,\n",
      "          -0.8998, -1.5070, -2.0250,  0.5180,  0.8824],\n",
      "         [ 0.5726,  0.0512,  2.0009, -0.1943,  1.4484,  0.0675, -1.2951,\n",
      "          -0.9454, -1.7281, -1.6693,  0.4393,  1.4868],\n",
      "         [ 0.5727,  0.0968,  1.7434, -0.0642,  1.5184, -0.1479, -1.0103,\n",
      "          -1.1843, -1.7148, -1.7651,  0.4416,  1.7640]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 0, 1]], device='cuda:0') ys\n",
      "tensor([[ 2.2335,  2.2957, -3.5508, -1.0374]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-0.6280,  1.6423,  1.8768, -0.2662,  1.5226,  0.6821, -1.1435,\n",
      "          -0.3153, -0.7511, -2.0310,  0.1452, -0.6937],\n",
      "         [ 0.0872,  0.5035,  1.7554, -0.0619,  2.0380,  0.0854, -1.1793,\n",
      "          -0.8998, -1.5070, -2.0250,  0.5180,  0.8824],\n",
      "         [ 0.5726,  0.0512,  2.0009, -0.1943,  1.4484,  0.0675, -1.2951,\n",
      "          -0.9454, -1.7281, -1.6693,  0.4393,  1.4868],\n",
      "         [ 0.5727,  0.0968,  1.7434, -0.0642,  1.5184, -0.1479, -1.0103,\n",
      "          -1.1843, -1.7148, -1.7651,  0.4416,  1.7640],\n",
      "         [ 0.7556, -0.0744,  1.7128, -0.0919,  1.0895, -0.1839, -0.8164,\n",
      "          -1.3479, -1.7584, -1.5864,  0.4151,  2.1462]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 0, 1, 1]], device='cuda:0') ys\n",
      "tensor([[ 2.1177,  2.1232, -3.9537, -0.6977]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-0.6280,  1.6423,  1.8768, -0.2662,  1.5226,  0.6821, -1.1435,\n",
      "          -0.3153, -0.7511, -2.0310,  0.1452, -0.6937],\n",
      "         [ 0.0872,  0.5035,  1.7554, -0.0619,  2.0380,  0.0854, -1.1793,\n",
      "          -0.8998, -1.5070, -2.0250,  0.5180,  0.8824],\n",
      "         [ 0.5726,  0.0512,  2.0009, -0.1943,  1.4484,  0.0675, -1.2951,\n",
      "          -0.9454, -1.7281, -1.6693,  0.4393,  1.4868],\n",
      "         [ 0.5727,  0.0968,  1.7434, -0.0642,  1.5184, -0.1479, -1.0103,\n",
      "          -1.1843, -1.7148, -1.7651,  0.4416,  1.7640],\n",
      "         [ 0.7556, -0.0744,  1.7128, -0.0919,  1.0895, -0.1839, -0.8164,\n",
      "          -1.3479, -1.7584, -1.5864,  0.4151,  2.1462],\n",
      "         [ 0.8993, -0.2161,  1.5968, -0.1160,  0.6255, -0.2078, -0.6724,\n",
      "          -1.4753, -1.7642, -1.3056,  0.3982,  2.5035]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 0, 1, 1, 1]], device='cuda:0') ys\n",
      "tensor([[ 1.9085,  1.8740, -4.2471, -0.3412]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-0.6280,  1.6423,  1.8768, -0.2662,  1.5226,  0.6821, -1.1435,\n",
      "          -0.3153, -0.7511, -2.0310,  0.1452, -0.6937],\n",
      "         [ 0.0872,  0.5035,  1.7554, -0.0619,  2.0380,  0.0854, -1.1793,\n",
      "          -0.8998, -1.5070, -2.0250,  0.5180,  0.8824],\n",
      "         [ 0.5726,  0.0512,  2.0009, -0.1943,  1.4484,  0.0675, -1.2951,\n",
      "          -0.9454, -1.7281, -1.6693,  0.4393,  1.4868],\n",
      "         [ 0.5727,  0.0968,  1.7434, -0.0642,  1.5184, -0.1479, -1.0103,\n",
      "          -1.1843, -1.7148, -1.7651,  0.4416,  1.7640],\n",
      "         [ 0.7556, -0.0744,  1.7128, -0.0919,  1.0895, -0.1839, -0.8164,\n",
      "          -1.3479, -1.7584, -1.5864,  0.4151,  2.1462],\n",
      "         [ 0.8993, -0.2161,  1.5968, -0.1160,  0.6255, -0.2078, -0.6724,\n",
      "          -1.4753, -1.7642, -1.3056,  0.3982,  2.5035],\n",
      "         [ 1.1288, -0.6601,  1.3922, -0.1988, -0.3689, -0.1136, -0.7826,\n",
      "          -1.0776, -1.6617, -0.6556,  0.2344,  3.0170]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 0, 1, 1, 1, 0]], device='cuda:0') ys\n",
      "tensor([[ 1.1865,  1.3552, -4.4923,  0.3852]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-0.6280,  1.6423,  1.8768, -0.2662,  1.5226,  0.6821, -1.1435,\n",
      "          -0.3153, -0.7511, -2.0310,  0.1452, -0.6937],\n",
      "         [ 0.0872,  0.5035,  1.7554, -0.0619,  2.0380,  0.0854, -1.1793,\n",
      "          -0.8998, -1.5070, -2.0250,  0.5180,  0.8824],\n",
      "         [ 0.5726,  0.0512,  2.0009, -0.1943,  1.4484,  0.0675, -1.2951,\n",
      "          -0.9454, -1.7281, -1.6693,  0.4393,  1.4868],\n",
      "         [ 0.5727,  0.0968,  1.7434, -0.0642,  1.5184, -0.1479, -1.0103,\n",
      "          -1.1843, -1.7148, -1.7651,  0.4416,  1.7640],\n",
      "         [ 0.7556, -0.0744,  1.7128, -0.0919,  1.0895, -0.1839, -0.8164,\n",
      "          -1.3479, -1.7584, -1.5864,  0.4151,  2.1462],\n",
      "         [ 0.8993, -0.2161,  1.5968, -0.1160,  0.6255, -0.2078, -0.6724,\n",
      "          -1.4753, -1.7642, -1.3056,  0.3982,  2.5035],\n",
      "         [ 1.1288, -0.6601,  1.3922, -0.1988, -0.3689, -0.1136, -0.7826,\n",
      "          -1.0776, -1.6617, -0.6556,  0.2344,  3.0170],\n",
      "         [ 1.1614, -0.5539,  1.1018, -0.0831, -0.2899, -0.2808, -0.5148,\n",
      "          -1.2700, -1.6579, -0.7209,  0.2225,  3.1418]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 0, 1, 1, 1, 0, 1]], device='cuda:0') ys\n",
      "tensor([[ 1.1497,  1.3716, -4.4711,  0.5277]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[-0.6280,  1.6423,  1.8768, -0.2662,  1.5226,  0.6821, -1.1435,\n",
      "          -0.3153, -0.7511, -2.0310,  0.1452, -0.6937],\n",
      "         [ 0.0872,  0.5035,  1.7554, -0.0619,  2.0380,  0.0854, -1.1793,\n",
      "          -0.8998, -1.5070, -2.0250,  0.5180,  0.8824],\n",
      "         [ 0.5726,  0.0512,  2.0009, -0.1943,  1.4484,  0.0675, -1.2951,\n",
      "          -0.9454, -1.7281, -1.6693,  0.4393,  1.4868],\n",
      "         [ 0.5727,  0.0968,  1.7434, -0.0642,  1.5184, -0.1479, -1.0103,\n",
      "          -1.1843, -1.7148, -1.7651,  0.4416,  1.7640],\n",
      "         [ 0.7556, -0.0744,  1.7128, -0.0919,  1.0895, -0.1839, -0.8164,\n",
      "          -1.3479, -1.7584, -1.5864,  0.4151,  2.1462],\n",
      "         [ 0.8993, -0.2161,  1.5968, -0.1160,  0.6255, -0.2078, -0.6724,\n",
      "          -1.4753, -1.7642, -1.3056,  0.3982,  2.5035],\n",
      "         [ 1.1288, -0.6601,  1.3922, -0.1988, -0.3689, -0.1136, -0.7826,\n",
      "          -1.0776, -1.6617, -0.6556,  0.2344,  3.0170],\n",
      "         [ 1.1614, -0.5539,  1.1018, -0.0831, -0.2899, -0.2808, -0.5148,\n",
      "          -1.2700, -1.6579, -0.7209,  0.2225,  3.1418],\n",
      "         [ 1.1841, -0.6730,  0.8869, -0.0701, -0.6337, -0.3089, -0.4273,\n",
      "          -1.1356, -1.5578, -0.4835,  0.1608,  3.3023]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 1, 0, 1, 1, 1, 0, 1, 1]], device='cuda:0') ys\n",
      "tensor([[ 0.8511,  1.1259, -4.4046,  0.8478]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "yv_i tensor([2, 1, 1, 1, 0, 0, 1, 1, 1, 3], device='cuda:0')\n",
      "prediction tensor([2, 1, 0, 1, 1, 1, 0, 1, 1, 1], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# function to generate output sequence using greedy algorithm\n",
    "def greedy_decode(model, src, src_mask, max_len):\n",
    "    \"\"\"Standard autoregressive decoder output scheme.\n",
    "\n",
    "    Current issue: my train batching didn't have time-sliced data, so I think the \n",
    "    model has no idea what to do with a length-1 <SOS> sequence as input.\n",
    "    \"\"\"\n",
    "    src = src.to(DEVICE)\n",
    "    # src_mask = src_mask.to(DEVICE)\n",
    "    src_mask = None  # FIXME\n",
    "    \n",
    "    # For a single example evaluation, we need to add a dummy batch dimension (1, *) with unsqueeze(0)\n",
    "    memory = model.encode(src.unsqueeze(0), src_mask)\n",
    "    # The [1, 1] shape starts us off with a dummy batch dimension \n",
    "    ys = torch.ones(1, 1).fill_(SOS_TOKEN).type(torch.long).to(DEVICE)\n",
    "\n",
    "    # FIXME: should I enforce the length? Or should I enforce the length+1, \n",
    "    # and then checksum for an EOS? Or should I allow variable length :(\n",
    "    for i in range(max_len - 1): # -1 since we start with SOS\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (ucan_transformer.generate_square_subsequent_mask(ys.size(1), device=DEVICE).type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(tgt=ys, memory=memory, tgt_mask=tgt_mask) # (1, tgt_seq_len, emb_dim)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        print(out, \"out\")\n",
    "        print(ys, \"ys\")\n",
    "        print(prob, \"prob\")\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "        print()\n",
    "    return ys\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "# inp\n",
    "def translate(model: torch.nn.Module, src):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        src: tensor. single input bitstring of length 2n + 2. Shape (2n + 2,) \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    seq_len = src.shape[0] - 2\n",
    "    out_len = seq_len // 2 + 2 # 2:1 UCAN conversion, plus EOS/SOS\n",
    "    # src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    src_mask = None\n",
    "    tgt_tokens = greedy_decode(model, src, src_mask, max_len=out_len).flatten()\n",
    "\n",
    "    return tgt_tokens\n",
    "\n",
    "\n",
    "xv, yv = get_batch('val')\n",
    "\n",
    "for xvi, yvi in zip(xv, yv):\n",
    "    print(\"xv_i\", xvi)\n",
    "    pred = translate(model, xvi)\n",
    "    print(\"yv_i\", yvi)\n",
    "    print(\"prediction\", pred)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try to use DataLoader, Dataset, etc. for batching\n",
    "\n",
    "def train_step(model, optimizer):\n",
    "    losses = 0\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        # These aren't technically epochs i guess.\n",
    "        xb, yb = get_batch('train')\n",
    "\n",
    "        logits = model(xb, yb, None, None) # No masks for now\n",
    "        optimizer.zero_grad(set_to_none=True) #?\n",
    "\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), yb.reshape(-1))\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        losses += loss.item() / BATCH_SIZE # check this\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16, 10]) embedding\n",
      "torch.Size([4, 16, 10]) positional\n",
      "torch.Size([4, 8, 2]) model forward\n"
     ]
    }
   ],
   "source": [
    "emb_size = 10\n",
    "x, y = get_batch('train')\n",
    "embedding = TokenEmbedding(2, emb_size)\n",
    "xx = embedding.forward(x)\n",
    "print(xx.shape, \"embedding\")\n",
    "positional = PositionalEncoding(emb_size, 0.1)\n",
    "xxx = positional.forward(xx)\n",
    "print(xxx.shape, \"positional\")\n",
    "\n",
    "xxxx = model.forward(x, y, None, None)\n",
    "print(xxxx.shape, \"model forward\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "Re-adapt this for prediction/evaluation, and probably incorporate as a function into the transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Scratchwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 18\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "\n",
    "# function to generate output sequence using greedy algorithm\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "# inp\n",
    "def translate(model: torch.nn.Module, src):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        single\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
    "\n",
    "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "SRC_LANGUAGE = 'fuck'\n",
    "TGT_LANGUAGE = 'this'\n",
    "\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "    \n",
    "\n",
    "val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "# function to collate data samples into batch tensors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multi30k, Multi30k\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Fixme\u001b[39;00m\n\u001b[0;32m      5\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "\n",
    "# Fixme\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(train_dataloader))\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
