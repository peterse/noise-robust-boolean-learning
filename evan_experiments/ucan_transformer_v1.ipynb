{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# reload magic\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import deterministic.py using local file path\n",
    "import sys\n",
    "sys.path.append('../sequence_generators')\n",
    "import deterministic\n",
    "\n",
    "sys.path.append('../ucan')\n",
    "import ucan\n",
    "import ucan_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UCAN dataset\n",
    "\n",
    "The UCAN dataset has inputs $(Y \\oplus \\Gamma, \\Delta)$, targets $Y$, and 'hidden variable' $\\Gamma$, all of which are length-n bitstrings. Note that I'm changing my notation from whats in the writeup to match input/target sequence labels better.\n",
    "\n",
    "This has to be done in a few steps:\n",
    "1. Generate a dataset for $Y$, array $(n_{data}, n)$\n",
    "2. Generate a matched dataset $(\\Gamma, \\Delta)$, array $(N, n, 2)$ for whatever version of UCAN\n",
    "3. Compose and discard, i.e. $X = Y \\oplus \\Gamma$, data = $[Z, \\Delta]$ array $(N, 2n)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a deterministic set of sequences and batch them\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import ucan\n",
    "\n",
    "# Get a deterministic set of sequences of length n\n",
    "# with the sos and eos tokens, this becomes length 2+n\n",
    "n = 8\n",
    "n_data = 100\n",
    "\n",
    "gen = deterministic.SequenceGen(lookback=4, seed=228, number_of_generating_methods=1)\n",
    "Y, _ = gen.deterministically_generate_sequences(length=n, num_seq=n_data, save=False)\n",
    "Y = np.array(Y, dtype=np.int32)\n",
    "\n",
    "# Generate our UCAN. For the first experiment, gamma=delta (so p_diff = 0)\n",
    "p_diff = 0\n",
    "p0_delta = 0.5 # if this is too hard, change to 1 (i.e. ignore delta)\n",
    "out = ucan.bitwise_ucan_v1(n, n_data, p0_delta, p_diff, seed=228)\n",
    "gammas = out[:,:,0]\n",
    "deltas = out[:,:,1]\n",
    "\n",
    "# Generate the noise and concatenate the data\n",
    "Z = Y ^ gammas\n",
    "X = np.concatenate((Z, deltas), axis=1) # (n_data, 2n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# DEVICE = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the entire text into integers\n",
    "data = torch.tensor(X, dtype=torch.int) # (n_data, 2n)\n",
    "targets = torch.tensor(Y, dtype=torch.int) # (n_data, n)\n",
    "\n",
    "# SOS is needed to get reasonable marginals for first token / seed generator\n",
    "# I'm not sure if EOS is necessary for anything\n",
    "SOS_TOKEN = 2\n",
    "EOS_TOKEN = 3\n",
    "data = torch.cat((torch.ones(data.size(0), 1, dtype=torch.int) * SOS_TOKEN, data, torch.ones(data.size(0), 1, dtype=torch.int) * EOS_TOKEN), dim=1)\n",
    "targets = torch.cat((torch.ones(targets.size(0), 1, dtype=torch.int) * SOS_TOKEN, targets, torch.ones(targets.size(0), 1, dtype=torch.int) * EOS_TOKEN), dim=1)\n",
    "\n",
    "n_train = int(len(data) * 0.9)\n",
    "train_data = data[:n_train]\n",
    "val_data = data[n_train:]\n",
    "train_targets = targets[:n_train]\n",
    "val_targets = targets[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337) # to get what andrej karpathy got\n",
    "\n",
    "block_size = 8 # this is the context window size. \n",
    "batch_size = 4\n",
    "\n",
    "def get_batch(split):\n",
    "    \"\"\"Generate a small batch of data with inputs x, targets y.\n",
    "    \n",
    "    Note that there's no interesting block structure going on here,\n",
    "    since we're learning a map from x \\in \\{0,1\\}^{2n} -> y \\in \\{0,1\\}^n.\n",
    "\n",
    "    Outputs are shaped (batch_size, 2n) and (batch_size, n) respectively.\n",
    "    This corresponds to `batch_first` in the torch transformer\n",
    "    \"\"\"\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    targets = train_targets if split == 'train' else val_targets\n",
    "\n",
    "    ix = torch.randint(0, len(data), (batch_size,)) # indices for batch sample\n",
    "    x = torch.stack([data[i] for i in ix]) # `block_size` many data points\n",
    "    y = torch.stack([targets[i] for i in ix]) # target for each input (the Y string)\n",
    "    # CUDA has a problem with short int. see: https://stackoverflow.com/questions/69742930/runtimeerror-nll-loss-forward-reduce-cuda-kernel-2d-index-not-implemented-for\n",
    "    y = y.type(torch.LongTensor)\n",
    "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture notes:\n",
    "\n",
    " - Do we want/need a causal mask in our decoder??? This is not an autoregressive task\n",
    " - Reminder to modify the mask for the encoder to reflect the position-wise dependence of $\\Delta$, $Z$\n",
    " - I do not have a tokenizer nor plans for one???\n",
    "\n",
    "\n",
    " #### Data notes:\n",
    "\n",
    "  - I am avoiding using <EOS> and <BOS> partly because I can get away with bool-type data right now. I don't know how smart that actually is, since these values get cast regardless...\n",
    "\n",
    "  Notes from nn.Transformer https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
    "\n",
    " - If a boolean tensor is provided for any of the [src/tgt/memory]_mask arguments, positions with a True value are not allowed to participate in the attention, which is the opposite of the definition for attn_mask in torch.nn.functional.scaled_dot_product_attention().\n",
    " - src, tgt, memory mask are masks applied to the x input seq, the y target seq, and the last layer of encoder seq resp.\n",
    "\n",
    "Things I'm confused about:\n",
    " - at training time, we have targets, so we can embed (positional and vector) the targets to feed into the decoder. At evaluation time, we start with a source vector, encode into memory, then autoregressively build the target I guess?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encoder_layers = 2\n",
    "num_decoder_layers = 2\n",
    "emb_size = 12\n",
    "nhead = 4\n",
    "src_vocab_size = 4\n",
    "tgt_vocab_size = 4\n",
    "dim_feedforward = 64\n",
    "dropout = 0.1\n",
    "\n",
    "# hyperparameters\n",
    "eval_iters = 200\n",
    "LEARNING_RATE = 0.0001\n",
    "# BATCH_SIZE = 32\n",
    "\n",
    "# Train loop\n",
    "eval_interval = 50\n",
    "max_iters = 1000\n",
    "\n",
    "model = ucan_transformer.Seq2SeqTransformer(\n",
    "    num_encoder_layers, \n",
    "    num_decoder_layers, \n",
    "    emb_size, \n",
    "    nhead, \n",
    "    src_vocab_size, \n",
    "    tgt_vocab_size, \n",
    "    dim_feedforward, \n",
    "    dropout\n",
    ").to(DEVICE)\n",
    "# `forward` signature: (src, trg, src_mask, tgt_mask, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    # Average the loss over many batches. Hardcoded cross_entropy loss\n",
    "    # Needs to be in same namespace as model and get_batch\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    tgt_mask = ucan_transformer.generate_square_subsequent_mask(n + 2, device=DEVICE)\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y, None, tgt_mask)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peter\\Desktop\\projects\\MindReadingAutobot\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 1.7526, val loss 1.7280\n",
      "step 50: train loss 1.1336, val loss 1.1391\n",
      "step 100: train loss 0.8750, val loss 0.8705\n",
      "step 150: train loss 0.7000, val loss 0.6967\n",
      "step 200: train loss 0.5872, val loss 0.5837\n",
      "step 250: train loss 0.5056, val loss 0.5015\n",
      "step 300: train loss 0.4382, val loss 0.4353\n",
      "step 350: train loss 0.3866, val loss 0.3832\n",
      "step 400: train loss 0.3427, val loss 0.3398\n",
      "step 450: train loss 0.3027, val loss 0.3010\n",
      "step 500: train loss 0.2734, val loss 0.2723\n",
      "step 550: train loss 0.2454, val loss 0.2439\n",
      "step 600: train loss 0.2232, val loss 0.2229\n",
      "step 650: train loss 0.2052, val loss 0.2050\n",
      "step 700: train loss 0.1889, val loss 0.1891\n",
      "step 750: train loss 0.1764, val loss 0.1766\n",
      "step 800: train loss 0.1647, val loss 0.1646\n",
      "step 850: train loss 0.1544, val loss 0.1547\n",
      "step 900: train loss 0.1441, val loss 0.1443\n",
      "step 950: train loss 0.1357, val loss 0.1361\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb, None, ucan_transformer.generate_square_subsequent_mask(n + 2, device=DEVICE))\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # function to generate output sequence using greedy algorithm\n",
    "# def greedy_decode(model, src, src_mask, max_len):\n",
    "#     \"\"\"First attempt at non-autoregressive decoding.\"\"\"\n",
    "    \n",
    "#     ys = torch.cat((src[:9], torch.tensor([EOS_TOKEN]).to(DEVICE)), dim=0).unsqueeze(0)\n",
    "#     src = src.to(DEVICE)\n",
    "#     src_mask = None  \n",
    "    \n",
    "#     # For a single example evaluation, we need to add a dummy batch dimension (1, *) with unsqueeze(0)\n",
    "#     memory = model.encode(src.unsqueeze(0), src_mask)    \n",
    "#     memory = memory.to(DEVICE)\n",
    "\n",
    "#     out = model.decode(tgt=ys, memory=memory, tgt_mask=None) # (1, tgt_seq_len, emb_dim)\n",
    "#     prob = model.generator(out) # (1, tgt_seq_len, num_tokens)\n",
    "#     _, pred = torch.max(prob, dim=2)\n",
    "\n",
    "#     return pred.squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1236,  1.2823,  0.6321,  1.4439,  0.9602,  0.9766, -0.4973,\n",
      "          -0.9076, -0.5309, -0.2988, -1.6624, -1.3608],\n",
      "         [-0.0534,  0.7805, -0.3174,  0.3710,  1.1181,  1.4887,  0.4814,\n",
      "          -0.9515, -0.0999, -2.0325, -1.4425,  0.6218],\n",
      "         [-0.0534,  0.7805, -0.3174,  0.3710,  1.1181,  1.4887,  0.4814,\n",
      "          -0.9515, -0.0999, -2.0325, -1.4425,  0.6218],\n",
      "         [-0.0534,  0.7805, -0.3174,  0.3710,  1.1181,  1.4887,  0.4814,\n",
      "          -0.9515, -0.0999, -2.0325, -1.4425,  0.6218],\n",
      "         [-0.0534,  0.7805, -0.3174,  0.3710,  1.1181,  1.4887,  0.4814,\n",
      "          -0.9515, -0.0999, -2.0325, -1.4425,  0.6218],\n",
      "         [-0.6038,  0.5767,  0.8809, -0.9446, -0.2169,  1.5485, -0.2948,\n",
      "          -1.3890,  1.9180, -0.8868, -0.8581,  0.2718],\n",
      "         [-0.6038,  0.5767,  0.8809, -0.9446, -0.2169,  1.5485, -0.2948,\n",
      "          -1.3890,  1.9180, -0.8868, -0.8581,  0.2718],\n",
      "         [-0.0534,  0.7805, -0.3174,  0.3710,  1.1181,  1.4887,  0.4814,\n",
      "          -0.9515, -0.0999, -2.0325, -1.4425,  0.6218],\n",
      "         [-0.0534,  0.7805, -0.3174,  0.3710,  1.1181,  1.4887,  0.4814,\n",
      "          -0.9515, -0.0999, -2.0325, -1.4425,  0.6218],\n",
      "         [-0.6038,  0.5767,  0.8809, -0.9446, -0.2169,  1.5485, -0.2948,\n",
      "          -1.3890,  1.9180, -0.8868, -0.8581,  0.2718],\n",
      "         [-0.6038,  0.5767,  0.8809, -0.9446, -0.2169,  1.5485, -0.2948,\n",
      "          -1.3890,  1.9180, -0.8868, -0.8581,  0.2718],\n",
      "         [-0.0534,  0.7805, -0.3174,  0.3710,  1.1181,  1.4887,  0.4814,\n",
      "          -0.9515, -0.0999, -2.0325, -1.4425,  0.6218],\n",
      "         [-0.0534,  0.7805, -0.3174,  0.3710,  1.1181,  1.4887,  0.4814,\n",
      "          -0.9515, -0.0999, -2.0325, -1.4425,  0.6218],\n",
      "         [-0.0534,  0.7805, -0.3174,  0.3710,  1.1181,  1.4887,  0.4814,\n",
      "          -0.9515, -0.0999, -2.0325, -1.4425,  0.6218],\n",
      "         [-0.0534,  0.7805, -0.3174,  0.3710,  1.1181,  1.4887,  0.4814,\n",
      "          -0.9515, -0.0999, -2.0325, -1.4425,  0.6218],\n",
      "         [-0.6038,  0.5767,  0.8809, -0.9446, -0.2169,  1.5485, -0.2948,\n",
      "          -1.3890,  1.9180, -0.8868, -0.8581,  0.2718],\n",
      "         [-0.0534,  0.7805, -0.3174,  0.3710,  1.1181,  1.4887,  0.4814,\n",
      "          -0.9515, -0.0999, -2.0325, -1.4425,  0.6218],\n",
      "         [-0.7304,  1.3500, -0.2196, -1.0922,  0.1100,  1.5598,  0.0754,\n",
      "          -1.1152,  0.0265, -1.5744,  0.0729,  1.5642]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[1]], device='cuda:0') ys\n",
      "tensor([[ 0.1032,  2.6160, -0.9327,  0.4254]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[1, 1]], device='cuda:0') ys\n",
      "tensor([[ 0.1032,  2.6160, -0.9327,  0.4254]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[1, 1, 1]], device='cuda:0') ys\n",
      "tensor([[ 0.1032,  2.6160, -0.9327,  0.4254]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[1, 1, 1, 1]], device='cuda:0') ys\n",
      "tensor([[ 0.1032,  2.6160, -0.9327,  0.4254]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[1, 1, 1, 1, 1]], device='cuda:0') ys\n",
      "tensor([[ 0.1032,  2.6160, -0.9327,  0.4254]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[1, 1, 1, 1, 1, 1]], device='cuda:0') ys\n",
      "tensor([[ 0.1032,  2.6160, -0.9327,  0.4254]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1]], device='cuda:0') ys\n",
      "tensor([[ 0.1032,  2.6160, -0.9327,  0.4254]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0') ys\n",
      "tensor([[ 0.1032,  2.6160, -0.9327,  0.4254]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030],\n",
      "         [ 0.4766, -0.2339,  1.8777, -0.6075, -1.3737,  0.0771,  2.0332,\n",
      "          -0.7443, -1.4473,  1.0762, -0.0564, -1.0030]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0') ys\n",
      "tensor([[ 0.1032,  2.6160, -0.9327,  0.4254]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xv, yv = get_batch('val')\n",
    "src = xv[0]\n",
    "truth = yv[0]\n",
    "\n",
    "src = src.to(DEVICE)\n",
    "src_mask = None\n",
    "max_len = 10\n",
    "\n",
    "# For a single example evaluation, we need to add a dummy batch dimension (1, *) with unsqueeze(0)\n",
    "memory = model.encode(src.unsqueeze(0), src_mask)\n",
    "# The [1, 1] shape starts us off with a dummy batch dimension \n",
    "ys = torch.ones(1, 1).fill_(1).type(torch.long).to(DEVICE)\n",
    "\n",
    "# FIXME: encode acts identically on every token in every position.\n",
    "print(memory)\n",
    "\n",
    "for i in range(max_len - 1): # -1 since we start with SOS\n",
    "    memory = memory.to(DEVICE)\n",
    "    tgt_mask = (ucan_transformer.generate_square_subsequent_mask(ys.size(1), device=DEVICE).type(torch.bool)).to(DEVICE)\n",
    "    out = model.decode(tgt=ys, memory=memory, tgt_mask=tgt_mask) # (1, tgt_seq_len, emb_dim)\n",
    "\n",
    "    # FIXME: decode acts identically on every token in every position.\n",
    "    prob = model.generator(out[:, -1])\n",
    "    print(out, \"out\")\n",
    "    print(ys, \"ys\")\n",
    "    print(prob, \"prob\")\n",
    "    _, next_word = torch.max(prob, dim=1)\n",
    "    next_word = next_word.item()\n",
    "    ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xv_i tensor([2, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 3], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "tensor([[[ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2]], device='cuda:0') ys\n",
      "tensor([[-1.0226, -0.8345,  1.9569, -0.7579]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2]], device='cuda:0') ys\n",
      "tensor([[-1.0226, -0.8345,  1.9569, -0.7579]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2, 2]], device='cuda:0') ys\n",
      "tensor([[-1.0226, -0.8345,  1.9569, -0.7579]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2, 2, 2]], device='cuda:0') ys\n",
      "tensor([[-1.0226, -0.8345,  1.9569, -0.7579]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2, 2, 2, 2]], device='cuda:0') ys\n",
      "tensor([[-1.0226, -0.8345,  1.9569, -0.7579]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2, 2, 2, 2, 2]], device='cuda:0') ys\n",
      "tensor([[-1.0226, -0.8345,  1.9569, -0.7579]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2]], device='cuda:0') ys\n",
      "tensor([[-1.0226, -0.8345,  1.9569, -0.7579]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2, 2]], device='cuda:0') ys\n",
      "tensor([[-1.0226, -0.8345,  1.9569, -0.7579]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2]], device='cuda:0') ys\n",
      "tensor([[-1.0226, -0.8345,  1.9569, -0.7579]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "yv_i tensor([2, 1, 0, 1, 1, 0, 1, 0, 1, 3], device='cuda:0')\n",
      "prediction tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
      "\n",
      "xv_i tensor([2, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 3], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "tensor([[[ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2]], device='cuda:0') ys\n",
      "tensor([[-1.0226, -0.8345,  1.9569, -0.7579]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2]], device='cuda:0') ys\n",
      "tensor([[-1.0226, -0.8345,  1.9569, -0.7579]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2, 2]], device='cuda:0') ys\n",
      "tensor([[-1.0226, -0.8345,  1.9569, -0.7579]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2, 2, 2]], device='cuda:0') ys\n",
      "tensor([[-1.0226, -0.8345,  1.9569, -0.7579]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2, 2, 2, 2]], device='cuda:0') ys\n",
      "tensor([[-1.0226, -0.8345,  1.9569, -0.7579]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2, 2, 2, 2, 2]], device='cuda:0') ys\n",
      "tensor([[-1.0226, -0.8345,  1.9569, -0.7579]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2]], device='cuda:0') ys\n",
      "tensor([[-1.0226, -0.8345,  1.9569, -0.7579]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2, 2]], device='cuda:0') ys\n",
      "tensor([[-1.0226, -0.8345,  1.9569, -0.7579]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168],\n",
      "         [ 0.7594, -0.8275, -1.9494,  0.1014,  0.9055, -0.2788, -1.9472,\n",
      "           0.5500,  1.7124,  0.2659,  0.7651, -0.0168]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2]], device='cuda:0') ys\n",
      "tensor([[-1.0226, -0.8345,  1.9569, -0.7579]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "yv_i tensor([2, 1, 1, 0, 0, 1, 1, 1, 0, 3], device='cuda:0')\n",
      "prediction tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
      "\n",
      "xv_i tensor([2, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 3], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "tensor([[[ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2]], device='cuda:0') ys\n",
      "tensor([[-0.8864, -0.9258,  1.9152, -0.9349]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2]], device='cuda:0') ys\n",
      "tensor([[-0.8864, -0.9258,  1.9152, -0.9349]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2, 2]], device='cuda:0') ys\n",
      "tensor([[-0.8864, -0.9258,  1.9152, -0.9349]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2, 2, 2]], device='cuda:0') ys\n",
      "tensor([[-0.8864, -0.9258,  1.9152, -0.9349]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2, 2, 2, 2]], device='cuda:0') ys\n",
      "tensor([[-0.8864, -0.9258,  1.9152, -0.9349]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2, 2, 2, 2, 2]], device='cuda:0') ys\n",
      "tensor([[-0.8864, -0.9258,  1.9152, -0.9349]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2]], device='cuda:0') ys\n",
      "tensor([[-0.8864, -0.9258,  1.9152, -0.9349]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2, 2]], device='cuda:0') ys\n",
      "tensor([[-0.8864, -0.9258,  1.9152, -0.9349]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2]], device='cuda:0') ys\n",
      "tensor([[-0.8864, -0.9258,  1.9152, -0.9349]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "yv_i tensor([2, 0, 0, 0, 1, 1, 1, 0, 0, 3], device='cuda:0')\n",
      "prediction tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
      "\n",
      "xv_i tensor([2, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 3], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "tensor([[[ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2]], device='cuda:0') ys\n",
      "tensor([[-0.8864, -0.9258,  1.9152, -0.9349]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2]], device='cuda:0') ys\n",
      "tensor([[-0.8864, -0.9258,  1.9152, -0.9349]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2, 2]], device='cuda:0') ys\n",
      "tensor([[-0.8864, -0.9258,  1.9152, -0.9349]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2, 2, 2]], device='cuda:0') ys\n",
      "tensor([[-0.8864, -0.9258,  1.9152, -0.9349]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2, 2, 2, 2]], device='cuda:0') ys\n",
      "tensor([[-0.8864, -0.9258,  1.9152, -0.9349]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2, 2, 2, 2, 2]], device='cuda:0') ys\n",
      "tensor([[-0.8864, -0.9258,  1.9152, -0.9349]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2]], device='cuda:0') ys\n",
      "tensor([[-0.8864, -0.9258,  1.9152, -0.9349]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2, 2]], device='cuda:0') ys\n",
      "tensor([[-0.8864, -0.9258,  1.9152, -0.9349]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "tensor([[[ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853],\n",
      "         [ 0.8141, -0.9603, -1.9615,  0.1558,  0.7309, -0.0263, -1.8588,\n",
      "           0.6107,  1.9107,  0.0991,  0.4507,  0.0853]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>) out\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2]], device='cuda:0') ys\n",
      "tensor([[-0.8864, -0.9258,  1.9152, -0.9349]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) prob\n",
      "\n",
      "yv_i tensor([2, 0, 0, 0, 0, 1, 1, 1, 0, 3], device='cuda:0')\n",
      "prediction tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# function to generate output sequence using greedy algorithm\n",
    "def greedy_decode(model, src, src_mask, max_len):\n",
    "    \"\"\"Standard autoregressive decoder output scheme.\n",
    "\n",
    "    Current issue: my train batching didn't have time-sliced data, so I think the \n",
    "    model has no idea what to do with a length-1 <SOS> sequence as input.\n",
    "    \"\"\"\n",
    "    src = src.to(DEVICE)\n",
    "    # src_mask = src_mask.to(DEVICE)\n",
    "    src_mask = None  # FIXME\n",
    "    \n",
    "    # For a single example evaluation, we need to add a dummy batch dimension (1, *) with unsqueeze(0)\n",
    "    memory = model.encode(src.unsqueeze(0), src_mask)\n",
    "    # The [1, 1] shape starts us off with a dummy batch dimension \n",
    "    ys = torch.ones(1, 1).fill_(SOS_TOKEN).type(torch.long).to(DEVICE)\n",
    "\n",
    "    # FIXME: should I enforce the length? Or should I enforce the length+1, \n",
    "    # and then checksum for an EOS? Or should I allow variable length :(\n",
    "    for i in range(max_len - 1): # -1 since we start with SOS\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (ucan_transformer.generate_square_subsequent_mask(ys.size(1), device=DEVICE).type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(tgt=ys, memory=memory, tgt_mask=tgt_mask) # (1, tgt_seq_len, emb_dim)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        print(out, \"out\")\n",
    "        print(ys, \"ys\")\n",
    "        print(prob, \"prob\")\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "        print()\n",
    "    return ys\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "# inp\n",
    "def translate(model: torch.nn.Module, src):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        src: tensor. single input bitstring of length 2n + 2. Shape (2n + 2,) \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    seq_len = src.shape[0] - 2\n",
    "    out_len = seq_len // 2 + 2 # 2:1 UCAN conversion, plus EOS/SOS\n",
    "    # src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    src_mask = None\n",
    "    tgt_tokens = greedy_decode(model, src, src_mask, max_len=out_len).flatten()\n",
    "\n",
    "    return tgt_tokens\n",
    "\n",
    "\n",
    "xv, yv = get_batch('val')\n",
    "\n",
    "for xvi, yvi in zip(xv, yv):\n",
    "    print(\"xv_i\", xvi)\n",
    "    pred = translate(model, xvi)\n",
    "    print(\"yv_i\", yvi)\n",
    "    print(\"prediction\", pred)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "ys = torch.ones(1, 1).fill_(SOS_TOKEN).type(torch.long).to(DEVICE)\n",
    "print(ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try to use DataLoader, Dataset, etc. for batching\n",
    "\n",
    "def train_step(model, optimizer):\n",
    "    losses = 0\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        # These aren't technically epochs i guess.\n",
    "        xb, yb = get_batch('train')\n",
    "\n",
    "        logits = model(xb, yb, None, None) # No masks for now\n",
    "        optimizer.zero_grad(set_to_none=True) #?\n",
    "\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), yb.reshape(-1))\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        losses += loss.item() / BATCH_SIZE # check this\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16, 10]) embedding\n",
      "torch.Size([4, 16, 10]) positional\n",
      "torch.Size([4, 8, 2]) model forward\n"
     ]
    }
   ],
   "source": [
    "emb_size = 10\n",
    "x, y = get_batch('train')\n",
    "embedding = TokenEmbedding(2, emb_size)\n",
    "xx = embedding.forward(x)\n",
    "print(xx.shape, \"embedding\")\n",
    "positional = PositionalEncoding(emb_size, 0.1)\n",
    "xxx = positional.forward(xx)\n",
    "print(xxx.shape, \"positional\")\n",
    "\n",
    "xxxx = model.forward(x, y, None, None)\n",
    "print(xxxx.shape, \"model forward\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "Re-adapt this for prediction/evaluation, and probably incorporate as a function into the transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Scratchwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 18\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "\n",
    "# function to generate output sequence using greedy algorithm\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "# inp\n",
    "def translate(model: torch.nn.Module, src):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        single\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
    "\n",
    "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "SRC_LANGUAGE = 'fuck'\n",
    "TGT_LANGUAGE = 'this'\n",
    "\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "    \n",
    "\n",
    "val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "# function to collate data samples into batch tensors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multi30k, Multi30k\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Fixme\u001b[39;00m\n\u001b[0;32m      5\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "\n",
    "# Fixme\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(train_dataloader))\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
