{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    return np.exp(X) / np.sum(np.exp(X), axis=0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "XOR_INPUTS = [np.array([[1, 0], [1, 0]]), np.array([[0, 1], [1, 0]]), np.array([[1, 0], [0, 1]]), np.array([[0, 1], [0, 1]]), ]\n",
    "INPUTS = [(0, 0), (1, 0), (0, 1), (1, 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of encoder is\n",
    "$$\n",
    "H = \\sigma(X W_Q W_K^T X^T) X W_V\n",
    "$$\n",
    "\n",
    "Output of decoder times linear layer is\n",
    "$$\n",
    "D = \\sigma(q^T H) H W\n",
    "$$\n",
    "Where $W = W_V^{dec} W'$ absorbs the decoder values matrix and $q^T = v_{SOS} W_Q^{dec} (W_K^{dec})^T$ absorbs several weight matrices. \n",
    "\n",
    "Without the FFNN at the end of the encoder, I'm actually, very suspicious that this might just be a mostly-linear function, i.e. the input appears right here:\n",
    "$$\n",
    "D = \\sigma(q^T \\sigma(X W_Q W_K^T X^T) X W_V) \\sigma(X W_Q W_K^T X^T) \\textcolor{red}{X} W_V W \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute Q, K, V for encoder self-attention\n",
    "def get_QKV(X, Wq, Wk, Wv):\n",
    "\n",
    "    Q = X @ Wq\n",
    "    K = X @ Wk\n",
    "    V = X @ Wv\n",
    "    return Q, K, V\n",
    "\n",
    "# Compute output of encoder\n",
    "def encoder(X, Wq, Wk, Wv):\n",
    "    Q, K, V = get_QKV(X, Wq, Wk, Wv)\n",
    "    Z = softmax(Q @ K.T) @ V\n",
    "    return Z\n",
    "\n",
    "def rowwise_FFNN(X, W1, W2):\n",
    "    \"\"\"With ReLU activation function.\"\"\"\n",
    "    return np.maximum(0, X @ W1) @ W2\n",
    "\n",
    "# Compute output of decoder with cross attention\n",
    "def sigma_qkt(memory, v):   \n",
    "    return softmax(v @ memory)\n",
    "\n",
    "def decoder_linear(memory, v, W):\n",
    "    return sigma_qkt(memory, v) @ memory @ W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pair (0, 0) S:\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "pair (1, 0) S:\n",
      "[[0.88079708 0.11920292]\n",
      " [0.11920292 0.88079708]]\n",
      "pair (0, 1) S:\n",
      "[[0.88079708 0.11920292]\n",
      " [0.11920292 0.88079708]]\n",
      "pair (1, 1) S:\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "Wq = np.array([[1, 1], [1, -1]])\n",
    "Wk = np.array([[1, 1], [1, -1]])\n",
    "for pair, X in zip(INPUTS, XOR_INPUTS):\n",
    "    \n",
    "    S = softmax(X @ Wq @ Wk.T @ X.T)\n",
    "    print(f\"pair {pair} S:\")\n",
    "    print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) encoder\n",
      "[[1.  0.5]\n",
      " [1.  0.5]]\n",
      "[[1.  0.5]\n",
      " [1.  0.5]]\n",
      "sigma_qT_H [0.66702773 0.33297227]\n",
      "     decoder_prelinear [1.  0.5]\n",
      "output: (0, 0) -> [0.62245933 0.37754067]\n",
      "\n",
      "(1, 0) encoder\n",
      "[[ 0.55960146 -0.82119562]\n",
      " [ 0.94039854  0.32119562]]\n",
      "[[0.55960146 0.        ]\n",
      " [0.94039854 0.32119562]]\n",
      "sigma_qT_H [0.6966924 0.3033076]\n",
      "     decoder_prelinear [0.67510011 0.09742107]\n",
      "output: (1, 0) -> [0.64053318 0.35946682]\n",
      "\n",
      "(0, 1) encoder\n",
      "[[ 0.94039854  0.32119562]\n",
      " [ 0.55960146 -0.82119562]]\n",
      "[[0.94039854 0.32119562]\n",
      " [0.55960146 0.        ]]\n",
      "sigma_qT_H [0.69134289 0.30865711]\n",
      "     decoder_prelinear [0.82286281 0.22205631]\n",
      "output: (0, 1) -> [0.6458408 0.3541592]\n",
      "\n",
      "(1, 1) encoder\n",
      "[[ 0.5 -1. ]\n",
      " [ 0.5 -1. ]]\n",
      "[[0.5 0. ]\n",
      " [0.5 0. ]]\n",
      "sigma_qT_H [0.66702773 0.33297227]\n",
      "     decoder_prelinear [0.5 0. ]\n",
      "output: (1, 1) -> [0.62245933 0.37754067]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Specify query vector for the SOS token\n",
    "# This abosrbs the Wq and Wk matrices for decoder\n",
    "\n",
    "\n",
    "# Specify encoder Wq, Wk, Wv for self-attention\n",
    "Wq = np.array([[1, 1], [1, -1]])\n",
    "Wk = np.array([[1, 1], [1, -1]])\n",
    "Wv = np.array(\n",
    "    [[1, 1/2],\n",
    "     [1/2, -1]]\n",
    "\n",
    ")\n",
    "# Wq = np.random.rand(2, 2)\n",
    "# Wk = np.random.rand(2, 2)\n",
    "# Wv = np.random.rand(2, 2)\n",
    "\n",
    "W1 = np.array([[1, 0], [0, 1]])\n",
    "W2 = np.array([[1, 0], [0, 1]])\n",
    "\n",
    "# DECODER\n",
    "# vqk_vec = np.array([-1, .2])\n",
    "vqk_vec = np.random.rand(2)\n",
    "W = np.array(\n",
    "    [[1, 0],\n",
    "     [0, 1]]\n",
    ")\n",
    "\n",
    "for pair, X in zip(INPUTS, XOR_INPUTS):\n",
    "    print(f\"{pair} encoder\")\n",
    "    enc_out = encoder(X, Wq, Wk, Wv)\n",
    "    print(enc_out)\n",
    "    memory = rowwise_FFNN(enc_out, W1, W2)\n",
    "    print(memory)\n",
    "\n",
    "    sigma_q_Kt = sigma_qkt(memory, vqk_vec)\n",
    "    print(\"sigma_qT_H\", sigma_q_Kt)\n",
    "\n",
    "    decoder_prelinear = sigma_qkt(memory, vqk_vec) @ memory\n",
    "    print(\"     decoder_prelinear\", decoder_prelinear)\n",
    "\n",
    "    decoder = decoder_linear(memory, vqk_vec, W)\n",
    "    # print(\"decoder_linear\", decoder)  \n",
    "    print(f\"output: {pair} ->\", softmax(decoder))\n",
    "    print()\n",
    "\n",
    "# print(Wq, Wk, Wv, vqk_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 2]\n",
      "[5 2]\n",
      "[5 2]\n",
      "[6 2]\n"
     ]
    }
   ],
   "source": [
    "v = np.array([1, 1])\n",
    "Sigma = np.array([[1, 0], [0, 1]])\n",
    "W = np.array([\n",
    "    [2, 1], \n",
    "    [3, 1]])\n",
    "\n",
    "for pair, X in zip(INPUTS, XOR_INPUTS):\n",
    "    print(v.T @ Sigma @ X @ W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
