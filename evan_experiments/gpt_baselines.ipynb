{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.10.8)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/peter/Desktop/projects/foreqast.ai/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# reload magic\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "with open('OPENAI_API_KEY', 'r') as f:\n",
    "    OPENAI_API_KEY = f.read().strip()\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import deterministic.py using local file path\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('../sequence_generators')\n",
    "import deterministic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_sequence_sets(length, n_train, n_data, p_bitflip=0.0):\n",
    "    sequence_sets = []\n",
    "    for i in range(n_data):\n",
    "        gen = deterministic.SequenceGen(lookback=4, seed=228+i, number_of_generating_methods=1)\n",
    "        data, generating_func = gen.deterministically_generate_sequences(length=length, num_seq=1, save=False)\n",
    "        train_data = data[0][:n_train]\n",
    "        if p_bitflip > 0:\n",
    "            mask = np.random.choice([0, 1], size=(len(train_data),), p=[1-p_bitflip, p_bitflip]).astype(np.uint8)\n",
    "            train_data = np.array(train_data, dtype=np.uint) ^ mask\n",
    "        training_string = \" \".join(train_data.astype(str))\n",
    "        test_string = \" \".join(data[0][n_train:])\n",
    "        sequence_sets.append((training_string, test_string, generating_func[0]))\n",
    "\n",
    "    return sequence_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_completions_inline(model, input_str, steps, num_samples, noisy=False, logprobs=True, top_logprobs=5, temp=None, logit_bias=None, **kwargs):\n",
    "    ''' Sample completions from GPT-3\n",
    "    Args:\n",
    "        input_str: input sequence as a string\n",
    "        steps: number of steps to predict\n",
    "        num_samples: number of samples to return\n",
    "        temp: temperature for sampling\n",
    "        prompt: additional prompt before the input string\n",
    "        model: name of GPT-3 model to use\n",
    "    Returns:\n",
    "        list of completion strings\n",
    "    \n",
    "    Ripped from: https://github.com/ngruver/llmtime/blob/main/models/promptcast.py\n",
    "    '''\n",
    "    trick_token_count = 1000 # this is a trick to get the model to attempt to predict a large number of tokens, which we truncate with the api\n",
    "    chatgpt_sys_message = f\"\"\"You are a helpful assistant that predicts the next bit. The user will provide a sequence containing ONLY 0 or 1, \n",
    "                             and you will predict the next {trick_token_count} digits that come next. The sequence is represented by only digits 0 or 1 separated by spaces, NO COMMAS.\n",
    "                             The data may be noisy, in which case you should predict the most likely sequence.\"\"\"\n",
    "    \n",
    "    extra_input = \"\"\"Please continue the following sequence with only digits 0 or 1 separated by only spaces. Do not produce any additional text. Do not include commas. \n",
    "                     Do not say anything like 'the next terms in the sequence are', just return the numbers. \"\"\"\n",
    "    noisy_prompt = \"\"\"This data has been generated with some bitflip noise. Predict the most likely sequence WITHOUT NOISE. \"\"\"\n",
    "    if noisy:\n",
    "        extra_input = extra_input + noisy_prompt\n",
    "    extra_input = extra_input + \"Sequence:\\n\"\n",
    "\n",
    "    if model in ['gpt-3.5-turbo','gpt-4']:\n",
    "        chatgpt_sys_message = chatgpt_sys_message\n",
    "        extra_input = extra_input\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                    {\"role\": \"system\", \"content\": chatgpt_sys_message},\n",
    "                    {\"role\": \"user\", \"content\": extra_input+input_str}\n",
    "                ],\n",
    "            max_tokens=int(steps), \n",
    "            temperature=temp,\n",
    "            logit_bias=logit_bias,\n",
    "            n=num_samples,\n",
    "            logprobs=logprobs,\n",
    "            top_logprobs=top_logprobs,\n",
    "            **kwargs\n",
    "        )\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_shifted_acc(preds, truth, bounds=(-4, 4)):\n",
    "    \"\"\"Since GPT starts in the wrong place sometimes, we will be generous and look for the best alignment between its predicted string and the test string.\n",
    "    if we have to move by m places, we will discard m of the string during evaluation.\n",
    "    \"\"\"\n",
    "    best_acc = 0\n",
    "    best_shift = 0\n",
    "    preds = np.array(preds)\n",
    "    truth = np.array(truth)\n",
    "    assert len(preds) == len(truth)\n",
    "    for shift in range(bounds[0], bounds[1]):\n",
    "        # minus sign means we shift \n",
    "        if shift < 0:\n",
    "            x = preds[abs(shift):] # shifted preds\n",
    "            y = truth[0:len(x)] # truncated truth\n",
    "        elif shift > 0:\n",
    "            x = truth[shift:] # shifted truth\n",
    "            y = preds[0:len(x)] # truncated preds\n",
    "        else:\n",
    "            x = preds\n",
    "            y = truth\n",
    "\n",
    "        acc = 1 - np.mean(abs(x - y))\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_shift = shift\n",
    "    return best_acc, best_shift\n",
    "\n",
    "\n",
    "def last_bit_acc(preds, truth):\n",
    "    \"\"\"Compute the accuracy of the last bit of the prediction.\"\"\"\n",
    "    return 1 - abs(preds[-1] - truth[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9285714285714286, -2)\n"
     ]
    }
   ],
   "source": [
    "preds = [0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0]\n",
    "truth = [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1]\n",
    "print(best_shifted_acc(preds, truth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1\n",
    "\n",
    "Each digit gets its own token (with a space getting another token between each). We will use deterministic data only, and ask GPT to learn to extend a single sequence that we generate.\n",
    "\n",
    " - no noise\n",
    " - single input, space-separated bits\n",
    " - 20 trials, for 20 different generating functions\n",
    " \n",
    " **Succeeded** - the model had no problem doing a 3-lookback sequence extension. One issue is that these data are very repetitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a single-generating-method dataset\n",
    "\n",
    "# For the first attempt, we will have one really long sequence, \n",
    "# and gpt will try to predict the final bit, then we will average\n",
    "# its performance over some large number of evaluations\n",
    "# TODO: hyperparameter tuning with temperature, other gpt-3 parameters from [1]\n",
    "\n",
    "N_BITS=100\n",
    "n_train = 80\n",
    "n_test = N_BITS - n_train\n",
    "n_data=20\n",
    "\n",
    "\n",
    "sequence_sets = make_sequence_sets(N_BITS, n_train, n_data, p_bitflip=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs = 4\n",
    "model = 'gpt-3.5-turbo'\n",
    "steps = n_test*2 - 1\n",
    "\n",
    "completions = []\n",
    "for training_string, test_string, generating_func in sequence_sets:\n",
    "    completion = sample_completions_inline(model, training_string, steps, 1, logprobs=True, top_logprobs=5, temp=1, logit_bias=None)\n",
    "    completions.append( completion )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0, Shift: -1\n",
      "Predicted: [0 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0]\n",
      "Truth:     [1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1]\n",
      "\n",
      "Accuracy: 1.0, Shift: -1\n",
      "Predicted: [1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0]\n",
      "Truth:     [1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1]\n",
      "\n",
      "Accuracy: 1.0, Shift: 0\n",
      "Predicted: [1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1]\n",
      "Truth:     [1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1]\n",
      "\n",
      "Accuracy: 0.95, Shift: 0\n",
      "Predicted: [1 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1]\n",
      "Truth:     [0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1]\n",
      "\n",
      "Accuracy: 1.0, Shift: -4\n",
      "Predicted: [0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Truth:     [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "Accuracy: 1.0, Shift: -4\n",
      "Predicted: [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Truth:     [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "Accuracy: 1.0, Shift: -1\n",
      "Predicted: [1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0]\n",
      "Truth:     [0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1]\n",
      "\n",
      "Accuracy: 1.0, Shift: -2\n",
      "Predicted: [1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0]\n",
      "Truth:     [0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0]\n",
      "\n",
      "Accuracy: 1.0, Shift: 0\n",
      "Predicted: [1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0]\n",
      "Truth:     [1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0]\n",
      "\n",
      "Accuracy: 1.0, Shift: -1\n",
      "Predicted: [0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1]\n",
      "Truth:     [0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0]\n",
      "\n",
      "Accuracy: 1.0, Shift: 0\n",
      "Predicted: [1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0]\n",
      "Truth:     [1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0]\n",
      "\n",
      "Accuracy: 1.0, Shift: -1\n",
      "Predicted: [0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1]\n",
      "Truth:     [0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0]\n",
      "\n",
      "Accuracy: 1.0, Shift: -3\n",
      "Predicted: [1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0]\n",
      "Truth:     [0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0]\n",
      "\n",
      "Accuracy: 1.0, Shift: -4\n",
      "Predicted: [1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Truth:     [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "Accuracy: 1.0, Shift: -3\n",
      "Predicted: [1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0]\n",
      "Truth:     [0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0]\n",
      "\n",
      "Accuracy: 1.0, Shift: -4\n",
      "Predicted: [1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Truth:     [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "Accuracy: 1.0, Shift: 0\n",
      "Predicted: [0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1]\n",
      "Truth:     [0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1]\n",
      "\n",
      "Accuracy: 1.0, Shift: -1\n",
      "Predicted: [0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1]\n",
      "Truth:     [0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0]\n",
      "\n",
      "Accuracy: 1.0, Shift: 0\n",
      "Predicted: [1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1]\n",
      "Truth:     [1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1]\n",
      "\n",
      "Accuracy: 1.0, Shift: -3\n",
      "Predicted: [0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0]\n",
      "Truth:     [1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_data):\n",
    "    completion = completions[i]\n",
    "    preds = np.array([int(x) for x in completion.choices[0].message.content.split()])\n",
    "    truth = np.array([int(x) for x in sequence_sets[i][1].split()])\n",
    "    acc, shift = best_shifted_acc(preds, truth)\n",
    "    print(f\"Accuracy: {acc}, Shift: {shift}\")\n",
    "    print(f\"Predicted: {preds}\")\n",
    "    print(f\"Truth:     {truth}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Add bitflip noise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bits = 500 # need more because of bitflip chance...\n",
    "n_train = 400\n",
    "n_test = n_bits - n_train\n",
    "n_data = 20\n",
    "p_bitflip = 0.05\n",
    "noisy_sequence_sets = make_sequence_sets(n_bits, n_train, n_data, p_bitflip=p_bitflip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs = 4\n",
    "model = 'gpt-3.5-turbo'\n",
    "steps = n_test*2 - 1\n",
    "\n",
    "noisy_completions = []\n",
    "for training_string, test_string, generating_func in noisy_sequence_sets:\n",
    "    completion = sample_completions_inline(model, training_string, steps, 1, logprobs=True, top_logprobs=5, temp=1, logit_bias=None, noisy=True)\n",
    "    noisy_completions.append( completion )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7604166666666666, Shift: -4\n",
      "Predicted: 1000111001110011100111001110011100111001110011111110011100111001100011000110001110111110111001110011\n",
      "Truth:     1100111001110011100111001110011100111001110011100111001110011100111001110011100111001110011100111001\n",
      "\n",
      "Accuracy: 1.0, Shift: -1\n",
      "Predicted: 1100011000110001100011000110001100011000110001100011000110001100011000110001100011000110001100011000\n",
      "Truth:     1000110001100011000110001100011000110001100011000110001100011000110001100011000110001100011000110001\n",
      "\n",
      "Accuracy: 0.9299999999999999, Shift: 0\n",
      "Predicted: 1110011101111011010111101111011111011001111011110111101111011110111101111011110110101111111110111101\n",
      "Truth:     1110111101111011110111101111011110111101111011110111101111011110111101111011110111101111011110111101\n",
      "\n",
      "Accuracy: 0.9696969696969697, Shift: -1\n",
      "Predicted: 0101001010100101001010010100101001010010100101001010010100101001010010100101001010010100101001010010\n",
      "Truth:     0010100101001010010100101001010010100101001010010100101001010010100101001010010100101001010010100101\n",
      "\n",
      "Accuracy: 0.96875, Shift: -4\n",
      "Predicted: 1000111111011111111111111111111111111111111011111110111111111111111111111111111111111111111111111111\n",
      "Truth:     1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
      "\n",
      "Accuracy: 0.9583333333333334, Shift: -4\n",
      "Predicted: 0001000000000000000000100000010000010000000000000000000000100000000000000000000000000000000000000000\n",
      "Truth:     0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "\n",
      "Accuracy: 0.92, Shift: 0\n",
      "Predicted: 0100101001010010101101001010011100101000011010110101001010010100101001010010100101001010011100001101\n",
      "Truth:     0100101001010010100101001010010100101001010010100101001010010100101001010010100101001010010100101001\n",
      "\n",
      "Accuracy: 1.0, Shift: -2\n",
      "Predicted: 1000010000100001000010000100001000010000100001000010000100001000010000100001000010000100001000010000\n",
      "Truth:     0001000010000100001000010000100001000010000100001000010000100001000010000100001000010000100001000010\n",
      "\n",
      "Accuracy: 1.0, Shift: 0\n",
      "Predicted: 1110011100111001110011100111001110011100111001110011100111001110011100111001110011100111001110011100\n",
      "Truth:     1110011100111001110011100111001110011100111001110011100111001110011100111001110011100111001110011100\n",
      "\n",
      "Accuracy: 1.0, Shift: -2\n",
      "Predicted: 1000010000100001000010000100001000010000100001000010000100001000010000100001000010000100001000010000\n",
      "Truth:     0001000010000100001000010000100001000010000100001000010000100001000010000100001000010000100001000010\n",
      "\n",
      "Accuracy: 1.0, Shift: 0\n",
      "Predicted: 1111011110111101111011110111101111011110111101111011110111101111011110111101111011110111101111011110\n",
      "Truth:     1111011110111101111011110111101111011110111101111011110111101111011110111101111011110111101111011110\n",
      "\n",
      "Accuracy: 0.96, Shift: 0\n",
      "Predicted: 1011000010000100001000010000100001000010000100101000010000100101000010000100001000010000100001000010\n",
      "Truth:     0001000010000100001000010000100001000010000100001000010000100001000010000100001000010000100001000010\n",
      "\n",
      "Accuracy: 0.9795918367346939, Shift: 2\n",
      "Predicted: 1100011010110001100011000110001100011000110001100011000110001100011000110101100011000110001100011000\n",
      "Truth:     0011000110001100011000110001100011000110001100011000110001100011000110001100011000110001100011000110\n",
      "\n",
      "Accuracy: 0.9791666666666666, Shift: -4\n",
      "Predicted: 0011000000000000000000000000000000000000100000000000000000000000000000100000000000000000000000000000\n",
      "Truth:     0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "\n",
      "Accuracy: 1.0, Shift: -3\n",
      "Predicted: 1000010000100001000010000100001000010000100001000010000100001000010000100001000010000100001000010000\n",
      "Truth:     0010000100001000010000100001000010000100001000010000100001000010000100001000010000100001000010000100\n",
      "\n",
      "Accuracy: 0.8865979381443299, Shift: -3\n",
      "Predicted: 1101011111111111011111111111111111111111111110101111111111111011111111101011110101111111100111111111\n",
      "Truth:     1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
      "\n",
      "Accuracy: 0.9696969696969697, Shift: 1\n",
      "Predicted: 1011010110101001011010110101101011010110100101011010010101101011010110101101011010110101101011010110\n",
      "Truth:     0101101011010110101101011010110101101011010110101101011010110101101011010110101101011010110101101011\n",
      "\n",
      "Accuracy: 0.9489795918367347, Shift: -2\n",
      "Predicted: 1000010000101001000010000100001000010000100011000010000101001000010100100001000010000110001000010000\n",
      "Truth:     0001000010000100001000010000100001000010000100001000010000100001000010000100001000010000100001000010\n",
      "\n",
      "Accuracy: 0.92, Shift: 0\n",
      "Predicted: 1100110001110011100111001110111100111001110011100101101110011100110001110010100110001110011100011001\n",
      "Truth:     1100111001110011100111001110011100111001110011100111001110011100111001110011100111001110011100111001\n",
      "\n",
      "Accuracy: 0.97, Shift: 0\n",
      "Predicted: 1001010010100101001010110000101001010010100101001010010100101001010010100101001110010100101001010010\n",
      "Truth:     1001010010100101001010010100101001010010100101001010010100101001010010100101001010010100101001010010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_data):\n",
    "    completion = noisy_completions[i]\n",
    "    preds = np.array([int(x) for x in completion.choices[0].message.content.split()])\n",
    "    truth = np.array([int(x) for x in noisy_sequence_sets[i][1].split()])\n",
    "    acc, shift = best_shifted_acc(preds, truth)\n",
    "    print(f\"Accuracy: {acc}, Shift: {shift}\")\n",
    "    predstr = \"\".join([str(x) for x in preds])\n",
    "    truthstr = \"\".join([str(x) for x in truth])\n",
    "    print(f\"Predicted: {predstr}\")\n",
    "    print(f\"Truth:     {truthstr}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Multi-line inputs\n",
    "\n",
    "The previous inputs (and the cited ref.) relied on a single-line string to do the time-series forecasting. Here, I want to see if the xformer can take in a multi-line dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_completions_multiline(model, input_str, num_samples=1, noisy=False, logprobs=True, top_logprobs=5, temp=None, delimiter='\\n', logit_bias=None, **kwargs):\n",
    "    ''' Sample completions from GPT-3. This will sample a single bit, given a sequence of '#'-separated bitstrings.\n",
    "\n",
    "    Because we're working with bits, we want exactly one final bit, and we don't need num_samples because we get the logprobs.\n",
    "    Args:\n",
    "        input_str: input sequence as a string\n",
    "        temp: temperature for sampling\n",
    "        prompt: additional prompt before the input string\n",
    "        model: name of GPT-3 model to use\n",
    "    Returns:\n",
    "        list of completion strings\n",
    "    \n",
    "    Ripped from: https://github.com/ngruver/llmtime/blob/main/models/promptcast.py\n",
    "    '''\n",
    "    trick_token_count = 1000 # this is a trick to get the model to attempt to predict a large number of tokens, which we truncate with the api\n",
    "    chatgpt_sys_message = f\"\"\"You are a helpful assistant that learns to predict the final bit. The user will provide bitstrings containing ONLY 0 or 1. Each bitstring will be separated by '{delimiter}'.\n",
    "                             The final bitstring will be the test sequence, and you will predict the next bit for that bitstring. The sequence is represented by only digits 0 or 1 separated by spaces, NO COMMAS.\n",
    "                             The data may be noisy, in which case you should predict the most likely sequence.\"\"\"\n",
    "    \n",
    "    extra_input = \"\"\"Please continue the following sequence with exactly one digit, either 0 or 1. Do not produce any additional text. Do not include commas. \n",
    "                     Do not say anything like 'the next terms in the sequence are', just return the single number. \"\"\"\n",
    "    noisy_prompt = \"\"\"This data has been generated with some bitflip noise. Predict the most likely bit WITHOUT NOISE. \"\"\"\n",
    "    if noisy:\n",
    "        extra_input = extra_input + noisy_prompt\n",
    "    extra_input = extra_input + \"Sequence:\\n\"\n",
    "\n",
    "    if model in ['gpt-3.5-turbo','gpt-4']:\n",
    "        chatgpt_sys_message = chatgpt_sys_message\n",
    "        extra_input = extra_input\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                    {\"role\": \"system\", \"content\": chatgpt_sys_message},\n",
    "                    {\"role\": \"user\", \"content\": extra_input+input_str}\n",
    "                ],\n",
    "            max_tokens=1, \n",
    "            temperature=temp,\n",
    "            logit_bias=logit_bias,\n",
    "            n=num_samples,\n",
    "            logprobs=logprobs,\n",
    "            top_logprobs=top_logprobs,\n",
    "            **kwargs\n",
    "        )\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_multiline_dataset(length, n_data, number_of_generating_methods=1, lookback=3, p_bitflip=0.0, seed=1234, delimiter=\"\\n\"):\n",
    "    gen = deterministic.SequenceGen(lookback=lookback, seed=seed, number_of_generating_methods=number_of_generating_methods)\n",
    "    train_data, generating_funcs = gen.deterministically_generate_sequences(length=length, num_seq=n_data, save=False)\n",
    "    if p_bitflip > 0:\n",
    "        mask = np.random.choice([0, 1], size=(len(train_data),), p=[1-p_bitflip, p_bitflip]).astype(np.uint8)\n",
    "        train_data = np.array(train_data, dtype=np.uint) ^ mask\n",
    "    \n",
    "    input_data = \"\"\n",
    "    for row in train_data[:-1]:\n",
    "        input_data += \" \".join(row.astype(str)) + \" {} \".format(delimiter)\n",
    "    input_data += \" \".join(train_data[-1,:-1].astype(str))\n",
    "    test_data = str(train_data[-1,-1])\n",
    "\n",
    "\n",
    "    return input_data, test_data, generating_funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approx. tokens per message:  1280\n",
      "Test string:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "test gen  :  {0: 0, 1: 1, 2: 1, 3: 0}\n",
      "Truth:       0\n",
      "Prediction:  0\n",
      "Logprobs:    [TopLogprob(token='1', bytes=[49], logprob=-0.060908753), TopLogprob(token='0', bytes=[48], logprob=-2.8333545)]\n",
      "\n",
      "Control input =  \n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "Control Prediction:  1\n",
      "Control Logprobs:    [TopLogprob(token='1', bytes=[49], logprob=-0.04119792), TopLogprob(token='0', bytes=[48], logprob=-3.2113526)]\n",
      "\n",
      "Test string:  0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0\n",
      "test gen  :  {0: 1, 1: 1, 2: 0, 3: 0}\n",
      "Truth:       1\n",
      "Prediction:  1\n",
      "Logprobs:    [TopLogprob(token='1', bytes=[49], logprob=-0.10940339), TopLogprob(token='0', bytes=[48], logprob=-2.2687528)]\n",
      "\n",
      "Control input =  \n",
      " 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0\n",
      "Control Prediction:  1\n",
      "Control Logprobs:    [TopLogprob(token='1', bytes=[49], logprob=-0.045276262), TopLogprob(token='0', bytes=[48], logprob=-3.12112)]\n",
      "\n",
      "Test string:  1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "test gen  :  {0: 0, 1: 0, 2: 1, 3: 0}\n",
      "Truth:       0\n",
      "Prediction:  1\n",
      "Logprobs:    [TopLogprob(token='1', bytes=[49], logprob=-0.0357564), TopLogprob(token='0', bytes=[48], logprob=-3.3586304)]\n",
      "\n",
      "Control input =  \n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "Control Prediction:  1\n",
      "Control Logprobs:    [TopLogprob(token='1', bytes=[49], logprob=-0.032183558), TopLogprob(token='0', bytes=[48], logprob=-3.4569426)]\n",
      "\n",
      "Test string:  1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0\n",
      "test gen  :  {0: 1, 1: 0, 2: 1, 3: 1}\n",
      "Truth:       0\n",
      "Prediction:  1\n",
      "Logprobs:    [TopLogprob(token='1', bytes=[49], logprob=-0.04972591), TopLogprob(token='0', bytes=[48], logprob=-3.04137)]\n",
      "\n",
      "Control input =  \n",
      " 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0\n",
      "Control Prediction:  1\n",
      "Control Logprobs:    [TopLogprob(token='1', bytes=[49], logprob=-0.012253216), TopLogprob(token='0', bytes=[48], logprob=-4.4190507)]\n",
      "\n",
      "Test string:  1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "test gen  :  {0: 1, 1: 1, 2: 1, 3: 1}\n",
      "Truth:       1\n",
      "Prediction:  0\n",
      "Logprobs:    [TopLogprob(token='1', bytes=[49], logprob=-0.4124486), TopLogprob(token='0', bytes=[48], logprob=-1.0860727)]\n",
      "\n",
      "Control input =  \n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "Control Prediction:  0\n",
      "Control Logprobs:    [TopLogprob(token='0', bytes=[48], logprob=-0.17523552), TopLogprob(token='1', bytes=[49], logprob=-1.8290632)]\n",
      "\n",
      "Test string:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "test gen  :  {0: 0, 1: 1, 2: 1, 3: 0}\n",
      "Truth:       0\n",
      "Prediction:  1\n",
      "Logprobs:    [TopLogprob(token='1', bytes=[49], logprob=-0.035552442), TopLogprob(token='0', bytes=[48], logprob=-3.3622012)]\n",
      "\n",
      "Control input =  \n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "Control Prediction:  1\n",
      "Control Logprobs:    [TopLogprob(token='1', bytes=[49], logprob=-0.06114886), TopLogprob(token='0', bytes=[48], logprob=-2.8261578)]\n",
      "\n",
      "Test string:  1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "test gen  :  {0: 1, 1: 1, 2: 1, 3: 1}\n",
      "Truth:       1\n",
      "Prediction:  1\n",
      "Logprobs:    [TopLogprob(token='1', bytes=[49], logprob=-0.32840437), TopLogprob(token='0', bytes=[48], logprob=-1.2751226)]\n",
      "\n",
      "Control input =  \n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "Control Prediction:  0\n",
      "Control Logprobs:    [TopLogprob(token='0', bytes=[48], logprob=-0.17523552), TopLogprob(token='1', bytes=[49], logprob=-1.8290632)]\n",
      "\n",
      "Test string:  1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1\n",
      "test gen  :  {0: 1, 1: 1, 2: 1, 3: 0}\n",
      "Truth:       1\n",
      "Prediction:  1\n",
      "Logprobs:    [TopLogprob(token='0', bytes=[48], logprob=-0.67591393), TopLogprob(token='1', bytes=[49], logprob=-0.7113887)]\n",
      "\n",
      "Control input =  \n",
      " 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1\n",
      "Control Prediction:  0\n",
      "Control Logprobs:    [TopLogprob(token='0', bytes=[48], logprob=-0.15932746), TopLogprob(token='1', bytes=[49], logprob=-1.9158506)]\n",
      "\n",
      "Test string:  1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0\n",
      "test gen  :  {0: 1, 1: 1, 2: 0, 3: 0}\n",
      "Truth:       1\n",
      "Prediction:  0\n",
      "Logprobs:    [TopLogprob(token='1', bytes=[49], logprob=-0.13281514), TopLogprob(token='0', bytes=[48], logprob=-2.086562)]\n",
      "\n",
      "Control input =  \n",
      " 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0\n",
      "Control Prediction:  1\n",
      "Control Logprobs:    [TopLogprob(token='1', bytes=[49], logprob=-0.0305709), TopLogprob(token='0', bytes=[48], logprob=-3.5103931)]\n",
      "\n",
      "Test string:  1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
      "test gen  :  {0: 1, 1: 0, 2: 0, 3: 1}\n",
      "Truth:       0\n",
      "Prediction:  1\n",
      "Logprobs:    [TopLogprob(token='1', bytes=[49], logprob=-0.6070216), TopLogprob(token='0', bytes=[48], logprob=-0.7884352)]\n",
      "\n",
      "Control input =  \n",
      " 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
      "Control Prediction:  1\n",
      "Control Logprobs:    [TopLogprob(token='1', bytes=[49], logprob=-0.47728863), TopLogprob(token='0', bytes=[48], logprob=-0.9693998)]\n",
      "\n",
      "Test string:  0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
      "test gen  :  {0: 1, 1: 0, 2: 0, 3: 1}\n",
      "Truth:       0\n",
      "Prediction:  0\n",
      "Logprobs:    [TopLogprob(token='1', bytes=[49], logprob=-0.23890547), TopLogprob(token='0', bytes=[48], logprob=-1.5503469)]\n",
      "\n",
      "Control input =  \n",
      " 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
      "Control Prediction:  1\n",
      "Control Logprobs:    [TopLogprob(token='1', bytes=[49], logprob=-0.11604569), TopLogprob(token='0', bytes=[48], logprob=-2.2137325)]\n",
      "\n",
      "Test string:  0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0\n",
      "test gen  :  {0: 1, 1: 0, 2: 0, 3: 0}\n",
      "Truth:       0\n",
      "Prediction:  1\n",
      "Logprobs:    [TopLogprob(token='1', bytes=[49], logprob=-0.11432972), TopLogprob(token='0', bytes=[48], logprob=-2.2270942)]\n",
      "\n",
      "Control input =  \n",
      " 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0\n",
      "Control Prediction:  1\n",
      "Control Logprobs:    [TopLogprob(token='1', bytes=[49], logprob=-0.058212243), TopLogprob(token='0', bytes=[48], logprob=-2.8748283)]\n",
      "\n",
      "Test string:  1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1\n",
      "test gen  :  {0: 0, 1: 1, 2: 0, 3: 1}\n",
      "Truth:       0\n",
      "Prediction:  1\n",
      "Logprobs:    [TopLogprob(token='1', bytes=[49], logprob=-0.3844933), TopLogprob(token='0', bytes=[48], logprob=-1.1429658)]\n",
      "\n",
      "Control input =  \n",
      " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1\n",
      "Control Prediction:  0\n",
      "Control Logprobs:    [TopLogprob(token='1', bytes=[49], logprob=-0.33165753), TopLogprob(token='0', bytes=[48], logprob=-1.2659703)]\n",
      "\n",
      "Test string:  0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0\n",
      "test gen  :  {0: 1, 1: 0, 2: 1, 3: 0}\n",
      "Truth:       1\n",
      "Prediction:  0\n",
      "Logprobs:    [TopLogprob(token='1', bytes=[49], logprob=-0.21802637), TopLogprob(token='0', bytes=[48], logprob=-1.6309388)]\n",
      "\n",
      "Control input =  \n",
      " 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0\n",
      "Control Prediction:  0\n",
      "Control Logprobs:    [TopLogprob(token='1', bytes=[49], logprob=-0.051289752), TopLogprob(token='0', bytes=[48], logprob=-2.9978173)]\n",
      "\n",
      "Test string:  0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0\n",
      "test gen  :  {0: 0, 1: 1, 2: 0, 3: 1}\n",
      "Truth:       1\n",
      "Prediction:  1\n",
      "Logprobs:    [TopLogprob(token='1', bytes=[49], logprob=-0.042170398), TopLogprob(token='0', bytes=[48], logprob=-3.1988494)]\n",
      "\n",
      "Control input =  \n",
      " 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0\n",
      "Control Prediction:  1\n",
      "Control Logprobs:    [TopLogprob(token='1', bytes=[49], logprob=-0.020558586), TopLogprob(token='0', bytes=[48], logprob=-3.899831)]\n",
      "\n",
      "Test string:  0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1\n",
      "test gen  :  {0: 0, 1: 0, 2: 1, 3: 0}\n",
      "Truth:       1\n",
      "Prediction:  1\n",
      "Logprobs:    [TopLogprob(token='1', bytes=[49], logprob=-0.60264975), TopLogprob(token='0', bytes=[48], logprob=-0.79336554)]\n",
      "\n",
      "Control input =  \n",
      " 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1\n",
      "Control Prediction:  0\n",
      "Control Logprobs:    [TopLogprob(token='0', bytes=[48], logprob=-0.12008799), TopLogprob(token='1', bytes=[49], logprob=-2.180427)]\n",
      "\n",
      "Test string:  1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1\n",
      "test gen  :  {0: 1, 1: 0, 2: 1, 3: 0}\n",
      "Truth:       0\n",
      "Prediction:  0\n",
      "Logprobs:    [TopLogprob(token='0', bytes=[48], logprob=-0.3678934), TopLogprob(token='1', bytes=[49], logprob=-1.1791918)]\n",
      "\n",
      "Control input =  \n",
      " 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1\n",
      "Control Prediction:  0\n",
      "Control Logprobs:    [TopLogprob(token='0', bytes=[48], logprob=-0.2156284), TopLogprob(token='1', bytes=[49], logprob=-1.6406753)]\n",
      "\n",
      "Test string:  0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0\n",
      "test gen  :  {0: 1, 1: 0, 2: 1, 3: 0}\n",
      "Truth:       0\n",
      "Prediction:  1\n",
      "Logprobs:    [TopLogprob(token='1', bytes=[49], logprob=-0.0655559), TopLogprob(token='0', bytes=[48], logprob=-2.7673075)]\n",
      "\n",
      "Control input =  \n",
      " 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0\n",
      "Control Prediction:  0\n",
      "Control Logprobs:    [TopLogprob(token='1', bytes=[49], logprob=-0.07397896), TopLogprob(token='0', bytes=[48], logprob=-2.6416287)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nbits = 32\n",
    "n_data = 20\n",
    "n_experiments = 20\n",
    "number_of_generating_methods = 1\n",
    "DELIMITER = \"\\n\"\n",
    "# Total input tokens are 2* nbits * n_data * n_experiments\n",
    "print(\"approx. tokens per message: \", 2 * nbits * n_data )\n",
    "\n",
    "multiline_completions = []\n",
    "control_completions = []\n",
    "test_data_strings = []\n",
    "for seed in range(9, 7 + n_experiments):\n",
    "    input_data, test_data, generating_funcs = make_multiline_dataset(nbits, n_data, lookback=3, delimiter=DELIMITER,\n",
    "                                                                     number_of_generating_methods=number_of_generating_methods, \n",
    "                                                                     p_bitflip=0.0, seed=seed)\n",
    "    completions = sample_completions_multiline(model, input_data, num_samples=1, delimiter=DELIMITER)\n",
    "    # print(\"input_data  \", input_data)\n",
    "    print(\"Test string:\", input_data[-(nbits - 1)*2:])\n",
    "    print(\"test gen  : \", generating_funcs[-1])\n",
    "    print(\"Truth:      \", test_data)\n",
    "    print(\"Prediction: \", completions.choices[0].message.content)\n",
    "    print(\"Logprobs:   \", completions.choices[0].logprobs.content[0].top_logprobs[:2])\n",
    "    print()\n",
    "    multiline_completions.append(completions)\n",
    "\n",
    "    # control\n",
    "    completions = sample_completions_multiline(model, input_data[-(nbits)*2 - 1:], num_samples=1)\n",
    "    print(\"Control input = \", input_data[-(nbits)*2 +1:])\n",
    "    print(\"Control Prediction: \", completions.choices[0].message.content)\n",
    "    print(\"Control Logprobs:   \", completions.choices[0].logprobs.content[0].top_logprobs[:2])\n",
    "    print()\n",
    "    control_completions.append(completions) \n",
    "\n",
    "    test_data_strings.append(test_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control acc: 0.3333333333333333\n",
      "multiline acc: 0.4444444444444444\n",
      "agreement between control and multiline: 0.5555555555555556\n"
     ]
    }
   ],
   "source": [
    "n_experiments = len(multiline_completions)\n",
    "xx = [int(multiline_completions[i].choices[0].message.content) for i in range(n_experiments)]\n",
    "yy = [int(test_data_strings[i]) for i in range(n_experiments)]\n",
    "zz = [int(control_completions[i].choices[0].message.content) for i in range(n_experiments)]\n",
    "print(\"control acc:\", np.mean([1 - abs(x - y) for x, y in zip(zz, yy)]))\n",
    "print(\"multiline acc:\", np.mean([1 - abs(x - y) for x, y in zip(xx, yy)]))\n",
    "\n",
    "print(\"agreement between control and multiline:\", np.mean([1 - abs(x - y) for x, y in zip(xx, zz)]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 1, 1, 1, 0, 1, 1, 0]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TopLogprob(token='1', bytes=[49], logprob=-0.64645034)\n",
      "0.22570940712686344\n",
      "TopLogprob(token='0', bytes=[48], logprob=-0.7427066)\n",
      "0.18083954264309682\n"
     ]
    }
   ],
   "source": [
    "for x in completions.choices[0].logprobs.content[0].top_logprobs[:2]:\n",
    "    print(x)\n",
    "    print(10 ** x.logprob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[TopLogprob(token='1', bytes=[49], logprob=-0.10679226), TopLogprob(token='0', bytes=[48], logprob=-2.301685)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the completions logprob\n",
    "print(completions.choices[0].message.content)\n",
    "print(completions.choices[0].logprobs.content[0].top_logprobs[:2])\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 # 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 # 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 # 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 # 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 # 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 # 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 # 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 # 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 # 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 # 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 # 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 # 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 # 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 # 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 # 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 # 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 # 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 # 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 # 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 # 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 # 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 # 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 # 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 # 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(input_data)\n",
    "print(test_data)\n",
    "for i in range(num_samples):\n",
    "    print(completions.choices[i].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
