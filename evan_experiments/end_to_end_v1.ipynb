{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# reload magic\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import deterministic.py using local file path\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('../sequence_generators')\n",
    "import make_datasets\n",
    "sys.path.append('../entropy')\n",
    "import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import the datasets we will be using across models\n",
    "n_train = 200\n",
    "n_val = 50\n",
    "n_bits = 32 + 1\n",
    "# p_bitflip = 0.15\n",
    "seed = 334\n",
    "k = 4\n",
    "noisy_not_majority_transition_matrix = {0: 0.05, 1: 0.05, 2: 0.05, 3: 0.95, 4: 0.95}\n",
    "\n",
    "p_x_conditional = entropy.one_prob_to_conditional(noisy_majority_transition_matrix, k) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28639695711595603\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Compute the entropy rate for this dataset\n",
    "# TODO: how do we determine m?\n",
    "p_S = np.ones(shape=(2,)*k) / (2 ** k)\n",
    "m = 1\n",
    "H_m_ahead_4lookback = entropy.conditional_H_of_xnplusm_given_kbits_klookback(n_bits, m, k, p_S, p_x_conditional)\n",
    "print(H_m_ahead_4lookback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train an RNN (w/ hyperparameter tuning) on this dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RNN Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 33, 1])\n",
      "data saved to: c:\\Users\\peter\\Desktop\\projects\\MindReadingAutobot\\evan_experiments\\data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_data(split):\n",
    "    \"\"\"Load data for either training or validation.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: data of shape (n_samples, n_bits, 1)\n",
    "    \"\"\"\n",
    "    X, _ = make_datasets.k_lookback_weight_dataset(noisy_majority_transition_matrix, k, n_train+n_val, n_bits, 0, seed)\n",
    "    if split == 'train':\n",
    "        train_data = X[:n_train]\n",
    "        train_data = torch.tensor(train_data).float()\n",
    "        # train_data = train_data.unsqueeze(0)\n",
    "        train_data = train_data.unsqueeze(-1) # feature dimension is 1 for bits\n",
    "        return train_data\n",
    "    elif split == 'val':\n",
    "        val_data = X[n_train:]\n",
    "        val_data = torch.tensor(val_data).float()\n",
    "        # val_data = val_data.unsqueeze(0)\n",
    "        val_data = val_data.unsqueeze(-1) # feature dimension is 1 for bits\n",
    "        return val_data\n",
    "\n",
    "\n",
    "# Save this data to ./data\n",
    "train_data = get_data('train')\n",
    "val_data = get_data('val')\n",
    "# We want to work with absolute filepaths so that tune workers can find the data\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "data_dir = os.path.abspath('./data')\n",
    "print(\"data saved to:\", data_dir)\n",
    "torch.save(train_data, os.path.join(data_dir, \"train_data.pt\"))\n",
    "torch.save(val_data, os.path.join(data_dir, \"val_data.pt\"))\n",
    "\n",
    "def load_data(data_dir=data_dir):\n",
    "\n",
    "    train_data = torch.load(os.path.join(data_dir, \"train_data.pt\"))\n",
    "    val_data = torch.load(os.path.join(data_dir, \"val_data.pt\"))\n",
    "\n",
    "    return train_data, val_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function adapted for Ray Tune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-15 18:49:13,261\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-07-15 18:49:13 (running for 00:00:00.26)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Logical resource usage: 5.0/8 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:M1200)\n",
      "Result logdir: C:/Users/peter/AppData/Local/Temp/ray/session_2024-07-15_18-45-02_044221_22072/artifacts/2024-07-15_18-49-13/train_binary_rnn_2024-07-15_18-49-13/driver_artifacts\n",
      "Number of trials: 5/5 (5 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-07-15 18:49:18 (running for 00:00:05.29)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Logical resource usage: 5.0/8 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:M1200)\n",
      "Result logdir: C:/Users/peter/AppData/Local/Temp/ray/session_2024-07-15_18-45-02_044221_22072/artifacts/2024-07-15_18-49-13/train_binary_rnn_2024-07-15_18-49-13/driver_artifacts\n",
      "Number of trials: 5/5 (5 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-07-15 18:49:23 (running for 00:00:10.35)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Logical resource usage: 5.0/8 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:M1200)\n",
      "Result logdir: C:/Users/peter/AppData/Local/Temp/ray/session_2024-07-15_18-45-02_044221_22072/artifacts/2024-07-15_18-49-13/train_binary_rnn_2024-07-15_18-49-13/driver_artifacts\n",
      "Number of trials: 5/5 (5 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-07-15 18:49:28 (running for 00:00:15.45)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Logical resource usage: 5.0/8 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:M1200)\n",
      "Result logdir: C:/Users/peter/AppData/Local/Temp/ray/session_2024-07-15_18-45-02_044221_22072/artifacts/2024-07-15_18-49-13/train_binary_rnn_2024-07-15_18-49-13/driver_artifacts\n",
      "Number of trials: 5/5 (5 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th style=\"text-align: right;\">    loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_binary_rnn_74edc_00000</td><td style=\"text-align: right;\">0.66529 </td></tr>\n",
       "<tr><td>train_binary_rnn_74edc_00001</td><td style=\"text-align: right;\">0.714388</td></tr>\n",
       "<tr><td>train_binary_rnn_74edc_00002</td><td style=\"text-align: right;\">0.687697</td></tr>\n",
       "<tr><td>train_binary_rnn_74edc_00003</td><td style=\"text-align: right;\">0.684429</td></tr>\n",
       "<tr><td>train_binary_rnn_74edc_00004</td><td style=\"text-align: right;\">0.667484</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-07-15 18:49:33 (running for 00:00:20.53)\n",
      "Using AsyncHyperBand: num_stopped=2\n",
      "Bracket: Iter 16.000: None | Iter 8.000: -0.6821142256259918 | Iter 4.000: -0.6810106992721557 | Iter 2.000: -0.6853982865810394 | Iter 1.000: -0.6886071920394897\n",
      "Logical resource usage: 3.0/8 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:M1200)\n",
      "Result logdir: C:/Users/peter/AppData/Local/Temp/ray/session_2024-07-15_18-45-02_044221_22072/artifacts/2024-07-15_18-49-13/train_binary_rnn_2024-07-15_18-49-13/driver_artifacts\n",
      "Number of trials: 5/5 (3 RUNNING, 2 TERMINATED)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-15 18:49:36,057\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'C:/Users/peter/ray_results/train_binary_rnn_2024-07-15_18-49-13' in 0.0180s.\n",
      "2024-07-15 18:49:36,063\tINFO tune.py:1041 -- Total run time: 22.80 seconds (22.76 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-07-15 18:49:36 (running for 00:00:22.78)\n",
      "Using AsyncHyperBand: num_stopped=5\n",
      "Bracket: Iter 16.000: -0.6703729033470154 | Iter 8.000: -0.6772363185882568 | Iter 4.000: -0.6810106992721557 | Iter 2.000: -0.6853982865810394 | Iter 1.000: -0.6886071920394897\n",
      "Logical resource usage: 1.0/8 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:M1200)\n",
      "Result logdir: C:/Users/peter/AppData/Local/Temp/ray/session_2024-07-15_18-45-02_044221_22072/artifacts/2024-07-15_18-49-13/train_binary_rnn_2024-07-15_18-49-13/driver_artifacts\n",
      "Number of trials: 5/5 (5 TERMINATED)\n",
      "+------------------------------+------------+-----------------+---------------+--------+--------------+----------+----------------------+\n",
      "| Trial name                   | status     | loc             |   hidden_size |     lr |   num_layers |     loss |   training_iteration |\n",
      "|------------------------------+------------+-----------------+---------------+--------+--------------+----------+----------------------|\n",
      "| train_binary_rnn_74edc_00000 | TERMINATED | 127.0.0.1:14604 |            16 | 0.0001 |            1 | 0.66529  |                   20 |\n",
      "| train_binary_rnn_74edc_00001 | TERMINATED | 127.0.0.1:15760 |            16 | 0.0001 |            1 | 0.714388 |                    1 |\n",
      "| train_binary_rnn_74edc_00002 | TERMINATED | 127.0.0.1:22144 |            16 | 0.0001 |            1 | 0.687697 |                    2 |\n",
      "| train_binary_rnn_74edc_00003 | TERMINATED | 127.0.0.1:8768  |            16 | 0.0001 |            1 | 0.684429 |                   16 |\n",
      "| train_binary_rnn_74edc_00004 | TERMINATED | 127.0.0.1:1368  |            16 | 0.0001 |            1 | 0.667484 |                   20 |\n",
      "+------------------------------+------------+-----------------+---------------+--------+--------------+----------+----------------------+\n",
      "\n",
      "\n",
      "Best trial config: {'hidden_size': 16, 'num_layers': 1, 'lr': 0.0001}\n",
      "Best trial final validation loss: 0.6652895092964173\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss(verbose=False):\n",
    "    # Average the loss over many batches. Hardcoded cross_entropy loss\n",
    "    # Needs to be in same namespace as model and get_batch\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        \n",
    "        data = get_data(split)\n",
    "        X = data[:,:-1,:]\n",
    "        Y = data[:,-1,:]\n",
    "        output = model(X)\n",
    "        logits = torch.sigmoid(output[:, -1, :])\n",
    "        loss = F.binary_cross_entropy_with_logits(output[:, -1, :], Y)\n",
    "        if verbose:\n",
    "            preds = torch.argmax(logits.reshape(batch_size, N_BITS + 1, tgt_vocab_size), dim=2)\n",
    "            for i in range(len(X)):\n",
    "                print(f\"input: {X[i]}\")\n",
    "                print(f\"target: {tgt_out[i]}\")\n",
    "                print(f\"predicted: {preds[i]}\")\n",
    "                print()\n",
    "        loss = loss.item()\n",
    "        out[split] = loss\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, epochs, criterion, optimizer, eval_inter=50):\n",
    "    model.train()\n",
    "    # training involves next-bit prediction.\n",
    "    for epoch in range(epochs):\n",
    "        for i, seq in enumerate(data):\n",
    "            seq = seq.unsqueeze(-1)  # Adding feature dimension\n",
    "            # print(seq.shape)\n",
    "            inputs = seq[:, :-1]  # All but the last bit\n",
    "            targets = seq[:, 1:]  # All but the first bit\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if epoch % eval_inter == 0:\n",
    "            losses = estimate_loss()\n",
    "            print(f'Epoch [{epoch+1}/{epochs}]', losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2000] {'train': 0.7018217444419861, 'val': 0.6965770125389099}\n",
      "Epoch [51/2000] {'train': 0.5417907238006592, 'val': 0.554243266582489}\n",
      "Epoch [101/2000] {'train': 0.3833094835281372, 'val': 0.3069271743297577}\n",
      "Epoch [151/2000] {'train': 0.3639773428440094, 'val': 0.2917017638683319}\n",
      "Epoch [201/2000] {'train': 0.35240811109542847, 'val': 0.2819930911064148}\n",
      "Epoch [251/2000] {'train': 0.3421016037464142, 'val': 0.27489736676216125}\n",
      "Epoch [301/2000] {'train': 0.32974743843078613, 'val': 0.2673196494579315}\n",
      "Epoch [351/2000] {'train': 0.31663206219673157, 'val': 0.2570190727710724}\n",
      "Epoch [401/2000] {'train': 0.307634562253952, 'val': 0.2505490779876709}\n",
      "Epoch [451/2000] {'train': 0.2992391586303711, 'val': 0.24654994904994965}\n",
      "Epoch [501/2000] {'train': 0.29195377230644226, 'val': 0.24315164983272552}\n",
      "Epoch [551/2000] {'train': 0.2884753942489624, 'val': 0.2395324856042862}\n",
      "Epoch [601/2000] {'train': 0.28489232063293457, 'val': 0.22818109393119812}\n",
      "Epoch [651/2000] {'train': 0.28469085693359375, 'val': 0.231045663356781}\n",
      "Epoch [701/2000] {'train': 0.282918781042099, 'val': 0.24315430223941803}\n",
      "Epoch [751/2000] {'train': 0.26664185523986816, 'val': 0.255447655916214}\n",
      "Epoch [801/2000] {'train': 0.24956394731998444, 'val': 0.2811291813850403}\n",
      "Epoch [851/2000] {'train': 0.23409612476825714, 'val': 0.28829726576805115}\n",
      "Epoch [901/2000] {'train': 0.2213578075170517, 'val': 0.29782283306121826}\n",
      "Epoch [951/2000] {'train': 0.20003271102905273, 'val': 0.295077383518219}\n",
      "Epoch [1001/2000] {'train': 0.20337848365306854, 'val': 0.29435986280441284}\n",
      "Epoch [1051/2000] {'train': 0.170390322804451, 'val': 0.29401353001594543}\n",
      "Epoch [1101/2000] {'train': 0.18699400126934052, 'val': 0.2912343144416809}\n",
      "Epoch [1151/2000] {'train': 0.1647217720746994, 'val': 0.29294317960739136}\n",
      "Epoch [1201/2000] {'train': 0.16140304505825043, 'val': 0.29054370522499084}\n",
      "Epoch [1251/2000] {'train': 0.1659357249736786, 'val': 0.2821948528289795}\n",
      "Epoch [1301/2000] {'train': 0.14790262281894684, 'val': 0.2861019968986511}\n",
      "Epoch [1351/2000] {'train': 0.14998817443847656, 'val': 0.27652478218078613}\n",
      "Epoch [1401/2000] {'train': 0.13285869359970093, 'val': 0.2887669801712036}\n",
      "Epoch [1451/2000] {'train': 0.13332068920135498, 'val': 0.2860434353351593}\n",
      "Epoch [1501/2000] {'train': 0.14146946370601654, 'val': 0.2979375720024109}\n",
      "Epoch [1551/2000] {'train': 0.12939317524433136, 'val': 0.30821043252944946}\n",
      "Epoch [1601/2000] {'train': 0.11793279647827148, 'val': 0.2967613935470581}\n",
      "Epoch [1651/2000] {'train': 0.11276578903198242, 'val': 0.2825656235218048}\n",
      "Epoch [1701/2000] {'train': 0.10984069854021072, 'val': 0.2899329960346222}\n",
      "Epoch [1751/2000] {'train': 0.11365213990211487, 'val': 0.2836637496948242}\n",
      "Epoch [1801/2000] {'train': 0.10557714104652405, 'val': 0.29262733459472656}\n",
      "Epoch [1851/2000] {'train': 0.10689694434404373, 'val': 0.29539698362350464}\n",
      "Epoch [1901/2000] {'train': 0.10262342542409897, 'val': 0.29361632466316223}\n",
      "Epoch [1951/2000] {'train': 0.10332176834344864, 'val': 0.2998441457748413}\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 32\n",
    "num_layers = 1\n",
    "lr = 0.001\n",
    "n_epochs = 2000\n",
    "\n",
    "\n",
    "model = BinaryRNN(1, hidden_size, 1, num_layers)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "train(model, train_data, n_epochs, criterion, optimizer, eval_inter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = torch.tensor(val_data).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, start_sequence, num_future_bits):\n",
    "    model.eval()\n",
    "    inputs = torch.tensor(start_sequence, dtype=torch.float32).unsqueeze(0).unsqueeze(-1)\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_future_bits):\n",
    "            output = model(inputs)\n",
    "            next_bit = torch.round(torch.sigmoid(output[:, -1, :])).item()\n",
    "            predictions.append(next_bit)\n",
    "            next_input = torch.tensor([[next_bit]], dtype=torch.float32)\n",
    "            inputs = torch.cat((inputs, next_input.unsqueeze(-1)), dim=1)\n",
    "    \n",
    "    return predictions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
