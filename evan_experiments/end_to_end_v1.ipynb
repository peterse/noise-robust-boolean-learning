{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload magic\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import deterministic.py using local file path\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('../sequence_generators')\n",
    "import make_datasets\n",
    "sys.path.append('../entropy')\n",
    "import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import the datasets we will be using across models\n",
    "n_train = 200\n",
    "n_val = 50\n",
    "n_bits = 32\n",
    "# p_bitflip = 0.15\n",
    "seed = 334\n",
    "k = 4\n",
    "noisy_transition_matrix = {0: 0.15, 1: 0.96, 2: 0.94, 3: 0.10, 4: 0.08}\n",
    "\n",
    "p_x_conditional = entropy.one_prob_to_conditional(noisy_transition_matrix, k) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.381112052756314\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Compute the entropy rate for this dataset\n",
    "# TODO: how do we determine m?\n",
    "p_S = np.ones(shape=(2,)*k) / (2 ** k)\n",
    "m = 1\n",
    "H_m_ahead_4lookback = entropy.conditional_H_of_xnplusm_given_kbits_klookback(n_bits, m, k, p_S, p_x_conditional)\n",
    "print(H_m_ahead_4lookback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train a recurrent neural network on this dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BinaryRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(BinaryRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(split):\n",
    "    X, _ = make_datasets.k_lookback_weight_dataset(noisy_transition_matrix, k, n_train+n_val, n_bits, 0, seed)\n",
    "    if split == 'train':\n",
    "        train_data = X[:n_train]\n",
    "        train_data = torch.tensor(train_data).float()\n",
    "        # train_data = train_data.unsqueeze(0)\n",
    "        train_data = train_data.unsqueeze(-1) # feature dimension is 1 for bits\n",
    "        return train_data\n",
    "    elif split == 'val':\n",
    "        val_data = X[n_train:]\n",
    "        val_data = torch.tensor(val_data).float()\n",
    "        # val_data = val_data.unsqueeze(0)\n",
    "        val_data = val_data.unsqueeze(-1) # feature dimension is 1 for bits\n",
    "        return val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss(verbose=False):\n",
    "    # Average the loss over many batches. Hardcoded cross_entropy loss\n",
    "    # Needs to be in same namespace as model and get_batch\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        \n",
    "        data = get_data(split)\n",
    "        X = data[:,:-1,:]\n",
    "        Y = data[:,-1,:]\n",
    "        output = model(X)\n",
    "        logits = torch.sigmoid(output[:, -1, :])\n",
    "        loss = F.binary_cross_entropy_with_logits(output[:, -1, :], Y)\n",
    "        if verbose:\n",
    "            preds = torch.argmax(logits.reshape(batch_size, N_BITS + 1, tgt_vocab_size), dim=2)\n",
    "            for i in range(len(X)):\n",
    "                print(f\"input: {X[i]}\")\n",
    "                print(f\"target: {tgt_out[i]}\")\n",
    "                print(f\"predicted: {preds[i]}\")\n",
    "                print()\n",
    "        loss = loss.item()\n",
    "        out[split] = loss\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, epochs, criterion, optimizer, eval_inter=50):\n",
    "    model.train()\n",
    "    # training involves next-bit prediction.\n",
    "    for epoch in range(epochs):\n",
    "        for i, seq in enumerate(data):\n",
    "            seq = seq.unsqueeze(-1)  # Adding feature dimension\n",
    "            # print(seq.shape)\n",
    "            inputs = seq[:, :-1]  # All but the last bit\n",
    "            targets = seq[:, 1:]  # All but the first bit\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if epoch % eval_inter == 0:\n",
    "            losses = estimate_loss()\n",
    "            print(f'Epoch [{epoch+1}/{epochs}]', losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2000] {'train': 0.7018217444419861, 'val': 0.6965770125389099}\n",
      "Epoch [51/2000] {'train': 0.5417907238006592, 'val': 0.554243266582489}\n",
      "Epoch [101/2000] {'train': 0.3833094835281372, 'val': 0.3069271743297577}\n",
      "Epoch [151/2000] {'train': 0.3639773428440094, 'val': 0.2917017638683319}\n",
      "Epoch [201/2000] {'train': 0.35240811109542847, 'val': 0.2819930911064148}\n",
      "Epoch [251/2000] {'train': 0.3421016037464142, 'val': 0.27489736676216125}\n",
      "Epoch [301/2000] {'train': 0.32974743843078613, 'val': 0.2673196494579315}\n",
      "Epoch [351/2000] {'train': 0.31663206219673157, 'val': 0.2570190727710724}\n",
      "Epoch [401/2000] {'train': 0.307634562253952, 'val': 0.2505490779876709}\n",
      "Epoch [451/2000] {'train': 0.2992391586303711, 'val': 0.24654994904994965}\n",
      "Epoch [501/2000] {'train': 0.29195377230644226, 'val': 0.24315164983272552}\n",
      "Epoch [551/2000] {'train': 0.2884753942489624, 'val': 0.2395324856042862}\n",
      "Epoch [601/2000] {'train': 0.28489232063293457, 'val': 0.22818109393119812}\n",
      "Epoch [651/2000] {'train': 0.28469085693359375, 'val': 0.231045663356781}\n",
      "Epoch [701/2000] {'train': 0.282918781042099, 'val': 0.24315430223941803}\n",
      "Epoch [751/2000] {'train': 0.26664185523986816, 'val': 0.255447655916214}\n",
      "Epoch [801/2000] {'train': 0.24956394731998444, 'val': 0.2811291813850403}\n",
      "Epoch [851/2000] {'train': 0.23409612476825714, 'val': 0.28829726576805115}\n",
      "Epoch [901/2000] {'train': 0.2213578075170517, 'val': 0.29782283306121826}\n",
      "Epoch [951/2000] {'train': 0.20003271102905273, 'val': 0.295077383518219}\n",
      "Epoch [1001/2000] {'train': 0.20337848365306854, 'val': 0.29435986280441284}\n",
      "Epoch [1051/2000] {'train': 0.170390322804451, 'val': 0.29401353001594543}\n",
      "Epoch [1101/2000] {'train': 0.18699400126934052, 'val': 0.2912343144416809}\n",
      "Epoch [1151/2000] {'train': 0.1647217720746994, 'val': 0.29294317960739136}\n",
      "Epoch [1201/2000] {'train': 0.16140304505825043, 'val': 0.29054370522499084}\n",
      "Epoch [1251/2000] {'train': 0.1659357249736786, 'val': 0.2821948528289795}\n",
      "Epoch [1301/2000] {'train': 0.14790262281894684, 'val': 0.2861019968986511}\n",
      "Epoch [1351/2000] {'train': 0.14998817443847656, 'val': 0.27652478218078613}\n",
      "Epoch [1401/2000] {'train': 0.13285869359970093, 'val': 0.2887669801712036}\n",
      "Epoch [1451/2000] {'train': 0.13332068920135498, 'val': 0.2860434353351593}\n",
      "Epoch [1501/2000] {'train': 0.14146946370601654, 'val': 0.2979375720024109}\n",
      "Epoch [1551/2000] {'train': 0.12939317524433136, 'val': 0.30821043252944946}\n",
      "Epoch [1601/2000] {'train': 0.11793279647827148, 'val': 0.2967613935470581}\n",
      "Epoch [1651/2000] {'train': 0.11276578903198242, 'val': 0.2825656235218048}\n",
      "Epoch [1701/2000] {'train': 0.10984069854021072, 'val': 0.2899329960346222}\n",
      "Epoch [1751/2000] {'train': 0.11365213990211487, 'val': 0.2836637496948242}\n",
      "Epoch [1801/2000] {'train': 0.10557714104652405, 'val': 0.29262733459472656}\n",
      "Epoch [1851/2000] {'train': 0.10689694434404373, 'val': 0.29539698362350464}\n",
      "Epoch [1901/2000] {'train': 0.10262342542409897, 'val': 0.29361632466316223}\n",
      "Epoch [1951/2000] {'train': 0.10332176834344864, 'val': 0.2998441457748413}\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 32\n",
    "num_layers = 1\n",
    "lr = 0.001\n",
    "n_epochs = 2000\n",
    "\n",
    "\n",
    "model = BinaryRNN(1, hidden_size, 1, num_layers)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "train(model, train_data, n_epochs, criterion, optimizer, eval_inter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = torch.tensor(val_data).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, start_sequence, num_future_bits):\n",
    "    model.eval()\n",
    "    inputs = torch.tensor(start_sequence, dtype=torch.float32).unsqueeze(0).unsqueeze(-1)\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_future_bits):\n",
    "            output = model(inputs)\n",
    "            next_bit = torch.round(torch.sigmoid(output[:, -1, :])).item()\n",
    "            predictions.append(next_bit)\n",
    "            next_input = torch.tensor([[next_bit]], dtype=torch.float32)\n",
    "            inputs = torch.cat((inputs, next_input.unsqueeze(-1)), dim=1)\n",
    "    \n",
    "    return predictions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
